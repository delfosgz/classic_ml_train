{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.lightgbm # Add other flavors if more models are used later (xgboost, catboost)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib # For potential model saving/loading, though MLflow handles most\n",
    "import time\n",
    "import shutil # For cleaning up temp directories if any\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge # Example meta-learner\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import col, udf # Keep for future Spark ML preprocessing part\n",
    "# from pyspark.sql.types import ArrayType, DoubleType # Keep for future Spark ML preprocessing part\n",
    "# from pyspark.ml.linalg import VectorUDT, DenseVector, SparseVector # Keep for future Spark ML preprocessing part\n",
    "\n",
    "# Suppress LightGBM verbosity for HPO trials\n",
    "import logging\n",
    "logging.getLogger('lightgbm').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Ensure spark session is available (Databricks notebooks usually provide 'spark')\n",
    "if 'spark' not in locals():\n",
    "    spark = SparkSession.builder.appName(\"AdvancedML_MVP_Sequential\").getOrCreate()\n",
    "\n",
    "print(\"Imports successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a426b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Init Cell - Global Configurations\n",
    "\n",
    "# --- MLflow Configuration ---\n",
    "# !!! IMPORTANT: SET YOUR MLFLOW EXPERIMENT PATH !!!\n",
    "EXPERIMENT_PATH = \"/Users/your_username@example.com/MVP_Regression_HPO_Ensemble\" # e.g., /Users/your.email@domain.com/MyProjectExperiment\n",
    "# Get your username from dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get() if needed\n",
    "\n",
    "# --- Data Paths (Unity Catalog Volumes) ---\n",
    "# !!! IMPORTANT: SET YOUR UNITY CATALOG VOLUME BASE PATH !!!\n",
    "# Example: \"/Volumes/my_main_catalog/my_bronze_schema/my_project_volume/\"\n",
    "UC_BASE_DATA_PATH = \"/Volumes/delfos/\" # As per your input\n",
    "\n",
    "# --- Paths for Preprocessed Data (Output of your Spark ML Preprocessing Pipeline) ---\n",
    "# These paths MUST point to Parquet files (or directories of Parquet files)\n",
    "# containing 'features_array' and your label column.\n",
    "# We'll assume a single version of preprocessed data for this MVP.\n",
    "# !!! IMPORTANT: UPDATE THESE AFTER YOUR PREPROCESSING STEP SAVES DATA !!!\n",
    "SHARED_PROCESSED_TRAIN_PATH = f\"{UC_BASE_DATA_PATH}processed_data/train_processed.parquet\"\n",
    "SHARED_PROCESSED_TEST_PATH = f\"{UC_BASE_DATA_PATH}processed_data/test_processed.parquet\"\n",
    "\n",
    "# !!! IMPORTANT: SET YOUR ACTUAL LABEL COLUMN NAME AS IT APPEARS IN THE PARQUET FILES !!!\n",
    "YOUR_LABEL_COLUMN_NAME = \"target\"\n",
    "\n",
    "# --- Paths for Intermediate OOF/Test Predictions (will be created under UC_BASE_DATA_PATH) ---\n",
    "UC_OOF_PREDS_DIR = os.path.join(UC_BASE_DATA_PATH, \"oof_predictions\")\n",
    "UC_TEST_PREDS_DIR = os.path.join(UC_BASE_DATA_PATH, \"test_predictions\")\n",
    "UC_FINAL_MODELS_DIR = os.path.join(UC_BASE_DATA_PATH, \"final_models_hpo\") # For non-MLflow saved models if any\n",
    "\n",
    "# --- HPO Configuration ---\n",
    "NUM_HPO_TRIALS = 25  # Number of trials for EACH base algorithm's HPO (keep low for MVP testing, increase later)\n",
    "PRIMARY_METRIC = \"rmse\"  # Choose 'rmse' (to minimize) or 'r2' (to maximize, HPO will minimize -r2)\n",
    "\n",
    "# --- Base Algorithms to Run ---\n",
    "# For MVP: DecisionTree, RandomForest, ExtraTrees, LightGBM\n",
    "BASE_ALGORITHMS_TO_RUN = ['decision_tree', 'random_forest', 'extra_trees', 'lightgbm']\n",
    "\n",
    "# --- Cross-Validation for OOF ---\n",
    "K_FOLDS_OOF = 5 # Number of folds for generating Out-of-Fold predictions\n",
    "\n",
    "# --- Reproducibility ---\n",
    "GLOBAL_SEED = 117\n",
    "\n",
    "# --- MLflow Setup ---\n",
    "# Function get_or_create_experiment will be defined in the next cell\n",
    "# experiment_id = get_or_create_experiment(EXPERIMENT_PATH, spark) # Pass spark if needed by function\n",
    "# if experiment_id:\n",
    "#    mlflow.set_experiment(experiment_id=experiment_id)\n",
    "# else:\n",
    "#    print(\"Error: MLflow experiment could not be set.\")\n",
    "\n",
    "# --- Ensemble Configuration ---\n",
    "META_LEARNERS_FOR_STACKING = {\n",
    "    'ridge': Ridge(random_state=GLOBAL_SEED),\n",
    "    'lgbm_meta': lgb.LGBMRegressor(random_state=GLOBAL_SEED, verbose=-1, n_jobs=-1) # Simple LGBM for meta\n",
    "}\n",
    "\n",
    "# --- Other Global Settings ---\n",
    "MAX_METRICS_TO_LOG = 5 # Max number of metrics to log per MLflow run besides primary\n",
    "\n",
    "# Create directories if they don't exist (use dbutils for UC volumes if direct os.makedirs fails)\n",
    "# For UC Volumes, direct os.makedirs might not work from driver for non /dbfs/ paths.\n",
    "# Spark can write to these paths, and for local operations, you might need to use /dbfs/ equivalent if copying.\n",
    "# For saving pandas DFs, ensure the path is accessible.\n",
    "# For now, we assume spark.write.parquet will handle UC Volume paths.\n",
    "# For pandas.to_parquet, use /dbfs/Volumes/... path.\n",
    "DBFS_UC_OOF_PREDS_DIR = f\"/dbfs{UC_OOF_PREDS_DIR}\"\n",
    "DBFS_UC_TEST_PREDS_DIR = f\"/dbfs{UC_TEST_PREDS_DIR}\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(DBFS_UC_OOF_PREDS_DIR, exist_ok=True)\n",
    "    os.makedirs(DBFS_UC_TEST_PREDS_DIR, exist_ok=True)\n",
    "    print(f\"Created/checked OOF directory: {DBFS_UC_OOF_PREDS_DIR}\")\n",
    "    print(f\"Created/checked Test Preds directory: {DBFS_UC_TEST_PREDS_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create directories using os.makedirs on {DBFS_UC_OOF_PREDS_DIR} or {DBFS_UC_TEST_PREDS_DIR}. This might be okay if Spark handles it or if paths are purely for Spark. Error: {e}\")\n",
    "\n",
    "\n",
    "print(f\"--- Global Configurations Initialized ---\")\n",
    "print(f\"MLflow Experiment Path: {EXPERIMENT_PATH}\")\n",
    "print(f\"Unity Catalog Base Data Path: {UC_BASE_DATA_PATH}\")\n",
    "print(f\"Processed Train Data Path: {SHARED_PROCESSED_TRAIN_PATH}\")\n",
    "print(f\"Processed Test Data Path: {SHARED_PROCESSED_TEST_PATH}\")\n",
    "print(f\"Label Column: {YOUR_LABEL_COLUMN_NAME}\")\n",
    "print(f\"Global Seed: {GLOBAL_SEED}\")\n",
    "print(f\"Primary Metric for HPO: {PRIMARY_METRIC.upper()}\")\n",
    "print(f\"Number of HPO Trials per Algorithm: {NUM_HPO_TRIALS}\")\n",
    "print(f\"K-Folds for OOF: {K_FOLDS_OOF}\")\n",
    "print(f\"Base Algorithms to run: {BASE_ALGORITHMS_TO_RUN}\")\n",
    "print(f\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b9a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Utility Functions & HPO/Model Training Components\n",
    "\n",
    "# --- MLflow Utility ---\n",
    "def get_or_create_experiment(experiment_name, spark_session):\n",
    "    \"\"\"Safely creates or fetches an MLflow experiment.\"\"\"\n",
    "    try:\n",
    "        # Check if running in a Databricks notebook environment\n",
    "        if hasattr(spark_session, 'databricks'): # A bit of a hacky check\n",
    "            # In Databricks, experiment names can be full paths\n",
    "            # client = mlflow.tracking.MlflowClient() # Not needed if using mlflow.set_experiment\n",
    "            experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "            if experiment:\n",
    "                print(f\"MLflow experiment '{experiment_name}' found with ID: {experiment.experiment_id}\")\n",
    "                return experiment.experiment_id\n",
    "            else:\n",
    "                print(f\"MLflow experiment '{experiment_name}' not found. Attempting to create.\")\n",
    "                experiment_id = mlflow.create_experiment(name=experiment_name)\n",
    "                print(f\"MLflow experiment '{experiment_name}' created with ID: {experiment_id}\")\n",
    "                return experiment_id\n",
    "        else: # Fallback for local execution if needed, though UC Volumes imply Databricks\n",
    "            if not mlflow.get_experiment_by_name(experiment_name):\n",
    "                mlflow.create_experiment(experiment_name)\n",
    "            return mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "    except mlflow.exceptions.MlflowException as e:\n",
    "        if \"RESOURCE_ALREADY_EXISTS\" in str(e) or \"Experiment with name\" in str(e) and \"already exists\" in str(e):\n",
    "            print(f\"Race condition or experiment '{experiment_name}' was created concurrently. Fetching again.\")\n",
    "            experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "            if experiment:\n",
    "                print(f\"Successfully fetched concurrently created experiment '{experiment_name}' with ID: {experiment.experiment_id}\")\n",
    "                return experiment.experiment_id\n",
    "        print(f\"MLflowException: Could not get or create experiment '{experiment_name}'. Error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in get_or_create_experiment for '{experiment_name}'. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Algorithm Search Spaces (Top ~5 HPs) ---\n",
    "ALGORITHM_SEARCH_SPACES = {\n",
    "    'decision_tree': {\n",
    "        'model_params': {\n",
    "            'max_depth': hp.choice('dt_max_depth', [3, 5, 7, 10, 15, None]),\n",
    "            'min_samples_split': hp.quniform('dt_min_samples_split', 2, 20, 1),\n",
    "            'min_samples_leaf': hp.quniform('dt_min_samples_leaf', 1, 20, 1),\n",
    "            'criterion': hp.choice('dt_criterion', ['squared_error', 'friedman_mse', 'absolute_error']), # Poisson removed as it's for counts\n",
    "            'splitter': hp.choice('dt_splitter', ['best', 'random'])\n",
    "        }\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model_params': {\n",
    "            'n_estimators': hp.quniform('rf_n_estimators', 20, 200, 10), # Reduced for MVP speed\n",
    "            'max_depth': hp.choice('rf_max_depth', [5, 10, 15, None]),\n",
    "            'min_samples_split': hp.quniform('rf_min_samples_split', 2, 10, 1),\n",
    "            'min_samples_leaf': hp.quniform('rf_min_samples_leaf', 1, 10, 1),\n",
    "            'max_features': hp.choice('rf_max_features', ['sqrt', 'log2', None])\n",
    "        }\n",
    "    },\n",
    "    'extra_trees': {\n",
    "        'model_params': {\n",
    "            'n_estimators': hp.quniform('et_n_estimators', 20, 200, 10), # Reduced for MVP speed\n",
    "            'max_depth': hp.choice('et_max_depth', [5, 10, 15, None]),\n",
    "            'min_samples_split': hp.quniform('et_min_samples_split', 2, 10, 1),\n",
    "            'min_samples_leaf': hp.quniform('et_min_samples_leaf', 1, 10, 1),\n",
    "            'max_features': hp.choice('et_max_features', ['sqrt', 'log2', None])\n",
    "        }\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'model_params': {\n",
    "            'n_estimators': hp.quniform('lgbm_n_estimators', 20, 200, 10), # Reduced for MVP speed\n",
    "            'learning_rate': hp.loguniform('lgbm_learning_rate', np.log(0.01), np.log(0.2)),\n",
    "            'num_leaves': hp.quniform('lgbm_num_leaves', 10, 100, 5), # Reduced for MVP speed\n",
    "            'max_depth': hp.quniform('lgbm_max_depth', 3, 10, 1), # More constrained for MVP\n",
    "            'subsample': hp.uniform('lgbm_subsample', 0.7, 1.0),\n",
    "            'reg_alpha': hp.uniform('lgbm_reg_alpha', 0.0, 0.5) # L1 regularization\n",
    "        }\n",
    "    }\n",
    "    # Add XGBoostRegressor, CatBoostRegressor search spaces here when you include them\n",
    "}\n",
    "print(\"Search spaces defined.\")\n",
    "\n",
    "\n",
    "# --- HPO Objective Function (Generalized for Regression) ---\n",
    "# Note: HPO_PARENT_RUN_ID_FOR_OBJECTIVE, SHARED_PROCESSED_TRAIN_PATH_FOR_OBJECTIVE etc.\n",
    "# will be set dynamically before calling fmin for each algorithm.\n",
    "# This function relies on these being in its execution scope.\n",
    "\n",
    "# Define these as placeholders, they will be updated by the HPO orchestrator for each algorithm\n",
    "HPO_PARENT_RUN_ID_FOR_OBJECTIVE = None\n",
    "CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE = None\n",
    "# Primary metric and seed are from global scope\n",
    "# Data paths also need to be accessible\n",
    "\n",
    "def load_processed_data_for_sklearn(train_path, test_path, label_col_name):\n",
    "    \"\"\"Loads preprocessed data from Parquet and prepares for scikit-learn.\"\"\"\n",
    "    try:\n",
    "        train_pdf = pd.read_parquet(train_path)\n",
    "        test_pdf = pd.read_parquet(test_path)\n",
    "\n",
    "        X_train = np.array(train_pdf['features_array'].tolist())\n",
    "        y_train = train_pdf[label_col_name].values.astype(float)\n",
    "        X_test = np.array(test_pdf['features_array'].tolist())\n",
    "        y_test = test_pdf[label_col_name].values.astype(float)\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading/processing data from {train_path} or {test_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def objective_function_regression(hyperparams_from_hyperopt):\n",
    "    \"\"\"\n",
    "    Objective function for Hyperopt HPO.\n",
    "    Trains a specific regressor, logs to MLflow, returns loss.\n",
    "    Relies on HPO_PARENT_RUN_ID_FOR_OBJECTIVE and CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE being set.\n",
    "    Also SHARED_PROCESSED_TRAIN_PATH, SHARED_PROCESSED_TEST_PATH, YOUR_LABEL_COLUMN_NAME,\n",
    "    PRIMARY_METRIC, GLOBAL_SEED, MAX_METRICS_TO_LOG from global scope.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sanitize hyperparams (convert numpy types to native Python for model constructors)\n",
    "    # This is crucial because Hyperopt can pass np.int64 etc.\n",
    "    sanitized_hyperparams = {}\n",
    "    for k, v in hyperparams_from_hyperopt.items():\n",
    "        if isinstance(v, np.generic):\n",
    "            sanitized_hyperparams[k] = v.item()\n",
    "        elif k in ['max_depth'] and v is not None: # Max_depth can be None or int\n",
    "             sanitized_hyperparams[k] = int(v) if v is not None else None\n",
    "        # Ensure integer types for specific hyperparameters expected by models\n",
    "        elif k in ['min_samples_split', 'min_samples_leaf', 'n_estimators', 'num_leaves', 'iterations'] and v is not None:\n",
    "             sanitized_hyperparams[k] = int(v)\n",
    "        else:\n",
    "            sanitized_hyperparams[k] = v\n",
    "    \n",
    "    trial_run_name = f\"Trial_{CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE}_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "    with mlflow.start_run(run_id=HPO_PARENT_RUN_ID_FOR_OBJECTIVE, run_name=trial_run_name, nested=True) as trial_run:\n",
    "        mlflow.log_param(\"model_type_trial\", CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE) # Log the actual model type for this trial\n",
    "        mlflow.log_params(sanitized_hyperparams)\n",
    "        mlflow.set_tag(\"seed\", GLOBAL_SEED)\n",
    "\n",
    "        try:\n",
    "            X_train, y_train, X_test, y_test = load_processed_data_for_sklearn(\n",
    "                SHARED_PROCESSED_TRAIN_PATH, SHARED_PROCESSED_TEST_PATH, YOUR_LABEL_COLUMN_NAME\n",
    "            )\n",
    "\n",
    "            model = None\n",
    "            if CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'decision_tree':\n",
    "                model = DecisionTreeRegressor(**sanitized_hyperparams, random_state=GLOBAL_SEED)\n",
    "            elif CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'random_forest':\n",
    "                model = RandomForestRegressor(**sanitized_hyperparams, random_state=GLOBAL_SEED, n_jobs=-1)\n",
    "            elif CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'extra_trees':\n",
    "                model = ExtraTreesRegressor(**sanitized_hyperparams, random_state=GLOBAL_SEED, n_jobs=-1)\n",
    "            elif CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'lightgbm':\n",
    "                model = lgb.LGBMRegressor(**sanitized_hyperparams, random_state=GLOBAL_SEED, n_jobs=-1, verbose=-1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model type in objective function: {CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE}\")\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "            r2 = r2_score(y_test, predictions)\n",
    "            mae = mean_absolute_error(y_test, predictions)\n",
    "            \n",
    "            metrics_to_log = {\"rmse\": rmse, \"r2\": r2, \"mae\": mae}\n",
    "            logged_metrics_count = 0\n",
    "            # Log up to MAX_METRICS_TO_LOG, prioritizing based on sorted name for consistency\n",
    "            for m_name, m_val in sorted(metrics_to_log.items()):\n",
    "                if logged_metrics_count < MAX_METRICS_TO_LOG:\n",
    "                    mlflow.log_metric(m_name, m_val)\n",
    "                    logged_metrics_count +=1\n",
    "\n",
    "            if CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'lightgbm':\n",
    "                mlflow.lightgbm.log_model(model, \"model\", signature=mlflow.models.infer_signature(X_test, pd.Series(predictions, name=YOUR_LABEL_COLUMN_NAME)))\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(model, \"model\", signature=mlflow.models.infer_signature(X_test, pd.Series(predictions, name=YOUR_LABEL_COLUMN_NAME)))\n",
    "            \n",
    "            mlflow.set_tag(\"status\", \"success\")\n",
    "\n",
    "            loss = None\n",
    "            if PRIMARY_METRIC == 'rmse':\n",
    "                loss = rmse\n",
    "            elif PRIMARY_METRIC == 'r2':\n",
    "                loss = -r2 \n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported PRIMARY_METRIC for loss calculation: {PRIMARY_METRIC}\")\n",
    "\n",
    "            return {'loss': loss, 'status': STATUS_OK, 'run_id': trial_run.info.run_id, \n",
    "                    'attachments': {'rmse': rmse, 'r2': r2, 'mae': mae, 'model_type': CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE}}\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message_short = str(e)[:250] # MLflow param limit\n",
    "            mlflow.log_param(\"error\", error_message_short)\n",
    "            mlflow.set_tag(\"status\", \"failed\")\n",
    "            print(f\"TRIAL ERROR in run {trial_run.info.run_id} for model {CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE}: {e}\")\n",
    "            # Ensure a sensible worst loss is returned\n",
    "            worst_loss = float('inf') if PRIMARY_METRIC == 'rmse' else float('inf') # if minimizing -r2, larger positive is worse\n",
    "            return {'loss': worst_loss, 'status': 'fail', 'run_id': trial_run.info.run_id, \n",
    "                    'error_message': error_message_short}\n",
    "\n",
    "print(\"Objective function defined.\")\n",
    "\n",
    "\n",
    "# --- OOF Generation and Final Model Training Function ---\n",
    "def train_final_model_and_generate_oof(model_type, best_hyperparams,\n",
    "                                     train_data_path, test_data_path, label_col_name,\n",
    "                                     k_folds, seed, mlflow_parent_run_name_prefix):\n",
    "    \"\"\"\n",
    "    Trains a final model with best HPs, generates OOF predictions on train set\n",
    "    and predictions on test set. Logs everything to MLflow.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=f\"{mlflow_parent_run_name_prefix}_{model_type}_OOF_Final\", nested=False) as oof_parent_run:\n",
    "        mlflow.log_params(best_hyperparams)\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        mlflow.log_param(\"k_folds_for_oof\", k_folds)\n",
    "        mlflow.set_tag(\"seed\", seed)\n",
    "\n",
    "        final_model_run_id = oof_parent_run.info.run_id\n",
    "        print(f\"Starting OOF generation and final model training for {model_type}. MLflow Run ID: {final_model_run_id}\")\n",
    "\n",
    "        try:\n",
    "            X_full_train, y_full_train, X_test, y_test = load_processed_data_for_sklearn(\n",
    "                train_data_path, test_data_path, label_col_name\n",
    "            )\n",
    "\n",
    "            oof_predictions = np.zeros(len(y_full_train))\n",
    "            test_predictions_from_folds = np.zeros((len(y_test), k_folds)) # To average later\n",
    "\n",
    "            kf = KFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "            for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_full_train, y_full_train)):\n",
    "                print(f\"  Processing Fold {fold_num+1}/{k_folds} for {model_type}...\")\n",
    "                X_fold_train, X_fold_val = X_full_train[train_idx], X_full_train[val_idx]\n",
    "                y_fold_train, y_fold_val = y_full_train[train_idx], y_full_train[val_idx]\n",
    "\n",
    "                model_fold = None\n",
    "                if model_type == 'decision_tree': model = DecisionTreeRegressor(**best_hyperparams, random_state=seed)\n",
    "                elif model_type == 'random_forest': model = RandomForestRegressor(**best_hyperparams, random_state=seed, n_jobs=-1)\n",
    "                elif model_type == 'extra_trees': model = ExtraTreesRegressor(**best_hyperparams, random_state=seed, n_jobs=-1)\n",
    "                elif model_type == 'lightgbm': model = lgb.LGBMRegressor(**best_hyperparams, random_state=seed, n_jobs=-1, verbose=-1)\n",
    "                else: raise ValueError(f\"Unsupported model type for OOF: {model_type}\")\n",
    "\n",
    "                model.fit(X_fold_train, y_fold_train)\n",
    "                oof_predictions[val_idx] = model.predict(X_fold_val)\n",
    "                test_predictions_from_folds[:, fold_num] = model.predict(X_test)\n",
    "            \n",
    "            # Average test predictions from all folds\n",
    "            final_test_predictions = np.mean(test_predictions_from_folds, axis=1)\n",
    "\n",
    "            # Calculate OOF metrics\n",
    "            oof_rmse = np.sqrt(mean_squared_error(y_full_train, oof_predictions))\n",
    "            oof_r2 = r2_score(y_full_train, oof_predictions)\n",
    "            mlflow.log_metric(\"oof_rmse\", oof_rmse)\n",
    "            mlflow.log_metric(\"oof_r2\", oof_r2)\n",
    "            print(f\"  {model_type} OOF RMSE: {oof_rmse:.4f}, OOF R2: {oof_r2:.4f}\")\n",
    "\n",
    "            # Save OOF and Test predictions as artifacts (and to DBFS for ensembling)\n",
    "            oof_df = pd.DataFrame({'true_label': y_full_train, f'oof_pred_{model_type}': oof_predictions})\n",
    "            test_preds_df = pd.DataFrame({'true_label': y_test, f'test_pred_{model_type}': final_test_predictions}) # Store true_label if available for test\n",
    "\n",
    "            oof_file_path_parquet = os.path.join(DBFS_UC_OOF_PREDS_DIR, f\"oof_preds_{model_type}.parquet\")\n",
    "            test_preds_file_path_parquet = os.path.join(DBFS_UC_TEST_PREDS_DIR, f\"test_preds_{model_type}.parquet\")\n",
    "            \n",
    "            oof_df.to_parquet(oof_file_path_parquet, index=False)\n",
    "            test_preds_df.to_parquet(test_preds_file_path_parquet, index=False)\n",
    "\n",
    "            mlflow.log_artifact(oof_file_path_parquet)\n",
    "            mlflow.log_artifact(test_preds_file_path_parquet)\n",
    "            mlflow.set_tag(f\"oof_preds_path_{model_type}\", oof_file_path_parquet.replace(\"/dbfs\",\"dbfs:\")) # Log path for reference\n",
    "            mlflow.set_tag(f\"test_preds_path_{model_type}\", test_preds_file_path_parquet.replace(\"/dbfs\",\"dbfs:\"))\n",
    "\n",
    "\n",
    "            # Train final model on ALL training data\n",
    "            print(f\"  Training final {model_type} model on all training data...\")\n",
    "            final_model = None\n",
    "            if model_type == 'decision_tree': final_model = DecisionTreeRegressor(**best_hyperparams, random_state=seed)\n",
    "            elif model_type == 'random_forest': final_model = RandomForestRegressor(**best_hyperparams, random_state=seed, n_jobs=-1)\n",
    "            elif model_type == 'extra_trees': final_model = ExtraTreesRegressor(**best_hyperparams, random_state=seed, n_jobs=-1)\n",
    "            elif model_type == 'lightgbm': final_model = lgb.LGBMRegressor(**best_hyperparams, random_state=seed, n_jobs=-1, verbose=-1)\n",
    "            else: raise ValueError(f\"Unsupported model type for final training: {model_type}\")\n",
    "\n",
    "            final_model.fit(X_full_train, y_full_train)\n",
    "\n",
    "            # Evaluate final model on test set\n",
    "            final_model_test_preds = final_model.predict(X_test) # These should be similar to final_test_predictions\n",
    "            final_model_rmse = np.sqrt(mean_squared_error(y_test, final_model_test_preds))\n",
    "            final_model_r2 = r2_score(y_test, final_model_test_preds)\n",
    "            final_model_mae = mean_absolute_error(y_test, final_model_test_preds)\n",
    "\n",
    "            mlflow.log_metric(\"final_model_test_rmse\", final_model_rmse)\n",
    "            mlflow.log_metric(\"final_model_test_r2\", final_model_r2)\n",
    "            mlflow.log_metric(\"final_model_test_mae\", final_model_mae)\n",
    "            print(f\"  {model_type} Final Model Test RMSE: {final_model_rmse:.4f}, R2: {final_model_r2:.4f}\")\n",
    "            \n",
    "            # Log final model\n",
    "            if model_type == 'lightgbm':\n",
    "                mlflow.lightgbm.log_model(final_model, \"final_model\", signature=mlflow.models.infer_signature(X_test, pd.Series(final_model_test_preds, name=YOUR_LABEL_COLUMN_NAME)))\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(final_model, \"final_model\", signature=mlflow.models.infer_signature(X_test, pd.Series(final_model_test_preds, name=YOUR_LABEL_COLUMN_NAME)))\n",
    "            \n",
    "            mlflow.set_tag(\"status\", \"success_oof_final\")\n",
    "            return {\n",
    "                \"status\": \"success\", \"model_type\": model_type, \"final_model_run_id\": final_model_run_id,\n",
    "                \"oof_rmse\": oof_rmse, \"final_model_test_rmse\": final_model_rmse,\n",
    "                \"oof_predictions_path\": oof_file_path_parquet,\n",
    "                \"test_predictions_path\": test_preds_file_path_parquet\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during OOF/Final training for {model_type}: {e}\")\n",
    "            mlflow.set_tag(\"status\", \"failed_oof_final\")\n",
    "            mlflow.log_param(\"error_oof_final\", str(e)[:250])\n",
    "            return {\"status\": \"failed\", \"model_type\": model_type, \"error_message\": str(e)}\n",
    "\n",
    "print(\"OOF generation and final model training function defined.\")\n",
    "\n",
    "\n",
    "# --- Ensemble Functions ---\n",
    "def create_ensemble_features(base_model_types, oof_dir, test_preds_dir, label_col_name, train_data_path_for_true_labels):\n",
    "    \"\"\"Loads OOF and test predictions for base models to create meta-features.\"\"\"\n",
    "    all_oof_preds = []\n",
    "    all_test_preds = []\n",
    "    \n",
    "    # Load true labels for training set (needed for meta-learner training)\n",
    "    # This assumes the original train_pdf (or equivalent) is available or can be re-read\n",
    "    # For simplicity, let's assume we need to load it to get y_train.\n",
    "    # It's better if y_train was saved with OOF preds.\n",
    "    # The oof_df from train_final_model_and_generate_oof already contains 'true_label'\n",
    "    # We just need to merge them.\n",
    "    \n",
    "    y_train_true_df = None\n",
    "\n",
    "    for model_type in base_model_types:\n",
    "        oof_path = os.path.join(oof_dir, f\"oof_preds_{model_type}.parquet\")\n",
    "        test_path = os.path.join(test_preds_dir, f\"test_preds_{model_type}.parquet\")\n",
    "        \n",
    "        if not os.path.exists(oof_path) or not os.path.exists(test_path):\n",
    "            print(f\"Warning: Prediction files for {model_type} not found. Skipping for ensemble.\")\n",
    "            continue\n",
    "            \n",
    "        oof_pdf = pd.read_parquet(oof_path)\n",
    "        test_pdf = pd.read_parquet(test_path)\n",
    "        \n",
    "        # Capture y_train_true from the first OOF file if not already done\n",
    "        if y_train_true_df is None and 'true_label' in oof_pdf.columns:\n",
    "            y_train_true_df = oof_pdf[['true_label']].copy()\n",
    "            # Make sure index aligns if we plan to concat later, or just use the values\n",
    "            \n",
    "        all_oof_preds.append(oof_pdf[[f'oof_pred_{model_type}']])\n",
    "        all_test_preds.append(test_pdf[[f'test_pred_{model_type}']])\n",
    "\n",
    "    if not all_oof_preds or y_train_true_df is None:\n",
    "        print(\"Error: Not enough OOF predictions to build ensemble features, or true labels missing.\")\n",
    "        return None, None, None\n",
    "\n",
    "    X_meta_train = pd.concat(all_oof_preds, axis=1)\n",
    "    X_meta_test = pd.concat(all_test_preds, axis=1)\n",
    "    y_meta_train = y_train_true_df['true_label'].values\n",
    "    \n",
    "    return X_meta_train, y_meta_train, X_meta_test\n",
    "\n",
    "\n",
    "def train_stacked_ensemble(meta_learner_name, meta_learner_model, \n",
    "                           X_meta_train, y_meta_train, X_meta_test, y_true_test, # y_true_test needed for evaluation\n",
    "                           mlflow_parent_run_name_prefix, seed, primary_metric_config, max_metrics_config):\n",
    "    \"\"\"Trains a stacked ensemble meta-learner.\"\"\"\n",
    "    with mlflow.start_run(run_name=f\"{mlflow_parent_run_name_prefix}_Stacked_{meta_learner_name}\", nested=False) as stack_run:\n",
    "        mlflow.log_param(\"meta_learner_type\", meta_learner_name)\n",
    "        mlflow.set_tag(\"ensemble_type\", \"stacking\")\n",
    "        mlflow.set_tag(\"seed\", seed)\n",
    "\n",
    "        print(f\"Training Stacked Ensemble with Meta-Learner: {meta_learner_name}\")\n",
    "        try:\n",
    "            meta_learner_model.fit(X_meta_train, y_meta_train)\n",
    "            \n",
    "            # It's good practice to log the meta-learner's own parameters if it's configurable\n",
    "            if hasattr(meta_learner_model, 'get_params'):\n",
    "                 mlflow.log_params({f\"meta_{k}\":v for k,v in meta_learner_model.get_params().items() if isinstance(v, (str, int, float, bool))})\n",
    "\n",
    "\n",
    "            stacked_test_predictions = meta_learner_model.predict(X_meta_test)\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(y_true_test, stacked_test_predictions))\n",
    "            r2 = r2_score(y_true_test, stacked_test_predictions)\n",
    "            mae = mean_absolute_error(y_true_test, stacked_test_predictions)\n",
    "            \n",
    "            metrics_to_log = {\"stacked_rmse\": rmse, \"stacked_r2\": r2, \"stacked_mae\": mae}\n",
    "            logged_metrics_count = 0\n",
    "            for m_name, m_val in sorted(metrics_to_log.items()):\n",
    "                if logged_metrics_count < max_metrics_config:\n",
    "                    mlflow.log_metric(m_name, m_val)\n",
    "                    logged_metrics_count +=1\n",
    "\n",
    "            print(f\"  Stacked ({meta_learner_name}) Test RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "            # Log the meta-learner model\n",
    "            # This requires a PyFunc model if you want to package base models + meta for inference\n",
    "            # For now, just log the meta-learner itself. Productionizing stacker is more complex.\n",
    "            if isinstance(meta_learner_model, lgb.LGBMRegressor):\n",
    "                 mlflow.lightgbm.log_model(meta_learner_model, f\"meta_learner_{meta_learner_name}\", signature=mlflow.models.infer_signature(X_meta_test, pd.Series(stacked_test_predictions, name=YOUR_LABEL_COLUMN_NAME)))\n",
    "            else:\n",
    "                 mlflow.sklearn.log_model(meta_learner_model, f\"meta_learner_{meta_learner_name}\", signature=mlflow.models.infer_signature(X_meta_test, pd.Series(stacked_test_predictions, name=YOUR_LABEL_COLUMN_NAME)))\n",
    "\n",
    "            mlflow.set_tag(\"status\", \"success_stacking\")\n",
    "            return {\"status\": \"success\", \"meta_learner\": meta_learner_name, \"rmse\": rmse, \"r2\": r2, \"run_id\": stack_run.info.run_id}\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR training stacked ensemble ({meta_learner_name}): {e}\")\n",
    "            mlflow.set_tag(\"status\", \"failed_stacking\")\n",
    "            mlflow.log_param(\"error_stacking\", str(e)[:250])\n",
    "            return {\"status\": \"failed\", \"meta_learner\": meta_learner_name, \"error_message\": str(e)}\n",
    "\n",
    "\n",
    "def calculate_weighted_ensemble(base_model_metrics_oof, # List of dicts: [{'model_type':'rf', 'oof_rmse':0.5, 'test_pred_path':'...'}, ...]\n",
    "                                X_meta_test, y_true_test,\n",
    "                                primary_metric_config, max_metrics_config,\n",
    "                                mlflow_parent_run_name_prefix):\n",
    "    \"\"\"Calculates and evaluates a weighted ensemble.\"\"\"\n",
    "    with mlflow.start_run(run_name=f\"{mlflow_parent_run_name_prefix}_WeightedEnsemble\", nested=False) as weighted_run:\n",
    "        mlflow.set_tag(\"ensemble_type\", \"weighted_average\")\n",
    "        print(\"Calculating Weighted Ensemble...\")\n",
    "\n",
    "        try:\n",
    "            weights = []\n",
    "            if primary_metric_config == 'rmse': # Lower RMSE is better, so inverse for weight\n",
    "                total_inverse_rmse = sum(1.0 / m['oof_rmse'] for m in base_model_metrics_oof if m['oof_rmse'] > 0)\n",
    "                if total_inverse_rmse == 0: \n",
    "                    print(\"Warning: Sum of inverse OOF RMSE is zero, cannot calculate RMSE-based weights. Defaulting to equal weights.\")\n",
    "                    weights = [1.0 / len(base_model_metrics_oof)] * len(base_model_metrics_oof)\n",
    "                else:\n",
    "                    weights = [(1.0 / m['oof_rmse']) / total_inverse_rmse for m in base_model_metrics_oof]\n",
    "            \n",
    "            elif primary_metric_config == 'r2': # Higher R2 is better\n",
    "                # Ensure R2 values are positive for weighting, shift if necessary or use rank\n",
    "                # For simplicity, let's assume R2 values are mostly > 0. Normalize positive R2s.\n",
    "                # A more robust method might be softmax of R2 scores or rank-based weighting.\n",
    "                positive_r2s = [max(0, m['oof_r2']) for m in base_model_metrics_oof] # Cap at 0\n",
    "                total_r2 = sum(positive_r2s)\n",
    "                if total_r2 == 0:\n",
    "                    print(\"Warning: Sum of positive OOF R2 is zero, cannot calculate R2-based weights. Defaulting to equal weights.\")\n",
    "                    weights = [1.0 / len(base_model_metrics_oof)] * len(base_model_metrics_oof)\n",
    "                else:\n",
    "                    weights = [r2 / total_r2 for r2 in positive_r2s]\n",
    "            else: # Default to equal weights if metric unknown for weighting\n",
    "                print(f\"Warning: Unknown primary metric '{primary_metric_config}' for weighting. Defaulting to equal weights.\")\n",
    "                weights = [1.0 / len(base_model_metrics_oof)] * len(base_model_metrics_oof)\n",
    "\n",
    "            mlflow.log_param(\"weighting_strategy\", f\"based_on_oof_{primary_metric_config}\")\n",
    "            for i, model_info in enumerate(base_model_metrics_oof):\n",
    "                mlflow.log_param(f\"weight_{model_info['model_type']}\", weights[i])\n",
    "                mlflow.log_metric(f\"oof_metric_for_weight_{model_info['model_type']}\", model_info['oof_rmse'] if primary_metric_config == 'rmse' else model_info['oof_r2'])\n",
    "\n",
    "            # Combine test predictions using weights\n",
    "            # X_meta_test is already a DataFrame of test_pred_algo1, test_pred_algo2, ...\n",
    "            weighted_predictions = np.zeros(len(X_meta_test))\n",
    "            if X_meta_test.shape[1] != len(weights):\n",
    "                raise ValueError(f\"Mismatch between number of models for weighting ({len(weights)}) and available test predictions ({X_meta_test.shape[1]})\")\n",
    "\n",
    "            for i, col in enumerate(X_meta_test.columns): # Assumes columns are in same order as base_model_metrics_oof\n",
    "                weighted_predictions += X_meta_test[col] * weights[i]\n",
    "            \n",
    "            rmse = np.sqrt(mean_squared_error(y_true_test, weighted_predictions))\n",
    "            r2 = r2_score(y_true_test, weighted_predictions)\n",
    "            mae = mean_absolute_error(y_true_test, weighted_predictions)\n",
    "\n",
    "            metrics_to_log = {\"weighted_rmse\": rmse, \"weighted_r2\": r2, \"weighted_mae\": mae}\n",
    "            logged_metrics_count = 0\n",
    "            for m_name, m_val in sorted(metrics_to_log.items()):\n",
    "                if logged_metrics_count < max_metrics_config:\n",
    "                    mlflow.log_metric(m_name, m_val)\n",
    "                    logged_metrics_count +=1\n",
    "\n",
    "            print(f\"  Weighted Ensemble Test RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "            mlflow.set_tag(\"status\", \"success_weighted\")\n",
    "            return {\"status\": \"success\", \"rmse\": rmse, \"r2\": r2, \"run_id\": weighted_run.info.run_id}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR calculating weighted ensemble: {e}\")\n",
    "            mlflow.set_tag(\"status\", \"failed_weighted\")\n",
    "            mlflow.log_param(\"error_weighted\", str(e)[:250])\n",
    "            return {\"status\": \"failed\", \"error_message\": str(e)}\n",
    "\n",
    "print(\"Ensemble functions defined.\")\n",
    "print(\"--- All Utility Functions and HPO Components Defined ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f3636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main Orchestration Logic\n",
    "\n",
    "print(\"--- Starting Main Orchestration ---\")\n",
    "\n",
    "# --- 0. Setup MLflow Experiment ---\n",
    "# Experiment ID is fetched/created using the global EXPERIMENT_PATH\n",
    "# This needs to be done once.\n",
    "try:\n",
    "    # Make sure spark session from Init cell is used if get_or_create_experiment needs it\n",
    "    experiment_id = get_or_create_experiment(EXPERIMENT_PATH, spark)\n",
    "    if experiment_id:\n",
    "        mlflow.set_experiment(experiment_id=experiment_id)\n",
    "        print(f\"MLflow experiment '{EXPERIMENT_PATH}' is set with ID: {experiment_id}\")\n",
    "    else:\n",
    "        raise Exception(\"MLflow experiment could not be set. Halting.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL: Could not initialize MLflow experiment. Please check EXPERIMENT_PATH and permissions. Error: {e}\")\n",
    "    # dbutils.notebook.exit(\"MLflow experiment setup failed\") # If in Databricks notebook and want to halt\n",
    "\n",
    "# This is a global variable that the objective function will read\n",
    "# It will be updated for each algorithm's HPO campaign.\n",
    "global HPO_PARENT_RUN_ID_FOR_OBJECTIVE\n",
    "global CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE\n",
    "\n",
    "\n",
    "# --- 1. Individual HPO for each Base Algorithm (Sequential) ---\n",
    "print(\"\\n--- Phase X.A: Individual Hyperparameter Optimization for Base Models ---\")\n",
    "best_hpo_results_per_algorithm = {} # To store best HPs and trial run_id for each algo\n",
    "\n",
    "for algo_type in BASE_ALGORITHMS_TO_RUN:\n",
    "    print(f\"\\nStarting HPO for Algorithm: {algo_type}...\")\n",
    "    if algo_type not in ALGORITHM_SEARCH_SPACES:\n",
    "        print(f\"Warning: Search space for {algo_type} not defined. Skipping HPO.\")\n",
    "        continue\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"HPO_Campaign_{algo_type}\", nested=False) as hpo_campaign_parent_run:\n",
    "        HPO_PARENT_RUN_ID_FOR_OBJECTIVE = hpo_campaign_parent_run.info.run_id\n",
    "        CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE = algo_type # Set for the objective function\n",
    "\n",
    "        mlflow.log_param(\"algorithm_being_optimized\", algo_type)\n",
    "        mlflow.log_param(\"num_hpo_trials_config\", NUM_HPO_TRIALS)\n",
    "        mlflow.log_param(\"primary_metric_config\", PRIMARY_METRIC)\n",
    "        mlflow.log_param(\"global_seed\", GLOBAL_SEED)\n",
    "        # Log the specific search space for this algorithm if possible (can be complex for hp objects)\n",
    "        # mlflow.log_dict(ALGORITHM_SEARCH_SPACES[algo_type]['model_params'], f\"search_space_{algo_type}.json\")\n",
    "\n",
    "\n",
    "        hpo_trials_database = Trials() # Sequential trials\n",
    "\n",
    "        try:\n",
    "            # The objective function now implicitly uses CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE\n",
    "            # and HPO_PARENT_RUN_ID_FOR_OBJECTIVE.\n",
    "            # It also uses global vars for data paths, label col, seed, primary metric.\n",
    "            \n",
    "            # The search space for `fmin` should be just the 'model_params' part\n",
    "            current_search_space = ALGORITHM_SEARCH_SPACES[algo_type]['model_params']\n",
    "\n",
    "            best_hyperopt_indices = fmin(\n",
    "                fn=objective_function_regression, # This now gets hyperparams for the current_algo_type\n",
    "                space=current_search_space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=NUM_HPO_TRIALS,\n",
    "                trials=hpo_trials_database,\n",
    "                rstate=np.random.default_rng(GLOBAL_SEED) # For hyperopt's internal randomness\n",
    "            )\n",
    "            \n",
    "            best_actual_params = space_eval(current_search_space, best_hyperopt_indices)\n",
    "            \n",
    "            # Find the best trial from the hpo_trials_database\n",
    "            best_trial_obj = hpo_trials_database.best_trial\n",
    "            best_trial_run_id = None\n",
    "            best_trial_loss = float('inf')\n",
    "            best_trial_attachments = {}\n",
    "\n",
    "            if best_trial_obj and best_trial_obj['result']['status'] == STATUS_OK:\n",
    "                best_trial_run_id = best_trial_obj['result'].get('run_id')\n",
    "                best_trial_loss = best_trial_obj['result']['loss']\n",
    "                best_trial_attachments = best_trial_obj['result'].get('attachments', {})\n",
    "                \n",
    "                print(f\"  Best HPO trial for {algo_type}: Loss={best_trial_loss:.4f}, Params={best_actual_params}, MLflow Run ID={best_trial_run_id}\")\n",
    "                mlflow.log_params({f\"best_hpo_{k}\": v for k,v in best_actual_params.items()})\n",
    "                mlflow.log_metric(\"best_hpo_loss\", best_trial_loss)\n",
    "                if best_trial_run_id:\n",
    "                    mlflow.set_tag(\"best_hpo_trial_run_id\", best_trial_run_id)\n",
    "                for att_k, att_v in best_trial_attachments.items():\n",
    "                     if isinstance(att_v, (int, float)) and att_k != \"model_type\": # model_type is already a param\n",
    "                        mlflow.log_metric(f\"best_hpo_trial_{att_k}\", att_v)\n",
    "\n",
    "\n",
    "                best_hpo_results_per_algorithm[algo_type] = {\n",
    "                    \"best_params\": best_actual_params,\n",
    "                    \"best_trial_run_id\": best_trial_run_id, # This is the run for the specific trial\n",
    "                    \"hpo_campaign_run_id\": HPO_PARENT_RUN_ID_FOR_OBJECTIVE,\n",
    "                    \"attachments\": best_trial_attachments\n",
    "                }\n",
    "                mlflow.set_tag(\"status_hpo_campaign\", \"success\")\n",
    "            else:\n",
    "                print(f\"  HPO for {algo_type} did not yield a successful best trial.\")\n",
    "                mlflow.set_tag(\"status_hpo_campaign\", \"no_successful_trial\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during HPO campaign for {algo_type}: {e}\")\n",
    "            mlflow.set_tag(\"status_hpo_campaign\", \"failed\")\n",
    "            mlflow.log_param(\"error_hpo_campaign\", str(e)[:250])\n",
    "\n",
    "# --- 2. OOF Generation & Final Base Model Training (Sequential) ---\n",
    "print(\"\\n--- Phase X.B: OOF Prediction Generation & Final Base Model Training ---\")\n",
    "final_base_model_details = {} # To store info about final models and their prediction paths\n",
    "\n",
    "for algo_type, hpo_result in best_hpo_results_per_algorithm.items():\n",
    "    if hpo_result and hpo_result.get(\"best_params\"):\n",
    "        print(f\"\\nGenerating OOF & Final Model for: {algo_type} with best params: {hpo_result['best_params']}\")\n",
    "        \n",
    "        oof_result = train_final_model_and_generate_oof(\n",
    "            model_type=algo_type,\n",
    "            best_hyperparams=hpo_result['best_params'],\n",
    "            train_data_path=SHARED_PROCESSED_TRAIN_PATH,\n",
    "            test_data_path=SHARED_PROCESSED_TEST_PATH,\n",
    "            label_col_name=YOUR_LABEL_COLUMN_NAME,\n",
    "            k_folds=K_FOLDS_OOF,\n",
    "            seed=GLOBAL_SEED,\n",
    "            mlflow_parent_run_name_prefix=\"MVP\" # To distinguish these runs\n",
    "        )\n",
    "        if oof_result['status'] == 'success':\n",
    "            final_base_model_details[algo_type] = oof_result\n",
    "            print(f\"  Successfully generated OOF and final model for {algo_type}.\")\n",
    "        else:\n",
    "            print(f\"  Failed to generate OOF/final model for {algo_type}: {oof_result.get('error_message')}\")\n",
    "    else:\n",
    "        print(f\"Skipping OOF for {algo_type} as no successful HPO result was found.\")\n",
    "\n",
    "\n",
    "# --- 3. Ensemble Creation ---\n",
    "print(\"\\n--- Phase X.C: Ensemble Creation ---\")\n",
    "if not final_base_model_details:\n",
    "    print(\"No base models successfully processed for OOF. Skipping ensemble creation.\")\n",
    "else:\n",
    "    # Prepare data for ensembling\n",
    "    # We need y_true_test for evaluating ensembles. Let's load it once.\n",
    "    try:\n",
    "        _, _, _, y_true_test_for_ensemble = load_processed_data_for_sklearn(\n",
    "            SHARED_PROCESSED_TRAIN_PATH, SHARED_PROCESSED_TEST_PATH, YOUR_LABEL_COLUMN_NAME\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL: Could not load test labels for ensemble evaluation. Error: {e}. Skipping ensembles.\")\n",
    "        y_true_test_for_ensemble = None\n",
    "\n",
    "    if y_true_test_for_ensemble is not None:\n",
    "        ensemble_base_model_types = list(final_base_model_details.keys())\n",
    "        \n",
    "        # This creates X_meta_train, y_meta_train, X_meta_test\n",
    "        # y_meta_train comes from the 'true_label' column in the OOF parquet files\n",
    "        meta_features_tuple = create_ensemble_features(\n",
    "            base_model_types=ensemble_base_model_types,\n",
    "            oof_dir=DBFS_UC_OOF_PREDS_DIR, # Use DBFS path for pandas\n",
    "            test_preds_dir=DBFS_UC_TEST_PREDS_DIR, # Use DBFS path for pandas\n",
    "            label_col_name=YOUR_LABEL_COLUMN_NAME,\n",
    "            train_data_path_for_true_labels=SHARED_PROCESSED_TRAIN_PATH # Only to get y_train if not in OOF df\n",
    "        )\n",
    "\n",
    "        if meta_features_tuple and meta_features_tuple[0] is not None:\n",
    "            X_meta_train, y_meta_train, X_meta_test = meta_features_tuple\n",
    "            print(f\"  Meta features created for stacking: X_meta_train shape {X_meta_train.shape}, X_meta_test shape {X_meta_test.shape}\")\n",
    "\n",
    "            # --- 3.A Weighted Ensemble ---\n",
    "            print(\"\\n  Creating Weighted Ensemble...\")\n",
    "            # Need to gather OOF metrics for weighting\n",
    "            base_model_oof_metrics_for_weighting = []\n",
    "            for algo, details in final_base_model_details.items():\n",
    "                if details['status'] == 'success':\n",
    "                    base_model_oof_metrics_for_weighting.append({\n",
    "                        'model_type': algo,\n",
    "                        'oof_rmse': details['oof_rmse'], # This was logged from OOF calculation\n",
    "                        'oof_r2': details.get('oof_r2', 1.0 - (details['oof_rmse']**2 / np.var(y_meta_train) if np.var(y_meta_train) > 0 else 0) ), # Estimate if not directly available, or log it directly\n",
    "                        'test_pred_path': details['test_predictions_path'] # Not used by current weighted function, but good to have\n",
    "                    })\n",
    "            \n",
    "            if base_model_oof_metrics_for_weighting:\n",
    "                weighted_ensemble_result = calculate_weighted_ensemble(\n",
    "                    base_model_metrics_oof=base_model_oof_metrics_for_weighting,\n",
    "                    X_meta_test=X_meta_test.copy(), # X_meta_test has columns like 'test_pred_decision_tree', etc.\n",
    "                    y_true_test=y_true_test_for_ensemble,\n",
    "                    primary_metric_config=PRIMARY_METRIC,\n",
    "                    max_metrics_config=MAX_METRICS_TO_LOG,\n",
    "                    mlflow_parent_run_name_prefix=\"MVP\"\n",
    "                )\n",
    "                if weighted_ensemble_result['status'] == 'success':\n",
    "                    print(f\"  Weighted Ensemble Test RMSE: {weighted_ensemble_result['rmse']:.4f}, R2: {weighted_ensemble_result['r2']:.4f}\")\n",
    "            else:\n",
    "                print(\"  Not enough successful base models with OOF metrics to create weighted ensemble.\")\n",
    "\n",
    "\n",
    "            # --- 3.B Stacked Ensemble ---\n",
    "            print(\"\\n  Creating Stacked Ensembles...\")\n",
    "            for meta_learner_key, meta_learner_instance in META_LEARNERS_FOR_STACKING.items():\n",
    "                print(f\"    Stacking with Meta-Learner: {meta_learner_key}\")\n",
    "                stacking_result = train_stacked_ensemble(\n",
    "                    meta_learner_name=meta_learner_key,\n",
    "                    meta_learner_model=meta_learner_instance, # Pass the actual model instance\n",
    "                    X_meta_train=X_meta_train.copy(),\n",
    "                    y_meta_train=y_meta_train.copy(),\n",
    "                    X_meta_test=X_meta_test.copy(),\n",
    "                    y_true_test=y_true_test_for_ensemble.copy(),\n",
    "                    mlflow_parent_run_name_prefix=\"MVP\",\n",
    "                    seed=GLOBAL_SEED,\n",
    "                    primary_metric_config=PRIMARY_METRIC, # Not directly used by stacker HPO, but good for consistency\n",
    "                    max_metrics_config=MAX_METRICS_TO_LOG\n",
    "                )\n",
    "                if stacking_result['status'] == 'success':\n",
    "                    print(f\"    Stacked Ensemble ({meta_learner_key}) Test RMSE: {stacking_result['rmse']:.4f}, R2: {stacking_result['r2']:.4f}\")\n",
    "                else:\n",
    "                    print(f\"    Failed to train Stacked Ensemble ({meta_learner_key}): {stacking_result.get('error_message')}\")\n",
    "        else:\n",
    "            print(\"  Failed to create meta-features for stacking. Skipping.\")\n",
    "    else:\n",
    "        print(\"Skipping ensemble creation due to failure in loading test labels for evaluation.\")\n",
    "\n",
    "print(\"\\n--- Main Orchestration Completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca82cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: (Optional) Clean up temporary files from DBFS if created directly by pandas\n",
    "# Note: Spark writes (like .write.parquet) to UC Volumes are managed by Spark.\n",
    "# If pandas created files via /dbfs/ paths, you might want to clean them.\n",
    "# Example:\n",
    "# try:\n",
    "#    if os.path.exists(DBFS_UC_OOF_PREDS_DIR):\n",
    "#        print(f\"Cleaning up OOF predictions directory: {DBFS_UC_OOF_PREDS_DIR}\")\n",
    "#        shutil.rmtree(DBFS_UC_OOF_PREDS_DIR) # Careful with this!\n",
    "#    if os.path.exists(DBFS_UC_TEST_PREDS_DIR):\n",
    "#        print(f\"Cleaning up test predictions directory: {DBFS_UC_TEST_PREDS_DIR}\")\n",
    "#        shutil.rmtree(DBFS_UC_TEST_PREDS_DIR)\n",
    "# except Exception as e:\n",
    "#    print(f\"Error during cleanup: {e}\")\n",
    "\n",
    "# If spark session is no longer needed by other cells:\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e8b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-------------------- CELL 3: UTILITY FUNCTIONS & HPO/MODEL TRAINING COMPONENTS (Updated for Parquet Input) -------------------->\n",
    "print(\"\\nCell 3: Utility Functions & HPO/Model Training Components - Defining (Updated for Parquet Input)...\")\n",
    "\n",
    "# --- MLflow Utility (get_or_create_experiment - assuming this is defined as before) ---\n",
    "# def get_or_create_experiment(experiment_name_param, spark_session_param): ... (from previous full code response)\n",
    "\n",
    "# --- Algorithm Search Spaces (ALGORITHM_SEARCH_SPACES - assuming this is defined as before for regression) ---\n",
    "# ALGORITHM_SEARCH_SPACES = { 'decision_tree': {...}, 'random_forest': {...}, ... } (from previous response)\n",
    "\n",
    "# Global variables for the objective function (will be set by the HPO orchestrator loop)\n",
    "global HPO_PARENT_RUN_ID_FOR_OBJECTIVE\n",
    "global CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE\n",
    "# Other global variables from HPO/Training Init Cell (Cell 2) will be used directly:\n",
    "# SHARED_PROCESSED_TRAIN_PATH (now path to named-cols Parquet for train)\n",
    "# SHARED_PROCESSED_TEST_PATH (now path to named-cols Parquet for test)\n",
    "# YOUR_LABEL_COLUMN_NAME\n",
    "# PRIMARY_METRIC, GLOBAL_SEED, MAX_METRICS_TO_LOG\n",
    "\n",
    "def objective_function_regression(hyperparams_from_hyperopt):\n",
    "    \"\"\"\n",
    "    Objective function for Hyperopt HPO (Regression).\n",
    "    Loads data from Parquet files with named, transformed columns.\n",
    "    Trains a specific regressor, logs to MLflow, returns loss.\n",
    "    \"\"\"\n",
    "    global HPO_PARENT_RUN_ID_FOR_OBJECTIVE, CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE\n",
    "    global SHARED_PROCESSED_TRAIN_PATH, SHARED_PROCESSED_TEST_PATH, YOUR_LABEL_COLUMN_NAME\n",
    "    global PRIMARY_METRIC, GLOBAL_SEED, MAX_METRICS_TO_LOG # From HPO Init cell\n",
    "\n",
    "    # Sanitize hyperparams (convert numpy types to native Python for model constructors)\n",
    "    sanitized_hyperparams = {}\n",
    "    for k, v in hyperparams_from_hyperopt.items():\n",
    "        if isinstance(v, np.generic): sanitized_hyperparams[k] = v.item()\n",
    "        elif k in ['max_depth'] and v is not None: sanitized_hyperparams[k] = int(v) if v is not None else None\n",
    "        elif k in ['min_samples_split', 'min_samples_leaf', 'n_estimators', 'num_leaves', 'iterations'] and v is not None:\n",
    "             sanitized_hyperparams[k] = int(v)\n",
    "        else: sanitized_hyperparams[k] = v\n",
    "    \n",
    "    trial_run_name = f\"Trial_{CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE}_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "    with mlflow.start_run(run_id=HPO_PARENT_RUN_ID_FOR_OBJECTIVE, run_name=trial_run_name, nested=True) as trial_run:\n",
    "        mlflow.log_param(\"model_type_trial\", CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE)\n",
    "        mlflow.log_params(sanitized_hyperparams)\n",
    "        mlflow.set_tag(\"seed\", GLOBAL_SEED)\n",
    "        mlflow.log_param(\"train_data_source_hpo\", SHARED_PROCESSED_TRAIN_PATH) # Log Parquet path\n",
    "        mlflow.log_param(\"test_data_source_hpo\", SHARED_PROCESSED_TEST_PATH)   # Log Parquet path\n",
    "\n",
    "        try:\n",
    "            # --- 1. Load preprocessed data from Named-Column Parquet files ---\n",
    "            # These paths (SHARED_PROCESSED_TRAIN_PATH, etc.) are now expected to be Parquet paths for Pandas.\n",
    "            # They should be /dbfs/ prefixed paths for direct Pandas access to UC Volumes.\n",
    "            print(f\"    ObjectiveFn: Loading train data from {SHARED_PROCESSED_TRAIN_PATH}\")\n",
    "            train_pdf_obj = pd.read_parquet(SHARED_PROCESSED_TRAIN_PATH)\n",
    "            X_train = train_pdf_obj.drop(columns=[YOUR_LABEL_COLUMN_NAME]).values # .values converts to NumPy\n",
    "            y_train = train_pdf_obj[YOUR_LABEL_COLUMN_NAME].astype(float).values\n",
    "            \n",
    "            print(f\"    ObjectiveFn: Loading test data from {SHARED_PROCESSED_TEST_PATH}\")\n",
    "            test_pdf_obj = pd.read_parquet(SHARED_PROCESSED_TEST_PATH)\n",
    "            # Test data might or might not have the label column, handle gracefully for X extraction\n",
    "            if YOUR_LABEL_COLUMN_NAME in test_pdf_obj.columns:\n",
    "                X_test = test_pdf_obj.drop(columns=[YOUR_LABEL_COLUMN_NAME]).values\n",
    "                y_test = test_pdf_obj[YOUR_LABEL_COLUMN_NAME].astype(float).values\n",
    "            else:\n",
    "                X_test = test_pdf_obj.values # Assume all columns are features if label is missing\n",
    "                y_test = None # Explicitly set to None if no label in test data\n",
    "                print(\"    ObjectiveFn: Test data does not contain the label column.\")\n",
    "\n",
    "            if y_test is None: # Critical for evaluation during HPO\n",
    "                raise ValueError(\"y_test is None after loading from Parquet. HPO requires test labels for evaluation. Ensure test Parquet file includes the label column or adjust HPO strategy (e.g., use CV).\")\n",
    "\n",
    "            print(f\"      ObjectiveFn: Loaded X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "\n",
    "            # --- 2. Instantiate and Train Model ---\n",
    "            model = None\n",
    "            if CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'decision_tree':\n",
    "                model = DecisionTreeRegressor(**sanitized_hyperparams, random_state=GLOBAL_SEED)\n",
    "            elif CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'random_forest':\n",
    "                model = RandomForestRegressor(**sanitized_hyperparams, random_state=GLOBAL_SEED, n_jobs=-1)\n",
    "            elif CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'extra_trees':\n",
    "                model = ExtraTreesRegressor(**sanitized_hyperparams, random_state=GLOBAL_SEED, n_jobs=-1)\n",
    "            elif CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'lightgbm':\n",
    "                model = lgb.LGBMRegressor(**sanitized_hyperparams, random_state=GLOBAL_SEED, n_jobs=-1, verbose=-1)\n",
    "            # Add other models (XGBoostRegressor, CatBoostRegressor) here when you include them\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model type: {CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE}\")\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "\n",
    "            # --- 3. Evaluate Model & Calculate Metrics ---\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "            r2 = r2_score(y_test, predictions)\n",
    "            mae = mean_absolute_error(y_test, predictions)\n",
    "            \n",
    "            metrics_to_log = {\"rmse\": rmse, \"r2\": r2, \"mae\": mae}\n",
    "            logged_metrics_count = 0\n",
    "            for m_name, m_val in sorted(metrics_to_log.items()):\n",
    "                if logged_metrics_count < MAX_METRICS_TO_LOG: # Global MAX_METRICS_TO_LOG\n",
    "                    mlflow.log_metric(m_name, m_val)\n",
    "                    logged_metrics_count +=1\n",
    "            \n",
    "            # --- 4. Log Model ---\n",
    "            model_signature = mlflow.models.infer_signature(X_test, pd.Series(predictions, name=YOUR_LABEL_COLUMN_NAME))\n",
    "            if CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'lightgbm':\n",
    "                mlflow.lightgbm.log_model(model, \"model\", signature=model_signature)\n",
    "            # Add elif for xgboost, catboost if they have specific log_model functions\n",
    "            else: # For scikit-learn compatible\n",
    "                mlflow.sklearn.log_model(model, \"model\", signature=model_signature)\n",
    "            \n",
    "            mlflow.set_tag(\"status\", \"success\")\n",
    "\n",
    "            # --- 5. Determine Loss for Hyperopt ---\n",
    "            loss = rmse if PRIMARY_METRIC == 'rmse' else -r2\n",
    "            return {'loss': loss, 'status': STATUS_OK, 'run_id': trial_run.info.run_id, \n",
    "                    'attachments': {'rmse': rmse, 'r2': r2, 'mae': mae, 'model_type': CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE}}\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message_short = str(e)[:250]\n",
    "            mlflow.log_param(\"error_objective_function\", error_message_short) # More specific error param\n",
    "            mlflow.set_tag(\"status\", \"failed\")\n",
    "            print(f\"TRIAL ERROR in run {trial_run.info.run_id if 'trial_run' in locals() and hasattr(trial_run, 'info') else 'UNKNOWN'} for model {CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc() # Print full traceback for easier debugging in driver logs\n",
    "            worst_loss = float('inf') # General worst loss for minimization\n",
    "            return {'loss': worst_loss, 'status': 'fail', \n",
    "                    'run_id': trial_run.info.run_id if 'trial_run' in locals() and hasattr(trial_run, 'info') else None, \n",
    "                    'error_message': error_message_short}\n",
    "\n",
    "print(\"Objective function (for Parquet input) defined.\")\n",
    "\n",
    "\n",
    "def train_final_model_and_generate_oof_parquet_input(\n",
    "                                     model_type_oof, best_hyperparams_oof,\n",
    "                                     train_parquet_path_oof, test_parquet_path_oof, label_col_name_oof,\n",
    "                                     k_folds_oof_val, seed_oof, mlflow_parent_run_name_prefix_oof,\n",
    "                                     oof_output_dir_dbfs_oof, test_preds_output_dir_dbfs_oof):\n",
    "    \"\"\"\n",
    "    Trains final model, generates OOF (on train) & test predictions from named-column Parquet inputs.\n",
    "    Saves OOF and Test predictions as Parquet files to specified /dbfs/ UC Volume output dirs.\n",
    "    \"\"\"\n",
    "    # Sanitize hyperparams for model constructors\n",
    "    sanitized_best_hyperparams = {}\n",
    "    for k, v in best_hyperparams_oof.items():\n",
    "        if isinstance(v, np.generic): sanitized_best_hyperparams[k] = v.item()\n",
    "        elif k in ['max_depth'] and v is not None: sanitized_best_hyperparams[k] = int(v) if v is not None else None\n",
    "        elif k in ['min_samples_split', 'min_samples_leaf', 'n_estimators', 'num_leaves', 'iterations'] and v is not None:\n",
    "             sanitized_best_hyperparams[k] = int(v)\n",
    "        else: sanitized_best_hyperparams[k] = v\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{mlflow_parent_run_name_prefix_oof}_{model_type_oof}_OOF_Final\", nested=False) as oof_parent_run:\n",
    "        mlflow.log_params(sanitized_best_hyperparams)\n",
    "        mlflow.log_param(\"model_type\", model_type_oof)\n",
    "        mlflow.log_param(\"k_folds_for_oof\", k_folds_oof_val)\n",
    "        mlflow.set_tag(\"seed\", seed_oof)\n",
    "        mlflow.log_param(\"train_data_source_oof\", train_parquet_path_oof)\n",
    "        mlflow.log_param(\"test_data_source_oof\", test_parquet_path_oof)\n",
    "\n",
    "        final_model_run_id = oof_parent_run.info.run_id\n",
    "        print(f\"Starting OOF & Final Model for {model_type_oof} from Parquet. MLflow Run ID: {final_model_run_id}\")\n",
    "\n",
    "        try:\n",
    "            # Load full preprocessed training data from Parquet\n",
    "            print(f\"  OOF: Loading full train data from {train_parquet_path_oof}\")\n",
    "            train_pdf_full = pd.read_parquet(train_parquet_path_oof)\n",
    "            X_full_train_pd = train_pdf_full.drop(columns=[label_col_name_oof])\n",
    "            y_full_train_series = train_pdf_full[label_col_name_oof].astype(float)\n",
    "            X_full_train_np = X_full_train_pd.values\n",
    "            y_full_train_np = y_full_train_series.values\n",
    "\n",
    "            # Load full preprocessed test data from Parquet\n",
    "            print(f\"  OOF: Loading full test data from {test_parquet_path_oof}\")\n",
    "            test_pdf_full = pd.read_parquet(test_parquet_path_oof)\n",
    "            X_test_pd = test_pdf_full.drop(columns=[label_col_name_oof], errors='ignore')\n",
    "            y_test_series = test_pdf_full[label_col_name_oof].astype(float) if label_col_name_oof in test_pdf_full.columns else None\n",
    "            X_test_np = X_test_pd.values\n",
    "            y_test_np = y_test_series.values if y_test_series is not None else None\n",
    "            \n",
    "            if y_test_np is None:\n",
    "                print(\"  OOF: Test data does not contain labels. Will only generate test predictions.\")\n",
    "\n",
    "\n",
    "            oof_predictions_np = np.zeros_like(y_full_train_np)\n",
    "            kf = KFold(n_splits=k_folds_oof_val, shuffle=True, random_state=seed_oof)\n",
    "\n",
    "            for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_full_train_np, y_full_train_np)):\n",
    "                print(f\"    OOF Fold {fold_num+1}/{k_folds_oof_val} for {model_type_oof}...\")\n",
    "                X_fold_train, X_fold_val = X_full_train_np[train_idx], X_full_train_np[val_idx]\n",
    "                y_fold_train = y_full_train_np[train_idx]\n",
    "\n",
    "                model_fold = None\n",
    "                if model_type_oof == 'decision_tree': model_fold = DecisionTreeRegressor(**sanitized_best_hyperparams, random_state=seed_oof)\n",
    "                elif model_type_oof == 'random_forest': model_fold = RandomForestRegressor(**sanitized_best_hyperparams, random_state=seed_oof, n_jobs=-1)\n",
    "                elif model_type_oof == 'extra_trees': model_fold = ExtraTreesRegressor(**sanitized_best_hyperparams, random_state=seed_oof, n_jobs=-1)\n",
    "                elif model_type_oof == 'lightgbm': model_fold = lgb.LGBMRegressor(**sanitized_best_hyperparams, random_state=seed_oof, n_jobs=-1, verbose=-1)\n",
    "                else: raise ValueError(f\"Unsupported model type for OOF: {model_type_oof}\")\n",
    "\n",
    "                model_fold.fit(X_fold_train, y_fold_train)\n",
    "                oof_predictions_np[val_idx] = model_fold.predict(X_fold_val)\n",
    "            \n",
    "            oof_rmse = np.sqrt(mean_squared_error(y_full_train_np, oof_predictions_np))\n",
    "            oof_r2 = r2_score(y_full_train_np, oof_predictions_np)\n",
    "            mlflow.log_metric(\"oof_rmse\", oof_rmse)\n",
    "            mlflow.log_metric(\"oof_r2\", oof_r2)\n",
    "            print(f\"    {model_type_oof} OOF RMSE: {oof_rmse:.4f}, OOF R2: {oof_r2:.4f}\")\n",
    "\n",
    "            # --- Train final model on ALL training data & predict on test set ---\n",
    "            print(f\"    Training final {model_type_oof} model on all training data...\")\n",
    "            final_model = None # Instantiate final model (as above)\n",
    "            if model_type_oof == 'decision_tree': final_model = DecisionTreeRegressor(**sanitized_best_hyperparams, random_state=seed_oof)\n",
    "            elif model_type_oof == 'random_forest': final_model = RandomForestRegressor(**sanitized_best_hyperparams, random_state=seed_oof, n_jobs=-1)\n",
    "            elif model_type_oof == 'extra_trees': final_model = ExtraTreesRegressor(**sanitized_best_hyperparams, random_state=seed_oof, n_jobs=-1)\n",
    "            elif model_type_oof == 'lightgbm': final_model = lgb.LGBMRegressor(**sanitized_best_hyperparams, random_state=seed_oof, n_jobs=-1, verbose=-1)\n",
    "            else: raise ValueError(f\"Unsupported model type for final training: {model_type_oof}\")\n",
    "            \n",
    "            final_model.fit(X_full_train_np, y_full_train_np)\n",
    "            final_model_test_predictions = final_model.predict(X_test_np)\n",
    "\n",
    "            # Save OOF and Test predictions as Parquet files\n",
    "            # OOF df should have original index (if any) and true labels + predictions\n",
    "            oof_df_to_save = train_pdf_full[[label_col_name_oof]].copy() # Start with true labels and original index\n",
    "            oof_df_to_save.rename(columns={label_col_name_oof: f\"true_{label_col_name_oof}\"}, inplace=True)\n",
    "            oof_df_to_save[f'oof_pred_{model_type_oof}'] = oof_predictions_np\n",
    "            \n",
    "            test_preds_df_to_save = test_pdf_full[[label_col_name_oof]].copy() if y_test_np is not None else pd.DataFrame(index=test_pdf_full.index)\n",
    "            if y_test_np is not None:\n",
    "                test_preds_df_to_save.rename(columns={label_col_name_oof: f\"true_{label_col_name_oof}\"}, inplace=True)\n",
    "            test_preds_df_to_save[f'test_pred_{model_type_oof}'] = final_model_test_predictions\n",
    "\n",
    "            os.makedirs(oof_output_dir_dbfs_oof, exist_ok=True)\n",
    "            os.makedirs(test_preds_output_dir_dbfs_oof, exist_ok=True)\n",
    "\n",
    "            oof_file_path = os.path.join(oof_output_dir_dbfs_oof, f\"oof_preds_{model_type_oof}.parquet\")\n",
    "            test_preds_file_path = os.path.join(test_preds_output_dir_dbfs_oof, f\"test_preds_{model_type_oof}.parquet\")\n",
    "            \n",
    "            oof_df_to_save.to_parquet(oof_file_path, index=True) # Save index if it's meaningful\n",
    "            test_preds_df_to_save.to_parquet(test_preds_file_path, index=True)\n",
    "\n",
    "            mlflow.log_artifact(oof_file_path, artifact_path=\"oof_predictions_parquet\")\n",
    "            mlflow.log_artifact(test_preds_file_path, artifact_path=\"test_predictions_parquet\")\n",
    "            mlflow.set_tag(f\"oof_preds_path_parquet_{model_type_oof}\", oof_file_path.replace(\"/dbfs\", \"dbfs:\"))\n",
    "            mlflow.set_tag(f\"test_preds_path_parquet_{model_type_oof}\", test_preds_file_path.replace(\"/dbfs\", \"dbfs:\"))\n",
    "\n",
    "            # Evaluate final model on test set (if y_test_np exists)\n",
    "            if y_test_np is not None:\n",
    "                final_model_rmse = np.sqrt(mean_squared_error(y_test_np, final_model_test_predictions))\n",
    "                final_model_r2 = r2_score(y_test_np, final_model_test_predictions)\n",
    "                final_model_mae = mean_absolute_error(y_test_np, final_model_test_predictions)\n",
    "                mlflow.log_metric(\"final_model_test_rmse\", final_model_rmse)\n",
    "                mlflow.log_metric(\"final_model_test_r2\", final_model_r2)\n",
    "                mlflow.log_metric(\"final_model_test_mae\", final_model_mae)\n",
    "                print(f\"    {model_type_oof} Final Model Test RMSE: {final_model_rmse:.4f}, R2: {final_model_r2:.4f}\")\n",
    "            else:\n",
    "                final_model_rmse, final_model_r2 = None, None # cannot calculate if no y_test true values\n",
    "                print(f\"    {model_type_oof} Final Model Test predictions generated; no true labels for metrics.\")\n",
    "\n",
    "            # Log final model\n",
    "            model_signature_final = mlflow.models.infer_signature(X_test_np, pd.Series(final_model_test_predictions, name=YOUR_LABEL_COLUMN_NAME))\n",
    "            if model_type_oof == 'lightgbm':\n",
    "                mlflow.lightgbm.log_model(final_model, \"final_model\", signature=model_signature_final)\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(final_model, \"final_model\", signature=model_signature_final)\n",
    "            \n",
    "            mlflow.set_tag(\"status\", \"success_oof_final\")\n",
    "            return {\n",
    "                \"status\": \"success\", \"model_type\": model_type_oof, \"final_model_run_id\": final_model_run_id,\n",
    "                \"oof_rmse\": oof_rmse, \"oof_r2\": oof_r2,\n",
    "                \"final_model_test_rmse\": final_model_rmse, \"final_model_test_r2\": final_model_r2, # Can be None\n",
    "                \"oof_predictions_path\": oof_file_path, # DBFS path\n",
    "                \"test_predictions_path\": test_preds_file_path # DBFS path\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during OOF/Final training for {model_type_oof}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            mlflow.set_tag(\"status\", \"failed_oof_final\")\n",
    "            mlflow.log_param(\"error_oof_final\", str(e)[:250])\n",
    "            return {\"status\": \"failed\", \"model_type\": model_type_oof, \"error_message\": str(e)}\n",
    "\n",
    "# --- Ensemble Functions (create_ensemble_features_from_parquet, train_stacked_ensemble, calculate_weighted_ensemble) ---\n",
    "# These were defined in the previous \"full code\" response and should mostly work if the Parquet paths are correct.\n",
    "# Key is that create_ensemble_features_from_parquet loads the Parquet files correctly.\n",
    "# We'll ensure the y_true_test for evaluating ensembles is loaded correctly.\n",
    "\n",
    "def create_ensemble_features_from_parquet(base_model_types_ens, \n",
    "                                          oof_pred_dir_dbfs_ens, test_pred_dir_dbfs_ens, \n",
    "                                          label_col_name_in_oof_ens, test_true_labels_series_ens): # Pass test_true_labels\n",
    "    \"\"\"Loads OOF and test predictions from Parquet files to create meta-features.\"\"\"\n",
    "    all_oof_dfs_list = []\n",
    "    all_test_dfs_list = []\n",
    "    y_train_true_series_ens = None\n",
    "\n",
    "    for i, model_type_ens in enumerate(base_model_types_ens):\n",
    "        oof_path = os.path.join(oof_pred_dir_dbfs_ens, f\"oof_preds_{model_type_ens}.parquet\")\n",
    "        test_path = os.path.join(test_pred_dir_dbfs_ens, f\"test_preds_{model_type_ens}.parquet\")\n",
    "        \n",
    "        print(f\"  Ensembling: Loading OOF from {oof_path}\")\n",
    "        if not os.path.exists(oof_path):\n",
    "            print(f\"  Warning: OOF file for {model_type_ens} not found at {oof_path}. Skipping.\")\n",
    "            continue\n",
    "        oof_pdf_single_model = pd.read_parquet(oof_path)\n",
    "        \n",
    "        print(f\"  Ensembling: Loading Test Preds from {test_path}\")\n",
    "        if not os.path.exists(test_path):\n",
    "            print(f\"  Warning: Test prediction file for {model_type_ens} not found at {test_path}. Skipping.\")\n",
    "            continue\n",
    "        test_pdf_single_model = pd.read_parquet(test_path)\n",
    "\n",
    "        # Extract true label for training meta-learner from the OOF files (should be consistent)\n",
    "        true_label_col_in_oof = f\"true_{label_col_name_in_oof_ens}\" # As saved by OOF function\n",
    "        if i == 0 and true_label_col_in_oof in oof_pdf_single_model.columns:\n",
    "            y_train_true_series_ens = oof_pdf_single_model[true_label_col_in_oof].copy()\n",
    "        \n",
    "        # Ensure prediction column exists before trying to access\n",
    "        oof_pred_col = f'oof_pred_{model_type_ens}'\n",
    "        test_pred_col = f'test_pred_{model_type_ens}'\n",
    "\n",
    "        if oof_pred_col not in oof_pdf_single_model.columns:\n",
    "            print(f\"  Warning: Column {oof_pred_col} not in OOF file {oof_path}. Skipping model {model_type_ens} for OOF.\")\n",
    "            continue\n",
    "        if test_pred_col not in test_pdf_single_model.columns:\n",
    "            print(f\"  Warning: Column {test_pred_col} not in Test Pred file {test_path}. Skipping model {model_type_ens} for Test Preds.\")\n",
    "            continue\n",
    "\n",
    "        all_oof_dfs_list.append(oof_pdf_single_model[[oof_pred_col]].copy()) # Ensure it's a DataFrame\n",
    "        all_test_dfs_list.append(test_pdf_single_model[[test_pred_col]].copy())\n",
    "\n",
    "\n",
    "    if not all_oof_dfs_list or y_train_true_series_ens is None:\n",
    "        print(\"Error: Not enough OOF predictions to build ensemble features, or true labels missing from OOF files.\")\n",
    "        return None, None, None, None # Added one more None for y_true_test if it's not passed\n",
    "\n",
    "    X_meta_train_pdf = pd.concat(all_oof_dfs_list, axis=1)\n",
    "    X_meta_test_pdf = pd.concat(all_test_dfs_list, axis=1)\n",
    "    y_meta_train_np = y_train_true_series_ens.values\n",
    "    \n",
    "    print(f\"    Meta features created: X_meta_train shape {X_meta_train_pdf.shape}, X_meta_test shape {X_meta_test_pdf.shape}, y_meta_train shape {y_meta_train_np.shape}\")\n",
    "    return X_meta_train_pdf, y_meta_train_np, X_meta_test_pdf, test_true_labels_series_ens # Pass through test labels\n",
    "\n",
    "\n",
    "# train_stacked_ensemble - modified to take y_true_test_np for evaluation\n",
    "def train_stacked_ensemble(meta_learner_name, meta_learner_model, \n",
    "                           X_meta_train_pdf_ens, y_meta_train_np_ens, \n",
    "                           X_meta_test_pdf_ens, y_true_test_np_ens, # Pass true test labels\n",
    "                           mlflow_parent_run_name_prefix_ens, seed_ens, \n",
    "                           primary_metric_config_ens, max_metrics_config_ens,\n",
    "                           label_col_name_ens): # Pass label name for signature\n",
    "    with mlflow.start_run(run_name=f\"{mlflow_parent_run_name_prefix_ens}_Stacked_{meta_learner_name}\", nested=False) as stack_run:\n",
    "        mlflow.log_param(\"meta_learner_type\", meta_learner_name)\n",
    "        mlflow.set_tag(\"ensemble_type\", \"stacking\")\n",
    "        mlflow.set_tag(\"seed\", seed_ens)\n",
    "\n",
    "        print(f\"Training Stacked Ensemble with Meta-Learner: {meta_learner_name}\")\n",
    "        try:\n",
    "            # Models expect NumPy arrays\n",
    "            X_meta_train_np_ens = X_meta_train_pdf_ens.values\n",
    "            X_meta_test_np_ens = X_meta_test_pdf_ens.values\n",
    "\n",
    "            meta_learner_model.fit(X_meta_train_np_ens, y_meta_train_np_ens)\n",
    "            \n",
    "            if hasattr(meta_learner_model, 'get_params'):\n",
    "                 mlflow.log_params({f\"meta_{k}\":v for k,v in meta_learner_model.get_params().items() if isinstance(v, (str, int, float, bool))})\n",
    "\n",
    "            stacked_test_predictions = meta_learner_model.predict(X_meta_test_np_ens)\n",
    "\n",
    "            if y_true_test_np_ens is None:\n",
    "                print(\"  Cannot evaluate stacked ensemble as true test labels are not available.\")\n",
    "                rmse, r2, mae = None, None, None\n",
    "            else:\n",
    "                rmse = np.sqrt(mean_squared_error(y_true_test_np_ens, stacked_test_predictions))\n",
    "                r2 = r2_score(y_true_test_np_ens, stacked_test_predictions)\n",
    "                mae = mean_absolute_error(y_true_test_np_ens, stacked_test_predictions)\n",
    "            \n",
    "            metrics_to_log = {}\n",
    "            if rmse is not None: metrics_to_log[\"stacked_rmse\"] = rmse\n",
    "            if r2 is not None: metrics_to_log[\"stacked_r2\"] = r2\n",
    "            if mae is not None: metrics_to_log[\"stacked_mae\"] = mae\n",
    "            \n",
    "            logged_metrics_count = 0\n",
    "            for m_name, m_val in sorted(metrics_to_log.items()):\n",
    "                if logged_metrics_count < max_metrics_config_ens:\n",
    "                    mlflow.log_metric(m_name, m_val)\n",
    "                    logged_metrics_count +=1\n",
    "\n",
    "            print(f\"  Stacked ({meta_learner_name}) Test RMSE: {rmse if rmse is not None else 'N/A'}, R2: {r2 if r2 is not None else 'N/A'}\")\n",
    "\n",
    "            model_signature_stack = mlflow.models.infer_signature(X_meta_test_np_ens, pd.Series(stacked_test_predictions, name=label_col_name_ens))\n",
    "            if isinstance(meta_learner_model, lgb.LGBMRegressor):\n",
    "                 mlflow.lightgbm.log_model(meta_learner_model, f\"meta_learner_{meta_learner_name}\", signature=model_signature_stack)\n",
    "            else:\n",
    "                 mlflow.sklearn.log_model(meta_learner_model, f\"meta_learner_{meta_learner_name}\", signature=model_signature_stack)\n",
    "\n",
    "            mlflow.set_tag(\"status\", \"success_stacking\")\n",
    "            return {\"status\": \"success\", \"meta_learner\": meta_learner_name, \"rmse\": rmse, \"r2\": r2, \"run_id\": stack_run.info.run_id}\n",
    "        \n",
    "        except Exception as e:\n",
    "            # ... (error handling as before) ...\n",
    "            print(f\"ERROR training stacked ensemble ({meta_learner_name}): {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            mlflow.set_tag(\"status\", \"failed_stacking\")\n",
    "            mlflow.log_param(\"error_stacking\", str(e)[:250])\n",
    "            return {\"status\": \"failed\", \"meta_learner\": meta_learner_name, \"error_message\": str(e)}\n",
    "\n",
    "\n",
    "# calculate_weighted_ensemble - modified to take y_true_test_np for evaluation\n",
    "def calculate_weighted_ensemble(base_model_metrics_oof_ens, # List of dicts from OOF phase output\n",
    "                                X_meta_test_pdf_ens, y_true_test_np_ens, # Pass true test labels\n",
    "                                primary_metric_config_ens, max_metrics_config_ens,\n",
    "                                mlflow_parent_run_name_prefix_ens):\n",
    "    with mlflow.start_run(run_name=f\"{mlflow_parent_run_name_prefix_ens}_WeightedEnsemble\", nested=False) as weighted_run:\n",
    "        mlflow.set_tag(\"ensemble_type\", \"weighted_average\")\n",
    "        print(\"Calculating Weighted Ensemble...\")\n",
    "\n",
    "        try:\n",
    "            weights = []\n",
    "            # Filter out models that might have failed OOF or had issues\n",
    "            valid_base_models_for_weighting = [m for m in base_model_metrics_oof_ens if m.get('status') == 'success' and m.get('oof_rmse') is not None and m.get('oof_r2') is not None]\n",
    "            if not valid_base_models_for_weighting:\n",
    "                print(\"  No valid base models with OOF metrics for weighting. Skipping weighted ensemble.\")\n",
    "                mlflow.set_tag(\"status\", \"skipped_no_base_models\")\n",
    "                return {\"status\": \"skipped\", \"error_message\": \"No valid base models for weighting.\"}\n",
    "\n",
    "            if primary_metric_config_ens == 'rmse':\n",
    "                # Ensure oof_rmse is positive before division\n",
    "                total_inverse_rmse = sum(1.0 / m['oof_rmse'] for m in valid_base_models_for_weighting if m['oof_rmse'] > 1e-9) # Avoid division by zero\n",
    "                if total_inverse_rmse < 1e-9 : \n",
    "                    weights = [1.0 / len(valid_base_models_for_weighting)] * len(valid_base_models_for_weighting)\n",
    "                else:\n",
    "                    weights = [( (1.0 / m['oof_rmse']) / total_inverse_rmse if m['oof_rmse'] > 1e-9 else 0) for m in valid_base_models_for_weighting]\n",
    "            \n",
    "            elif primary_metric_config_ens == 'r2':\n",
    "                positive_r2s = [max(0, m['oof_r2']) for m in valid_base_models_for_weighting]\n",
    "                total_r2 = sum(positive_r2s)\n",
    "                if total_r2 < 1e-9 :\n",
    "                    weights = [1.0 / len(valid_base_models_for_weighting)] * len(valid_base_models_for_weighting)\n",
    "                else:\n",
    "                    weights = [r2 / total_r2 for r2 in positive_r2s]\n",
    "            else: \n",
    "                weights = [1.0 / len(valid_base_models_for_weighting)] * len(valid_base_models_for_weighting)\n",
    "            \n",
    "            # Normalize weights if any small numerical instability occurred\n",
    "            sum_weights = sum(weights)\n",
    "            if sum_weights > 1e-9:\n",
    "                weights = [w / sum_weights for w in weights]\n",
    "            else: # Fallback to equal weights if all weights became zero\n",
    "                 weights = [1.0 / len(valid_base_models_for_weighting)] * len(valid_base_models_for_weighting)\n",
    "\n",
    "\n",
    "            mlflow.log_param(\"weighting_strategy\", f\"based_on_oof_{primary_metric_config_ens}\")\n",
    "            for i, model_info in enumerate(valid_base_models_for_weighting):\n",
    "                mlflow.log_param(f\"weight_{model_info['model_type']}\", weights[i])\n",
    "                mlflow.log_metric(f\"oof_metric_for_weight_{model_info['model_type']}\", model_info['oof_rmse'] if primary_metric_config_ens == 'rmse' else model_info['oof_r2'])\n",
    "\n",
    "            weighted_predictions = np.zeros(len(X_meta_test_pdf_ens))\n",
    "            if X_meta_test_pdf_ens.shape[1] != len(weights):\n",
    "                # This can happen if create_ensemble_features skipped some models but valid_base_models_for_weighting didn't\n",
    "                # We need to align X_meta_test_pdf_ens columns with valid_base_models_for_weighting\n",
    "                print(f\"  Aligning X_meta_test_pdf columns for weighted ensemble. Original cols: {X_meta_test_pdf_ens.columns.tolist()}\")\n",
    "                aligned_X_meta_test_cols = []\n",
    "                temp_weights = []\n",
    "                for i, model_info in enumerate(valid_base_models_for_weighting):\n",
    "                    pred_col_name = f\"test_pred_{model_info['model_type']}\"\n",
    "                    if pred_col_name in X_meta_test_pdf_ens.columns:\n",
    "                        aligned_X_meta_test_cols.append(X_meta_test_pdf_ens[pred_col_name].values)\n",
    "                        temp_weights.append(weights[i]) # Keep the weight for this valid model\n",
    "                    else:\n",
    "                        print(f\"    Warning: Prediction column {pred_col_name} not found in X_meta_test_pdf. Skipping for weighted sum.\")\n",
    "                \n",
    "                if not aligned_X_meta_test_cols:\n",
    "                     raise ValueError(\"No valid prediction columns found in X_meta_test_pdf for weighted sum after alignment.\")\n",
    "\n",
    "                weights = temp_weights # Use the filtered weights\n",
    "                # Re-normalize weights\n",
    "                sum_weights = sum(weights)\n",
    "                if sum_weights > 1e-9: weights = [w / sum_weights for w in weights]\n",
    "                else: weights = [1.0 / len(weights)] * len(weights) if weights else []\n",
    "\n",
    "\n",
    "                if weights: # Proceed only if there are still weights\n",
    "                    aligned_X_meta_test_np_array = np.array(aligned_X_meta_test_cols).T # Transpose to make it (n_samples, n_models)\n",
    "                    weighted_predictions = np.sum(aligned_X_meta_test_np_array * np.array(weights), axis=1)\n",
    "                else:\n",
    "                    raise ValueError(\"No models left for weighted ensemble after alignment.\")\n",
    "\n",
    "            else: # If shapes matched initially\n",
    "                 weighted_predictions = np.sum(X_meta_test_pdf_ens.values * np.array(weights), axis=1)\n",
    "\n",
    "            if y_true_test_np_ens is None:\n",
    "                print(\"  Cannot evaluate weighted ensemble as true test labels are not available.\")\n",
    "                rmse, r2, mae = None, None, None\n",
    "            else:\n",
    "                rmse = np.sqrt(mean_squared_error(y_true_test_np_ens, weighted_predictions))\n",
    "                r2 = r2_score(y_true_test_np_ens, weighted_predictions)\n",
    "                mae = mean_absolute_error(y_true_test_np_ens, weighted_predictions)\n",
    "\n",
    "            metrics_to_log = {}\n",
    "            if rmse is not None: metrics_to_log[\"weighted_rmse\"] = rmse\n",
    "            if r2 is not None: metrics_to_log[\"weighted_r2\"] = r2\n",
    "            if mae is not None: metrics_to_log[\"weighted_mae\"] = mae\n",
    "            logged_metrics_count = 0\n",
    "            for m_name, m_val in sorted(metrics_to_log.items()):\n",
    "                if logged_metrics_count < max_metrics_config_ens:\n",
    "                    mlflow.log_metric(m_name, m_val)\n",
    "                    logged_metrics_count +=1\n",
    "\n",
    "            print(f\"  Weighted Ensemble Test RMSE: {rmse if rmse is not None else 'N/A'}, R2: {r2 if r2 is not None else 'N/A'}\")\n",
    "            mlflow.set_tag(\"status\", \"success_weighted\")\n",
    "            return {\"status\": \"success\", \"rmse\": rmse, \"r2\": r2, \"run_id\": weighted_run.info.run_id}\n",
    "\n",
    "        except Exception as e:\n",
    "            # ... (error handling as before) ...\n",
    "            print(f\"ERROR calculating weighted ensemble: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            mlflow.set_tag(\"status\", \"failed_weighted\")\n",
    "            mlflow.log_param(\"error_weighted\", str(e)[:250])\n",
    "            return {\"status\": \"failed\", \"error_message\": str(e)}\n",
    "\n",
    "print(\"--- All HPO, OOF, and Ensemble Utility Functions Defined (Updated for Parquet Input) ---\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d48dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-------------------- CELL 4: MAIN ORCHESTRATION LOGIC (Updated for Parquet Input) -------------------->\n",
    "print(\"\\nCell 4: Main Orchestration Logic (Using Parquet Input) - Executing...\")\n",
    "\n",
    "# --- 0. Setup MLflow Experiment ---\n",
    "# This variable `mlflow_main_experiment_id` is used for all parent runs in this orchestration\n",
    "global mlflow_main_experiment_id\n",
    "mlflow_main_experiment_id = None\n",
    "try:\n",
    "    mlflow_main_experiment_id = get_or_create_experiment(EXPERIMENT_PATH, spark) # EXPERIMENT_PATH from Init cell\n",
    "    if mlflow_main_experiment_id:\n",
    "        mlflow.set_experiment(experiment_id=mlflow_main_experiment_id)\n",
    "        print(f\"MLflow experiment '{EXPERIMENT_PATH}' is set with ID: {mlflow_main_experiment_id}\")\n",
    "    else:\n",
    "        raise Exception(\"Main MLflow experiment could not be set. Halting.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL: Could not initialize main MLflow experiment. Error: {e}\")\n",
    "    # Consider dbutils.notebook.exit(\"MLflow experiment setup failed\") # Halt if critical\n",
    "\n",
    "# Ensure execution proceeds only if experiment is set\n",
    "if mlflow_main_experiment_id:\n",
    "    # These globals are used by the objective function\n",
    "    global HPO_PARENT_RUN_ID_FOR_OBJECTIVE\n",
    "    global CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE\n",
    "\n",
    "    # --- 1. Individual HPO for each Base Algorithm (Sequential) ---\n",
    "    print(\"\\n--- Phase 1: Individual Hyperparameter Optimization for Base Models (Sequential) ---\")\n",
    "    best_hpo_configs_per_algorithm = {} # Stores: {'algo_type': {'best_params': {...}, 'best_trial_run_id': '...', 'hpo_campaign_run_id': '...'}}\n",
    "\n",
    "    for algo_type_hpo in BASE_ALGORITHMS_TO_RUN: # BASE_ALGORITHMS_TO_RUN from Init cell\n",
    "        print(f\"\\nStarting HPO Campaign for Algorithm: {algo_type_hpo}...\")\n",
    "        if algo_type_hpo not in ALGORITHM_SEARCH_SPACES: # ALGORITHM_SEARCH_SPACES from Functions cell\n",
    "            print(f\"  Warning: Search space for {algo_type_hpo} not defined. Skipping HPO.\")\n",
    "            continue\n",
    "\n",
    "        # Each HPO campaign gets its own parent run\n",
    "        with mlflow.start_run(run_name=f\"HPO_Campaign_{algo_type_hpo}\", nested=False) as hpo_campaign_run:\n",
    "            HPO_PARENT_RUN_ID_FOR_OBJECTIVE = hpo_campaign_run.info.run_id\n",
    "            CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE = algo_type_hpo # Set for the objective function\n",
    "\n",
    "            mlflow.log_param(\"hpo_algorithm_target\", algo_type_hpo)\n",
    "            mlflow.log_param(\"num_hpo_trials_config\", NUM_HPO_TRIALS) # From Init cell\n",
    "            mlflow.log_param(\"primary_metric_config\", PRIMARY_METRIC) # From Init cell\n",
    "            mlflow.log_param(\"global_seed_config\", GLOBAL_SEED)       # From Init cell\n",
    "            # Log the specific search space for this algorithm as a dict/json artifact\n",
    "            try:\n",
    "                search_space_to_log = ALGORITHM_SEARCH_SPACES[algo_type_hpo]['model_params']\n",
    "                # Convert hp objects to strings for logging if direct dict logging fails\n",
    "                str_search_space = {k: str(v) for k, v in search_space_to_log.items()}\n",
    "                mlflow.log_dict(str_search_space, f\"search_space_{algo_type_hpo}.json\")\n",
    "            except Exception as log_e:\n",
    "                print(f\"  Warning: Could not log search space for {algo_type_hpo}: {log_e}\")\n",
    "\n",
    "            hpo_trials_db = Trials() # For sequential HPO\n",
    "\n",
    "            try:\n",
    "                current_search_space_for_fmin = ALGORITHM_SEARCH_SPACES[algo_type_hpo]['model_params']\n",
    "                \n",
    "                print(f\"  Running fmin for {algo_type_hpo} with {NUM_HPO_TRIALS} trials...\")\n",
    "                best_hyperopt_result_indices = fmin(\n",
    "                    fn=objective_function_regression, # Updated to use Parquet input\n",
    "                    space=current_search_space_for_fmin,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=NUM_HPO_TRIALS,\n",
    "                    trials=hpo_trials_db,\n",
    "                    rstate=np.random.default_rng(GLOBAL_SEED)\n",
    "                )\n",
    "                \n",
    "                best_actual_params_found = space_eval(current_search_space_for_fmin, best_hyperopt_result_indices)\n",
    "                \n",
    "                best_trial_from_db = hpo_trials_db.best_trial\n",
    "                if best_trial_from_db and best_trial_from_db['result']['status'] == STATUS_OK:\n",
    "                    best_trial_mlflow_run_id = best_trial_from_db['result'].get('run_id')\n",
    "                    best_trial_loss_val = best_trial_from_db['result']['loss']\n",
    "                    best_trial_attachments_val = best_trial_from_db['result'].get('attachments', {})\n",
    "                    \n",
    "                    print(f\"    Best HPO trial for {algo_type_hpo}: Loss={best_trial_loss_val:.4f}, Params={best_actual_params_found}, MLflow Trial Run ID={best_trial_mlflow_run_id}\")\n",
    "                    mlflow.log_params({f\"best_hpo_{k}\": v for k,v in best_actual_params_found.items()})\n",
    "                    mlflow.log_metric(\"best_hpo_loss_campaign\", best_trial_loss_val)\n",
    "                    if best_trial_mlflow_run_id: mlflow.set_tag(\"best_hpo_trial_run_id\", best_trial_mlflow_run_id)\n",
    "                    for att_k, att_v in best_trial_attachments_val.items():\n",
    "                        if isinstance(att_v, (int, float)) and att_k != \"model_type\":\n",
    "                            mlflow.log_metric(f\"best_hpo_trial_{att_k}\", att_v)\n",
    "\n",
    "                    best_hpo_configs_per_algorithm[algo_type_hpo] = {\n",
    "                        \"best_params\": best_actual_params_found,\n",
    "                        \"best_trial_run_id\": best_trial_mlflow_run_id,\n",
    "                        \"hpo_campaign_run_id\": HPO_PARENT_RUN_ID_FOR_OBJECTIVE,\n",
    "                        \"attachments\": best_trial_attachments_val # Store attachments like rmse, r2\n",
    "                    }\n",
    "                    mlflow.set_tag(\"status_hpo_campaign\", \"success\")\n",
    "                else:\n",
    "                    print(f\"    HPO for {algo_type_hpo} did not yield a successful best trial from trials DB.\")\n",
    "                    mlflow.set_tag(\"status_hpo_campaign\", \"no_successful_best_trial\")\n",
    "\n",
    "            except Exception as e_fmin:\n",
    "                print(f\"  ERROR during HPO fmin execution for {algo_type_hpo}: {e_fmin}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                mlflow.set_tag(\"status_hpo_campaign\", \"fmin_error\")\n",
    "                mlflow.log_param(\"error_fmin\", str(e_fmin)[:250])\n",
    "    print(\"--- Individual HPO Phase Completed ---\")\n",
    "\n",
    "    # --- 2. OOF Generation & Final Base Model Training (Sequential) ---\n",
    "    print(\"\\n--- Phase 2: OOF Prediction Generation & Final Base Model Training ---\")\n",
    "    final_base_model_outputs = {} # Stores: {'algo_type': {'final_model_run_id': ..., 'oof_predictions_path': ..., ...}}\n",
    "\n",
    "    for algo_type_oof, hpo_config_data in best_hpo_configs_per_algorithm.items():\n",
    "        if hpo_config_data and hpo_config_data.get(\"best_params\"):\n",
    "            print(f\"\\nGenerating OOF & Final Model for: {algo_type_oof}...\")\n",
    "            \n",
    "            # Ensure paths for OOF output directories are using /dbfs/ for Pandas to_parquet\n",
    "            oof_output_dir = DBFS_UC_OOF_PREDS_DIR # From Init cell, already /dbfs/ prefixed\n",
    "            test_preds_output_dir = DBFS_UC_TEST_PREDS_DIR # From Init cell, already /dbfs/ prefixed\n",
    "\n",
    "            oof_result_data = train_final_model_and_generate_oof_parquet_input(\n",
    "                model_type_oof=algo_type_oof,\n",
    "                best_hyperparams_oof=hpo_config_data['best_params'],\n",
    "                train_parquet_path_oof=SHARED_PROCESSED_TRAIN_PATH, # Path to named-cols Parquet\n",
    "                test_parquet_path_oof=SHARED_PROCESSED_TEST_PATH,   # Path to named-cols Parquet\n",
    "                label_col_name_oof=YOUR_LABEL_COLUMN_NAME,\n",
    "                k_folds_oof_val=K_FOLDS_OOF,\n",
    "                seed_oof=GLOBAL_SEED,\n",
    "                mlflow_parent_run_name_prefix_oof=\"MVP\", # Prefix for this stage's MLflow run\n",
    "                oof_output_dir_dbfs_oof=oof_output_dir,\n",
    "                test_preds_output_dir_dbfs_oof=test_preds_output_dir\n",
    "            )\n",
    "            if oof_result_data['status'] == 'success':\n",
    "                final_base_model_outputs[algo_type_oof] = oof_result_data\n",
    "                print(f\"  Successfully generated OOF and final model for {algo_type_oof}.\")\n",
    "            else:\n",
    "                print(f\"  Failed to generate OOF/final model for {algo_type_oof}: {oof_result_data.get('error_message')}\")\n",
    "        else:\n",
    "            print(f\"  Skipping OOF for {algo_type_oof} as no successful HPO result was found.\")\n",
    "    print(\"--- OOF Generation & Final Base Model Training Phase Completed ---\")\n",
    "\n",
    "    # --- 3. Ensemble Creation ---\n",
    "    print(\"\\n--- Phase 3: Ensemble Creation ---\")\n",
    "    if not final_base_model_outputs:\n",
    "        print(\"No base models successfully processed with OOF data. Skipping ensemble creation.\")\n",
    "    else:\n",
    "        # Load true test labels ONCE for evaluating all ensembles\n",
    "        y_true_test_for_ensemble_eval_np = None\n",
    "        try:\n",
    "            print(f\"  Loading true test labels for ensemble evaluation from {SHARED_PROCESSED_TEST_PATH}...\")\n",
    "            test_pdf_for_labels = pd.read_parquet(SHARED_PROCESSED_TEST_PATH)\n",
    "            if YOUR_LABEL_COLUMN_NAME in test_pdf_for_labels.columns:\n",
    "                y_true_test_for_ensemble_eval_np = test_pdf_for_labels[YOUR_LABEL_COLUMN_NAME].astype(float).values\n",
    "                print(f\"    True test labels loaded. Shape: {y_true_test_for_ensemble_eval_np.shape}\")\n",
    "            else:\n",
    "                print(\"    Warning: Label column not found in test Parquet file. Cannot evaluate ensembles.\")\n",
    "        except Exception as e_label:\n",
    "            print(f\"    Error loading true test labels for ensemble evaluation: {e_label}. Ensembles may not be evaluated.\")\n",
    "\n",
    "        # Prepare meta features using the Parquet predictions\n",
    "        ensemble_base_model_types_list = [algo for algo, result in final_base_model_outputs.items() if result['status'] == 'success']\n",
    "        \n",
    "        if ensemble_base_model_types_list:\n",
    "            meta_features_data = create_ensemble_features_from_parquet(\n",
    "                base_model_types_ens=ensemble_base_model_types_list,\n",
    "                oof_pred_dir_dbfs_ens=DBFS_UC_OOF_PREDS_DIR, # Pass /dbfs/ path\n",
    "                test_pred_dir_dbfs_ens=DBFS_UC_TEST_PREDS_DIR, # Pass /dbfs/ path\n",
    "                label_col_name_in_oof_ens=YOUR_LABEL_COLUMN_NAME,\n",
    "                test_true_labels_series_ens=pd.Series(y_true_test_for_ensemble_eval_np, name=YOUR_LABEL_COLUMN_NAME) if y_true_test_for_ensemble_eval_np is not None else None\n",
    "            )\n",
    "\n",
    "            if meta_features_data and meta_features_data[0] is not None: # X_meta_train_pdf is first element\n",
    "                X_meta_train_pdf_ens, y_meta_train_np_ens, X_meta_test_pdf_ens, _ = meta_features_data # _ is y_true_test (already have it)\n",
    "\n",
    "                # --- 3.A Weighted Ensemble ---\n",
    "                print(\"\\n  Creating Weighted Ensemble...\")\n",
    "                oof_metrics_for_weighting = []\n",
    "                for algo, details in final_base_model_outputs.items():\n",
    "                    if details['status'] == 'success':\n",
    "                        oof_metrics_for_weighting.append({\n",
    "                            'model_type': algo,\n",
    "                            'oof_rmse': details.get('oof_rmse'), \n",
    "                            'oof_r2': details.get('oof_r2')\n",
    "                            # Ensure these keys exist and are valid numbers\n",
    "                        })\n",
    "                \n",
    "                if oof_metrics_for_weighting:\n",
    "                    weighted_ens_result = calculate_weighted_ensemble(\n",
    "                        base_model_metrics_oof_ens=oof_metrics_for_weighting,\n",
    "                        X_meta_test_pdf_ens=X_meta_test_pdf_ens.copy(), # Pass the test predictions DataFrame\n",
    "                        y_true_test_np_ens=y_true_test_for_ensemble_eval_np, # Pass true test labels for evaluation\n",
    "                        primary_metric_config_ens=PRIMARY_METRIC,\n",
    "                        max_metrics_config_ens=MAX_METRICS_TO_LOG,\n",
    "                        mlflow_parent_run_name_prefix_ens=\"MVP\"\n",
    "                    )\n",
    "                    if weighted_ens_result.get('status') == 'success':\n",
    "                        print(f\"    Weighted Ensemble Test RMSE: {weighted_ens_result.get('rmse', 'N/A'):.4f}, R2: {weighted_ens_result.get('r2', 'N/A'):.4f}\")\n",
    "                else:\n",
    "                    print(\"    Not enough successful base models with OOF metrics for weighted ensemble.\")\n",
    "\n",
    "                # --- 3.B Stacked Ensemble ---\n",
    "                print(\"\\n  Creating Stacked Ensembles...\")\n",
    "                for meta_key, meta_model_instance in META_LEARNERS_FOR_STACKING.items(): # META_LEARNERS_FOR_STACKING from Init\n",
    "                    print(f\"    Stacking with Meta-Learner: {meta_key}\")\n",
    "                    stacking_ens_result = train_stacked_ensemble(\n",
    "                        meta_learner_name=meta_key,\n",
    "                        meta_learner_model=meta_model_instance,\n",
    "                        X_meta_train_pdf_ens=X_meta_train_pdf_ens.copy(),\n",
    "                        y_meta_train_np_ens=y_meta_train_np_ens.copy(),\n",
    "                        X_meta_test_pdf_ens=X_meta_test_pdf_ens.copy(),\n",
    "                        y_true_test_np_ens=y_true_test_for_ensemble_eval_np.copy() if y_true_test_for_ensemble_eval_np is not None else None,\n",
    "                        mlflow_parent_run_name_prefix_ens=\"MVP\",\n",
    "                        seed_ens=GLOBAL_SEED,\n",
    "                        primary_metric_config_ens=PRIMARY_METRIC,\n",
    "                        max_metrics_config_ens=MAX_METRICS_TO_LOG,\n",
    "                        label_col_name_ens=YOUR_LABEL_COLUMN_NAME\n",
    "                    )\n",
    "                    if stacking_ens_result.get('status') == 'success':\n",
    "                        print(f\"    Stacked Ensemble ({meta_key}) Test RMSE: {stacking_ens_result.get('rmse', 'N/A'):.4f}, R2: {stacking_ens_result.get('r2', 'N/A'):.4f}\")\n",
    "                    else:\n",
    "                        print(f\"    Failed to train Stacked Ensemble ({meta_key}): {stacking_ens_result.get('error_message')}\")\n",
    "            else:\n",
    "                print(\"  Failed to create meta-features for stacking. Skipping stacking ensembles.\")\n",
    "        else:\n",
    "            print(\"  Skipping ensemble creation as no base models were processed for OOF generation.\")\n",
    "    print(\"--- Ensemble Creation Phase Completed ---\")\n",
    "\n",
    "else:\n",
    "    print(\"Halting script because main MLflow experiment could not be set.\")\n",
    "\n",
    "print(\"\\n--- FULL MVP ORCHESTRATION COMPLETED (SEQUENTIAL EXECUTION) ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
