{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.lightgbm # Add other flavors if more models are used later (xgboost, catboost)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib # For potential model saving/loading, though MLflow handles most\n",
    "import time\n",
    "import shutil # For cleaning up temp directories if any\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge # Example meta-learner\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import col, udf # Keep for future Spark ML preprocessing part\n",
    "# from pyspark.sql.types import ArrayType, DoubleType # Keep for future Spark ML preprocessing part\n",
    "# from pyspark.ml.linalg import VectorUDT, DenseVector, SparseVector # Keep for future Spark ML preprocessing part\n",
    "\n",
    "# Suppress LightGBM verbosity for HPO trials\n",
    "import logging\n",
    "logging.getLogger('lightgbm').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Ensure spark session is available (Databricks notebooks usually provide 'spark')\n",
    "if 'spark' not in locals():\n",
    "    spark = SparkSession.builder.appName(\"AdvancedML_MVP_Sequential\").getOrCreate()\n",
    "\n",
    "print(\"Imports successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a426b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Init Cell - Global Configurations\n",
    "\n",
    "# --- MLflow Configuration ---\n",
    "# !!! IMPORTANT: SET YOUR MLFLOW EXPERIMENT PATH !!!\n",
    "EXPERIMENT_PATH = \"/Users/your_username@example.com/MVP_Regression_HPO_Ensemble\" # e.g., /Users/your.email@domain.com/MyProjectExperiment\n",
    "# Get your username from dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get() if needed\n",
    "\n",
    "# --- Data Paths (Unity Catalog Volumes) ---\n",
    "# !!! IMPORTANT: SET YOUR UNITY CATALOG VOLUME BASE PATH !!!\n",
    "# Example: \"/Volumes/my_main_catalog/my_bronze_schema/my_project_volume/\"\n",
    "UC_BASE_DATA_PATH = \"/Volumes/delfos/\" # As per your input\n",
    "\n",
    "# --- Paths for Preprocessed Data (Output of your Spark ML Preprocessing Pipeline) ---\n",
    "# These paths MUST point to Parquet files (or directories of Parquet files)\n",
    "# containing 'features_array' and your label column.\n",
    "# We'll assume a single version of preprocessed data for this MVP.\n",
    "# !!! IMPORTANT: UPDATE THESE AFTER YOUR PREPROCESSING STEP SAVES DATA !!!\n",
    "SHARED_PROCESSED_TRAIN_PATH = f\"{UC_BASE_DATA_PATH}processed_data/train_processed.parquet\"\n",
    "SHARED_PROCESSED_TEST_PATH = f\"{UC_BASE_DATA_PATH}processed_data/test_processed.parquet\"\n",
    "\n",
    "# !!! IMPORTANT: SET YOUR ACTUAL LABEL COLUMN NAME AS IT APPEARS IN THE PARQUET FILES !!!\n",
    "YOUR_LABEL_COLUMN_NAME = \"target\"\n",
    "\n",
    "# --- Paths for Intermediate OOF/Test Predictions (will be created under UC_BASE_DATA_PATH) ---\n",
    "UC_OOF_PREDS_DIR = os.path.join(UC_BASE_DATA_PATH, \"oof_predictions\")\n",
    "UC_TEST_PREDS_DIR = os.path.join(UC_BASE_DATA_PATH, \"test_predictions\")\n",
    "UC_FINAL_MODELS_DIR = os.path.join(UC_BASE_DATA_PATH, \"final_models_hpo\") # For non-MLflow saved models if any\n",
    "\n",
    "# --- HPO Configuration ---\n",
    "NUM_HPO_TRIALS = 25  # Number of trials for EACH base algorithm's HPO (keep low for MVP testing, increase later)\n",
    "PRIMARY_METRIC = \"rmse\"  # Choose 'rmse' (to minimize) or 'r2' (to maximize, HPO will minimize -r2)\n",
    "\n",
    "# --- Base Algorithms to Run ---\n",
    "# For MVP: DecisionTree, RandomForest, ExtraTrees, LightGBM\n",
    "BASE_ALGORITHMS_TO_RUN = ['decision_tree', 'random_forest', 'extra_trees', 'lightgbm']\n",
    "\n",
    "# --- Cross-Validation for OOF ---\n",
    "K_FOLDS_OOF = 5 # Number of folds for generating Out-of-Fold predictions\n",
    "\n",
    "# --- Reproducibility ---\n",
    "GLOBAL_SEED = 117\n",
    "\n",
    "# --- MLflow Setup ---\n",
    "# Function get_or_create_experiment will be defined in the next cell\n",
    "# experiment_id = get_or_create_experiment(EXPERIMENT_PATH, spark) # Pass spark if needed by function\n",
    "# if experiment_id:\n",
    "#    mlflow.set_experiment(experiment_id=experiment_id)\n",
    "# else:\n",
    "#    print(\"Error: MLflow experiment could not be set.\")\n",
    "\n",
    "# --- Ensemble Configuration ---\n",
    "META_LEARNERS_FOR_STACKING = {\n",
    "    'ridge': Ridge(random_state=GLOBAL_SEED),\n",
    "    'lgbm_meta': lgb.LGBMRegressor(random_state=GLOBAL_SEED, verbose=-1, n_jobs=-1) # Simple LGBM for meta\n",
    "}\n",
    "\n",
    "# --- Other Global Settings ---\n",
    "MAX_METRICS_TO_LOG = 5 # Max number of metrics to log per MLflow run besides primary\n",
    "\n",
    "# Create directories if they don't exist (use dbutils for UC volumes if direct os.makedirs fails)\n",
    "# For UC Volumes, direct os.makedirs might not work from driver for non /dbfs/ paths.\n",
    "# Spark can write to these paths, and for local operations, you might need to use /dbfs/ equivalent if copying.\n",
    "# For saving pandas DFs, ensure the path is accessible.\n",
    "# For now, we assume spark.write.parquet will handle UC Volume paths.\n",
    "# For pandas.to_parquet, use /dbfs/Volumes/... path.\n",
    "DBFS_UC_OOF_PREDS_DIR = f\"/dbfs{UC_OOF_PREDS_DIR}\"\n",
    "DBFS_UC_TEST_PREDS_DIR = f\"/dbfs{UC_TEST_PREDS_DIR}\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(DBFS_UC_OOF_PREDS_DIR, exist_ok=True)\n",
    "    os.makedirs(DBFS_UC_TEST_PREDS_DIR, exist_ok=True)\n",
    "    print(f\"Created/checked OOF directory: {DBFS_UC_OOF_PREDS_DIR}\")\n",
    "    print(f\"Created/checked Test Preds directory: {DBFS_UC_TEST_PREDS_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create directories using os.makedirs on {DBFS_UC_OOF_PREDS_DIR} or {DBFS_UC_TEST_PREDS_DIR}. This might be okay if Spark handles it or if paths are purely for Spark. Error: {e}\")\n",
    "\n",
    "\n",
    "print(f\"--- Global Configurations Initialized ---\")\n",
    "print(f\"MLflow Experiment Path: {EXPERIMENT_PATH}\")\n",
    "print(f\"Unity Catalog Base Data Path: {UC_BASE_DATA_PATH}\")\n",
    "print(f\"Processed Train Data Path: {SHARED_PROCESSED_TRAIN_PATH}\")\n",
    "print(f\"Processed Test Data Path: {SHARED_PROCESSED_TEST_PATH}\")\n",
    "print(f\"Label Column: {YOUR_LABEL_COLUMN_NAME}\")\n",
    "print(f\"Global Seed: {GLOBAL_SEED}\")\n",
    "print(f\"Primary Metric for HPO: {PRIMARY_METRIC.upper()}\")\n",
    "print(f\"Number of HPO Trials per Algorithm: {NUM_HPO_TRIALS}\")\n",
    "print(f\"K-Folds for OOF: {K_FOLDS_OOF}\")\n",
    "print(f\"Base Algorithms to run: {BASE_ALGORITHMS_TO_RUN}\")\n",
    "print(f\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b9a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Utility Functions & HPO/Model Training Components\n",
    "\n",
    "# --- MLflow Utility ---\n",
    "def get_or_create_experiment(experiment_name, spark_session):\n",
    "    \"\"\"Safely creates or fetches an MLflow experiment.\"\"\"\n",
    "    try:\n",
    "        # Check if running in a Databricks notebook environment\n",
    "        if hasattr(spark_session, 'databricks'): # A bit of a hacky check\n",
    "            # In Databricks, experiment names can be full paths\n",
    "            # client = mlflow.tracking.MlflowClient() # Not needed if using mlflow.set_experiment\n",
    "            experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "            if experiment:\n",
    "                print(f\"MLflow experiment '{experiment_name}' found with ID: {experiment.experiment_id}\")\n",
    "                return experiment.experiment_id\n",
    "            else:\n",
    "                print(f\"MLflow experiment '{experiment_name}' not found. Attempting to create.\")\n",
    "                experiment_id = mlflow.create_experiment(name=experiment_name)\n",
    "                print(f\"MLflow experiment '{experiment_name}' created with ID: {experiment_id}\")\n",
    "                return experiment_id\n",
    "        else: # Fallback for local execution if needed, though UC Volumes imply Databricks\n",
    "            if not mlflow.get_experiment_by_name(experiment_name):\n",
    "                mlflow.create_experiment(experiment_name)\n",
    "            return mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "    except mlflow.exceptions.MlflowException as e:\n",
    "        if \"RESOURCE_ALREADY_EXISTS\" in str(e) or \"Experiment with name\" in str(e) and \"already exists\" in str(e):\n",
    "            print(f\"Race condition or experiment '{experiment_name}' was created concurrently. Fetching again.\")\n",
    "            experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "            if experiment:\n",
    "                print(f\"Successfully fetched concurrently created experiment '{experiment_name}' with ID: {experiment.experiment_id}\")\n",
    "                return experiment.experiment_id\n",
    "        print(f\"MLflowException: Could not get or create experiment '{experiment_name}'. Error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in get_or_create_experiment for '{experiment_name}'. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Algorithm Search Spaces (Top ~5 HPs) ---\n",
    "ALGORITHM_SEARCH_SPACES = {\n",
    "    'decision_tree': {\n",
    "        'model_params': {\n",
    "            'max_depth': hp.choice('dt_max_depth', [3, 5, 7, 10, 15, None]),\n",
    "            'min_samples_split': hp.quniform('dt_min_samples_split', 2, 20, 1),\n",
    "            'min_samples_leaf': hp.quniform('dt_min_samples_leaf', 1, 20, 1),\n",
    "            'criterion': hp.choice('dt_criterion', ['squared_error', 'friedman_mse', 'absolute_error']), # Poisson removed as it's for counts\n",
    "            'splitter': hp.choice('dt_splitter', ['best', 'random'])\n",
    "        }\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model_params': {\n",
    "            'n_estimators': hp.quniform('rf_n_estimators', 20, 200, 10), # Reduced for MVP speed\n",
    "            'max_depth': hp.choice('rf_max_depth', [5, 10, 15, None]),\n",
    "            'min_samples_split': hp.quniform('rf_min_samples_split', 2, 10, 1),\n",
    "            'min_samples_leaf': hp.quniform('rf_min_samples_leaf', 1, 10, 1),\n",
    "            'max_features': hp.choice('rf_max_features', ['sqrt', 'log2', None])\n",
    "        }\n",
    "    },\n",
    "    'extra_trees': {\n",
    "        'model_params': {\n",
    "            'n_estimators': hp.quniform('et_n_estimators', 20, 200, 10), # Reduced for MVP speed\n",
    "            'max_depth': hp.choice('et_max_depth', [5, 10, 15, None]),\n",
    "            'min_samples_split': hp.quniform('et_min_samples_split', 2, 10, 1),\n",
    "            'min_samples_leaf': hp.quniform('et_min_samples_leaf', 1, 10, 1),\n",
    "            'max_features': hp.choice('et_max_features', ['sqrt', 'log2', None])\n",
    "        }\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'model_params': {\n",
    "            'n_estimators': hp.quniform('lgbm_n_estimators', 20, 200, 10), # Reduced for MVP speed\n",
    "            'learning_rate': hp.loguniform('lgbm_learning_rate', np.log(0.01), np.log(0.2)),\n",
    "            'num_leaves': hp.quniform('lgbm_num_leaves', 10, 100, 5), # Reduced for MVP speed\n",
    "            'max_depth': hp.quniform('lgbm_max_depth', 3, 10, 1), # More constrained for MVP\n",
    "            'subsample': hp.uniform('lgbm_subsample', 0.7, 1.0),\n",
    "            'reg_alpha': hp.uniform('lgbm_reg_alpha', 0.0, 0.5) # L1 regularization\n",
    "        }\n",
    "    }\n",
    "    # Add XGBoostRegressor, CatBoostRegressor search spaces here when you include them\n",
    "}\n",
    "print(\"Search spaces defined.\")\n",
    "\n",
    "\n",
    "# --- HPO Objective Function (Generalized for Regression) ---\n",
    "# Note: HPO_PARENT_RUN_ID_FOR_OBJECTIVE, SHARED_PROCESSED_TRAIN_PATH_FOR_OBJECTIVE etc.\n",
    "# will be set dynamically before calling fmin for each algorithm.\n",
    "# This function relies on these being in its execution scope.\n",
    "\n",
    "# Define these as placeholders, they will be updated by the HPO orchestrator for each algorithm\n",
    "HPO_PARENT_RUN_ID_FOR_OBJECTIVE = None\n",
    "CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE = None\n",
    "# Primary metric and seed are from global scope\n",
    "# Data paths also need to be accessible\n",
    "\n",
    "def load_processed_data_for_sklearn(train_path, test_path, label_col_name):\n",
    "    \"\"\"Loads preprocessed data from Parquet and prepares for scikit-learn.\"\"\"\n",
    "    try:\n",
    "        train_pdf = pd.read_parquet(train_path)\n",
    "        test_pdf = pd.read_parquet(test_path)\n",
    "\n",
    "        X_train = np.array(train_pdf['features_array'].tolist())\n",
    "        y_train = train_pdf[label_col_name].values.astype(float)\n",
    "        X_test = np.array(test_pdf['features_array'].tolist())\n",
    "        y_test = test_pdf[label_col_name].values.astype(float)\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading/processing data from {train_path} or {test_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def objective_function_regression(hyperparams_from_hyperopt):\n",
    "    \"\"\"\n",
    "    Objective function for Hyperopt HPO.\n",
    "    Trains a specific regressor, logs to MLflow, returns loss.\n",
    "    Relies on HPO_PARENT_RUN_ID_FOR_OBJECTIVE and CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE being set.\n",
    "    Also SHARED_PROCESSED_TRAIN_PATH, SHARED_PROCESSED_TEST_PATH, YOUR_LABEL_COLUMN_NAME,\n",
    "    PRIMARY_METRIC, GLOBAL_SEED, MAX_METRICS_TO_LOG from global scope.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sanitize hyperparams (convert numpy types to native Python for model constructors)\n",
    "    # This is crucial because Hyperopt can pass np.int64 etc.\n",
    "    sanitized_hyperparams = {}\n",
    "    for k, v in hyperparams_from_hyperopt.items():\n",
    "        if isinstance(v, np.generic):\n",
    "            sanitized_hyperparams[k] = v.item()\n",
    "        elif k in ['max_depth'] and v is not None: # Max_depth can be None or int\n",
    "             sanitized_hyperparams[k] = int(v) if v is not None else None\n",
    "        # Ensure integer types for specific hyperparameters expected by models\n",
    "        elif k in ['min_samples_split', 'min_samples_leaf', 'n_estimators', 'num_leaves', 'iterations'] and v is not None:\n",
    "             sanitized_hyperparams[k] = int(v)\n",
    "        else:\n",
    "            sanitized_hyperparams[k] = v\n",
    "    \n",
    "    trial_run_name = f\"Trial_{CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE}_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "    with mlflow.start_run(run_id=HPO_PARENT_RUN_ID_FOR_OBJECTIVE, run_name=trial_run_name, nested=True) as trial_run:\n",
    "        mlflow.log_param(\"model_type_trial\", CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE) # Log the actual model type for this trial\n",
    "        mlflow.log_params(sanitized_hyperparams)\n",
    "        mlflow.set_tag(\"seed\", GLOBAL_SEED)\n",
    "\n",
    "        try:\n",
    "            X_train, y_train, X_test, y_test = load_processed_data_for_sklearn(\n",
    "                SHARED_PROCESSED_TRAIN_PATH, SHARED_PROCESSED_TEST_PATH, YOUR_LABEL_COLUMN_NAME\n",
    "            )\n",
    "\n",
    "            model = None\n",
    "            if CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'decision_tree':\n",
    "                model = DecisionTreeRegressor(**sanitized_hyperparams, random_state=GLOBAL_SEED)\n",
    "            elif CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'random_forest':\n",
    "                model = RandomForestRegressor(**sanitized_hyperparams, random_state=GLOBAL_SEED, n_jobs=-1)\n",
    "            elif CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'extra_trees':\n",
    "                model = ExtraTreesRegressor(**sanitized_hyperparams, random_state=GLOBAL_SEED, n_jobs=-1)\n",
    "            elif CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'lightgbm':\n",
    "                model = lgb.LGBMRegressor(**sanitized_hyperparams, random_state=GLOBAL_SEED, n_jobs=-1, verbose=-1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model type in objective function: {CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE}\")\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "            r2 = r2_score(y_test, predictions)\n",
    "            mae = mean_absolute_error(y_test, predictions)\n",
    "            \n",
    "            metrics_to_log = {\"rmse\": rmse, \"r2\": r2, \"mae\": mae}\n",
    "            logged_metrics_count = 0\n",
    "            # Log up to MAX_METRICS_TO_LOG, prioritizing based on sorted name for consistency\n",
    "            for m_name, m_val in sorted(metrics_to_log.items()):\n",
    "                if logged_metrics_count < MAX_METRICS_TO_LOG:\n",
    "                    mlflow.log_metric(m_name, m_val)\n",
    "                    logged_metrics_count +=1\n",
    "\n",
    "            if CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE == 'lightgbm':\n",
    "                mlflow.lightgbm.log_model(model, \"model\", signature=mlflow.models.infer_signature(X_test, pd.Series(predictions, name=YOUR_LABEL_COLUMN_NAME)))\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(model, \"model\", signature=mlflow.models.infer_signature(X_test, pd.Series(predictions, name=YOUR_LABEL_COLUMN_NAME)))\n",
    "            \n",
    "            mlflow.set_tag(\"status\", \"success\")\n",
    "\n",
    "            loss = None\n",
    "            if PRIMARY_METRIC == 'rmse':\n",
    "                loss = rmse\n",
    "            elif PRIMARY_METRIC == 'r2':\n",
    "                loss = -r2 \n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported PRIMARY_METRIC for loss calculation: {PRIMARY_METRIC}\")\n",
    "\n",
    "            return {'loss': loss, 'status': STATUS_OK, 'run_id': trial_run.info.run_id, \n",
    "                    'attachments': {'rmse': rmse, 'r2': r2, 'mae': mae, 'model_type': CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE}}\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message_short = str(e)[:250] # MLflow param limit\n",
    "            mlflow.log_param(\"error\", error_message_short)\n",
    "            mlflow.set_tag(\"status\", \"failed\")\n",
    "            print(f\"TRIAL ERROR in run {trial_run.info.run_id} for model {CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE}: {e}\")\n",
    "            # Ensure a sensible worst loss is returned\n",
    "            worst_loss = float('inf') if PRIMARY_METRIC == 'rmse' else float('inf') # if minimizing -r2, larger positive is worse\n",
    "            return {'loss': worst_loss, 'status': 'fail', 'run_id': trial_run.info.run_id, \n",
    "                    'error_message': error_message_short}\n",
    "\n",
    "print(\"Objective function defined.\")\n",
    "\n",
    "\n",
    "# --- OOF Generation and Final Model Training Function ---\n",
    "def train_final_model_and_generate_oof(model_type, best_hyperparams,\n",
    "                                     train_data_path, test_data_path, label_col_name,\n",
    "                                     k_folds, seed, mlflow_parent_run_name_prefix):\n",
    "    \"\"\"\n",
    "    Trains a final model with best HPs, generates OOF predictions on train set\n",
    "    and predictions on test set. Logs everything to MLflow.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=f\"{mlflow_parent_run_name_prefix}_{model_type}_OOF_Final\", nested=False) as oof_parent_run:\n",
    "        mlflow.log_params(best_hyperparams)\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        mlflow.log_param(\"k_folds_for_oof\", k_folds)\n",
    "        mlflow.set_tag(\"seed\", seed)\n",
    "\n",
    "        final_model_run_id = oof_parent_run.info.run_id\n",
    "        print(f\"Starting OOF generation and final model training for {model_type}. MLflow Run ID: {final_model_run_id}\")\n",
    "\n",
    "        try:\n",
    "            X_full_train, y_full_train, X_test, y_test = load_processed_data_for_sklearn(\n",
    "                train_data_path, test_data_path, label_col_name\n",
    "            )\n",
    "\n",
    "            oof_predictions = np.zeros(len(y_full_train))\n",
    "            test_predictions_from_folds = np.zeros((len(y_test), k_folds)) # To average later\n",
    "\n",
    "            kf = KFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "            for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_full_train, y_full_train)):\n",
    "                print(f\"  Processing Fold {fold_num+1}/{k_folds} for {model_type}...\")\n",
    "                X_fold_train, X_fold_val = X_full_train[train_idx], X_full_train[val_idx]\n",
    "                y_fold_train, y_fold_val = y_full_train[train_idx], y_full_train[val_idx]\n",
    "\n",
    "                model_fold = None\n",
    "                if model_type == 'decision_tree': model = DecisionTreeRegressor(**best_hyperparams, random_state=seed)\n",
    "                elif model_type == 'random_forest': model = RandomForestRegressor(**best_hyperparams, random_state=seed, n_jobs=-1)\n",
    "                elif model_type == 'extra_trees': model = ExtraTreesRegressor(**best_hyperparams, random_state=seed, n_jobs=-1)\n",
    "                elif model_type == 'lightgbm': model = lgb.LGBMRegressor(**best_hyperparams, random_state=seed, n_jobs=-1, verbose=-1)\n",
    "                else: raise ValueError(f\"Unsupported model type for OOF: {model_type}\")\n",
    "\n",
    "                model.fit(X_fold_train, y_fold_train)\n",
    "                oof_predictions[val_idx] = model.predict(X_fold_val)\n",
    "                test_predictions_from_folds[:, fold_num] = model.predict(X_test)\n",
    "            \n",
    "            # Average test predictions from all folds\n",
    "            final_test_predictions = np.mean(test_predictions_from_folds, axis=1)\n",
    "\n",
    "            # Calculate OOF metrics\n",
    "            oof_rmse = np.sqrt(mean_squared_error(y_full_train, oof_predictions))\n",
    "            oof_r2 = r2_score(y_full_train, oof_predictions)\n",
    "            mlflow.log_metric(\"oof_rmse\", oof_rmse)\n",
    "            mlflow.log_metric(\"oof_r2\", oof_r2)\n",
    "            print(f\"  {model_type} OOF RMSE: {oof_rmse:.4f}, OOF R2: {oof_r2:.4f}\")\n",
    "\n",
    "            # Save OOF and Test predictions as artifacts (and to DBFS for ensembling)\n",
    "            oof_df = pd.DataFrame({'true_label': y_full_train, f'oof_pred_{model_type}': oof_predictions})\n",
    "            test_preds_df = pd.DataFrame({'true_label': y_test, f'test_pred_{model_type}': final_test_predictions}) # Store true_label if available for test\n",
    "\n",
    "            oof_file_path_parquet = os.path.join(DBFS_UC_OOF_PREDS_DIR, f\"oof_preds_{model_type}.parquet\")\n",
    "            test_preds_file_path_parquet = os.path.join(DBFS_UC_TEST_PREDS_DIR, f\"test_preds_{model_type}.parquet\")\n",
    "            \n",
    "            oof_df.to_parquet(oof_file_path_parquet, index=False)\n",
    "            test_preds_df.to_parquet(test_preds_file_path_parquet, index=False)\n",
    "\n",
    "            mlflow.log_artifact(oof_file_path_parquet)\n",
    "            mlflow.log_artifact(test_preds_file_path_parquet)\n",
    "            mlflow.set_tag(f\"oof_preds_path_{model_type}\", oof_file_path_parquet.replace(\"/dbfs\",\"dbfs:\")) # Log path for reference\n",
    "            mlflow.set_tag(f\"test_preds_path_{model_type}\", test_preds_file_path_parquet.replace(\"/dbfs\",\"dbfs:\"))\n",
    "\n",
    "\n",
    "            # Train final model on ALL training data\n",
    "            print(f\"  Training final {model_type} model on all training data...\")\n",
    "            final_model = None\n",
    "            if model_type == 'decision_tree': final_model = DecisionTreeRegressor(**best_hyperparams, random_state=seed)\n",
    "            elif model_type == 'random_forest': final_model = RandomForestRegressor(**best_hyperparams, random_state=seed, n_jobs=-1)\n",
    "            elif model_type == 'extra_trees': final_model = ExtraTreesRegressor(**best_hyperparams, random_state=seed, n_jobs=-1)\n",
    "            elif model_type == 'lightgbm': final_model = lgb.LGBMRegressor(**best_hyperparams, random_state=seed, n_jobs=-1, verbose=-1)\n",
    "            else: raise ValueError(f\"Unsupported model type for final training: {model_type}\")\n",
    "\n",
    "            final_model.fit(X_full_train, y_full_train)\n",
    "\n",
    "            # Evaluate final model on test set\n",
    "            final_model_test_preds = final_model.predict(X_test) # These should be similar to final_test_predictions\n",
    "            final_model_rmse = np.sqrt(mean_squared_error(y_test, final_model_test_preds))\n",
    "            final_model_r2 = r2_score(y_test, final_model_test_preds)\n",
    "            final_model_mae = mean_absolute_error(y_test, final_model_test_preds)\n",
    "\n",
    "            mlflow.log_metric(\"final_model_test_rmse\", final_model_rmse)\n",
    "            mlflow.log_metric(\"final_model_test_r2\", final_model_r2)\n",
    "            mlflow.log_metric(\"final_model_test_mae\", final_model_mae)\n",
    "            print(f\"  {model_type} Final Model Test RMSE: {final_model_rmse:.4f}, R2: {final_model_r2:.4f}\")\n",
    "            \n",
    "            # Log final model\n",
    "            if model_type == 'lightgbm':\n",
    "                mlflow.lightgbm.log_model(final_model, \"final_model\", signature=mlflow.models.infer_signature(X_test, pd.Series(final_model_test_preds, name=YOUR_LABEL_COLUMN_NAME)))\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(final_model, \"final_model\", signature=mlflow.models.infer_signature(X_test, pd.Series(final_model_test_preds, name=YOUR_LABEL_COLUMN_NAME)))\n",
    "            \n",
    "            mlflow.set_tag(\"status\", \"success_oof_final\")\n",
    "            return {\n",
    "                \"status\": \"success\", \"model_type\": model_type, \"final_model_run_id\": final_model_run_id,\n",
    "                \"oof_rmse\": oof_rmse, \"final_model_test_rmse\": final_model_rmse,\n",
    "                \"oof_predictions_path\": oof_file_path_parquet,\n",
    "                \"test_predictions_path\": test_preds_file_path_parquet\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during OOF/Final training for {model_type}: {e}\")\n",
    "            mlflow.set_tag(\"status\", \"failed_oof_final\")\n",
    "            mlflow.log_param(\"error_oof_final\", str(e)[:250])\n",
    "            return {\"status\": \"failed\", \"model_type\": model_type, \"error_message\": str(e)}\n",
    "\n",
    "print(\"OOF generation and final model training function defined.\")\n",
    "\n",
    "\n",
    "# --- Ensemble Functions ---\n",
    "def create_ensemble_features(base_model_types, oof_dir, test_preds_dir, label_col_name, train_data_path_for_true_labels):\n",
    "    \"\"\"Loads OOF and test predictions for base models to create meta-features.\"\"\"\n",
    "    all_oof_preds = []\n",
    "    all_test_preds = []\n",
    "    \n",
    "    # Load true labels for training set (needed for meta-learner training)\n",
    "    # This assumes the original train_pdf (or equivalent) is available or can be re-read\n",
    "    # For simplicity, let's assume we need to load it to get y_train.\n",
    "    # It's better if y_train was saved with OOF preds.\n",
    "    # The oof_df from train_final_model_and_generate_oof already contains 'true_label'\n",
    "    # We just need to merge them.\n",
    "    \n",
    "    y_train_true_df = None\n",
    "\n",
    "    for model_type in base_model_types:\n",
    "        oof_path = os.path.join(oof_dir, f\"oof_preds_{model_type}.parquet\")\n",
    "        test_path = os.path.join(test_preds_dir, f\"test_preds_{model_type}.parquet\")\n",
    "        \n",
    "        if not os.path.exists(oof_path) or not os.path.exists(test_path):\n",
    "            print(f\"Warning: Prediction files for {model_type} not found. Skipping for ensemble.\")\n",
    "            continue\n",
    "            \n",
    "        oof_pdf = pd.read_parquet(oof_path)\n",
    "        test_pdf = pd.read_parquet(test_path)\n",
    "        \n",
    "        # Capture y_train_true from the first OOF file if not already done\n",
    "        if y_train_true_df is None and 'true_label' in oof_pdf.columns:\n",
    "            y_train_true_df = oof_pdf[['true_label']].copy()\n",
    "            # Make sure index aligns if we plan to concat later, or just use the values\n",
    "            \n",
    "        all_oof_preds.append(oof_pdf[[f'oof_pred_{model_type}']])\n",
    "        all_test_preds.append(test_pdf[[f'test_pred_{model_type}']])\n",
    "\n",
    "    if not all_oof_preds or y_train_true_df is None:\n",
    "        print(\"Error: Not enough OOF predictions to build ensemble features, or true labels missing.\")\n",
    "        return None, None, None\n",
    "\n",
    "    X_meta_train = pd.concat(all_oof_preds, axis=1)\n",
    "    X_meta_test = pd.concat(all_test_preds, axis=1)\n",
    "    y_meta_train = y_train_true_df['true_label'].values\n",
    "    \n",
    "    return X_meta_train, y_meta_train, X_meta_test\n",
    "\n",
    "\n",
    "def train_stacked_ensemble(meta_learner_name, meta_learner_model, \n",
    "                           X_meta_train, y_meta_train, X_meta_test, y_true_test, # y_true_test needed for evaluation\n",
    "                           mlflow_parent_run_name_prefix, seed, primary_metric_config, max_metrics_config):\n",
    "    \"\"\"Trains a stacked ensemble meta-learner.\"\"\"\n",
    "    with mlflow.start_run(run_name=f\"{mlflow_parent_run_name_prefix}_Stacked_{meta_learner_name}\", nested=False) as stack_run:\n",
    "        mlflow.log_param(\"meta_learner_type\", meta_learner_name)\n",
    "        mlflow.set_tag(\"ensemble_type\", \"stacking\")\n",
    "        mlflow.set_tag(\"seed\", seed)\n",
    "\n",
    "        print(f\"Training Stacked Ensemble with Meta-Learner: {meta_learner_name}\")\n",
    "        try:\n",
    "            meta_learner_model.fit(X_meta_train, y_meta_train)\n",
    "            \n",
    "            # It's good practice to log the meta-learner's own parameters if it's configurable\n",
    "            if hasattr(meta_learner_model, 'get_params'):\n",
    "                 mlflow.log_params({f\"meta_{k}\":v for k,v in meta_learner_model.get_params().items() if isinstance(v, (str, int, float, bool))})\n",
    "\n",
    "\n",
    "            stacked_test_predictions = meta_learner_model.predict(X_meta_test)\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(y_true_test, stacked_test_predictions))\n",
    "            r2 = r2_score(y_true_test, stacked_test_predictions)\n",
    "            mae = mean_absolute_error(y_true_test, stacked_test_predictions)\n",
    "            \n",
    "            metrics_to_log = {\"stacked_rmse\": rmse, \"stacked_r2\": r2, \"stacked_mae\": mae}\n",
    "            logged_metrics_count = 0\n",
    "            for m_name, m_val in sorted(metrics_to_log.items()):\n",
    "                if logged_metrics_count < max_metrics_config:\n",
    "                    mlflow.log_metric(m_name, m_val)\n",
    "                    logged_metrics_count +=1\n",
    "\n",
    "            print(f\"  Stacked ({meta_learner_name}) Test RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "            # Log the meta-learner model\n",
    "            # This requires a PyFunc model if you want to package base models + meta for inference\n",
    "            # For now, just log the meta-learner itself. Productionizing stacker is more complex.\n",
    "            if isinstance(meta_learner_model, lgb.LGBMRegressor):\n",
    "                 mlflow.lightgbm.log_model(meta_learner_model, f\"meta_learner_{meta_learner_name}\", signature=mlflow.models.infer_signature(X_meta_test, pd.Series(stacked_test_predictions, name=YOUR_LABEL_COLUMN_NAME)))\n",
    "            else:\n",
    "                 mlflow.sklearn.log_model(meta_learner_model, f\"meta_learner_{meta_learner_name}\", signature=mlflow.models.infer_signature(X_meta_test, pd.Series(stacked_test_predictions, name=YOUR_LABEL_COLUMN_NAME)))\n",
    "\n",
    "            mlflow.set_tag(\"status\", \"success_stacking\")\n",
    "            return {\"status\": \"success\", \"meta_learner\": meta_learner_name, \"rmse\": rmse, \"r2\": r2, \"run_id\": stack_run.info.run_id}\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR training stacked ensemble ({meta_learner_name}): {e}\")\n",
    "            mlflow.set_tag(\"status\", \"failed_stacking\")\n",
    "            mlflow.log_param(\"error_stacking\", str(e)[:250])\n",
    "            return {\"status\": \"failed\", \"meta_learner\": meta_learner_name, \"error_message\": str(e)}\n",
    "\n",
    "\n",
    "def calculate_weighted_ensemble(base_model_metrics_oof, # List of dicts: [{'model_type':'rf', 'oof_rmse':0.5, 'test_pred_path':'...'}, ...]\n",
    "                                X_meta_test, y_true_test,\n",
    "                                primary_metric_config, max_metrics_config,\n",
    "                                mlflow_parent_run_name_prefix):\n",
    "    \"\"\"Calculates and evaluates a weighted ensemble.\"\"\"\n",
    "    with mlflow.start_run(run_name=f\"{mlflow_parent_run_name_prefix}_WeightedEnsemble\", nested=False) as weighted_run:\n",
    "        mlflow.set_tag(\"ensemble_type\", \"weighted_average\")\n",
    "        print(\"Calculating Weighted Ensemble...\")\n",
    "\n",
    "        try:\n",
    "            weights = []\n",
    "            if primary_metric_config == 'rmse': # Lower RMSE is better, so inverse for weight\n",
    "                total_inverse_rmse = sum(1.0 / m['oof_rmse'] for m in base_model_metrics_oof if m['oof_rmse'] > 0)\n",
    "                if total_inverse_rmse == 0: \n",
    "                    print(\"Warning: Sum of inverse OOF RMSE is zero, cannot calculate RMSE-based weights. Defaulting to equal weights.\")\n",
    "                    weights = [1.0 / len(base_model_metrics_oof)] * len(base_model_metrics_oof)\n",
    "                else:\n",
    "                    weights = [(1.0 / m['oof_rmse']) / total_inverse_rmse for m in base_model_metrics_oof]\n",
    "            \n",
    "            elif primary_metric_config == 'r2': # Higher R2 is better\n",
    "                # Ensure R2 values are positive for weighting, shift if necessary or use rank\n",
    "                # For simplicity, let's assume R2 values are mostly > 0. Normalize positive R2s.\n",
    "                # A more robust method might be softmax of R2 scores or rank-based weighting.\n",
    "                positive_r2s = [max(0, m['oof_r2']) for m in base_model_metrics_oof] # Cap at 0\n",
    "                total_r2 = sum(positive_r2s)\n",
    "                if total_r2 == 0:\n",
    "                    print(\"Warning: Sum of positive OOF R2 is zero, cannot calculate R2-based weights. Defaulting to equal weights.\")\n",
    "                    weights = [1.0 / len(base_model_metrics_oof)] * len(base_model_metrics_oof)\n",
    "                else:\n",
    "                    weights = [r2 / total_r2 for r2 in positive_r2s]\n",
    "            else: # Default to equal weights if metric unknown for weighting\n",
    "                print(f\"Warning: Unknown primary metric '{primary_metric_config}' for weighting. Defaulting to equal weights.\")\n",
    "                weights = [1.0 / len(base_model_metrics_oof)] * len(base_model_metrics_oof)\n",
    "\n",
    "            mlflow.log_param(\"weighting_strategy\", f\"based_on_oof_{primary_metric_config}\")\n",
    "            for i, model_info in enumerate(base_model_metrics_oof):\n",
    "                mlflow.log_param(f\"weight_{model_info['model_type']}\", weights[i])\n",
    "                mlflow.log_metric(f\"oof_metric_for_weight_{model_info['model_type']}\", model_info['oof_rmse'] if primary_metric_config == 'rmse' else model_info['oof_r2'])\n",
    "\n",
    "            # Combine test predictions using weights\n",
    "            # X_meta_test is already a DataFrame of test_pred_algo1, test_pred_algo2, ...\n",
    "            weighted_predictions = np.zeros(len(X_meta_test))\n",
    "            if X_meta_test.shape[1] != len(weights):\n",
    "                raise ValueError(f\"Mismatch between number of models for weighting ({len(weights)}) and available test predictions ({X_meta_test.shape[1]})\")\n",
    "\n",
    "            for i, col in enumerate(X_meta_test.columns): # Assumes columns are in same order as base_model_metrics_oof\n",
    "                weighted_predictions += X_meta_test[col] * weights[i]\n",
    "            \n",
    "            rmse = np.sqrt(mean_squared_error(y_true_test, weighted_predictions))\n",
    "            r2 = r2_score(y_true_test, weighted_predictions)\n",
    "            mae = mean_absolute_error(y_true_test, weighted_predictions)\n",
    "\n",
    "            metrics_to_log = {\"weighted_rmse\": rmse, \"weighted_r2\": r2, \"weighted_mae\": mae}\n",
    "            logged_metrics_count = 0\n",
    "            for m_name, m_val in sorted(metrics_to_log.items()):\n",
    "                if logged_metrics_count < max_metrics_config:\n",
    "                    mlflow.log_metric(m_name, m_val)\n",
    "                    logged_metrics_count +=1\n",
    "\n",
    "            print(f\"  Weighted Ensemble Test RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "            mlflow.set_tag(\"status\", \"success_weighted\")\n",
    "            return {\"status\": \"success\", \"rmse\": rmse, \"r2\": r2, \"run_id\": weighted_run.info.run_id}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR calculating weighted ensemble: {e}\")\n",
    "            mlflow.set_tag(\"status\", \"failed_weighted\")\n",
    "            mlflow.log_param(\"error_weighted\", str(e)[:250])\n",
    "            return {\"status\": \"failed\", \"error_message\": str(e)}\n",
    "\n",
    "print(\"Ensemble functions defined.\")\n",
    "print(\"--- All Utility Functions and HPO Components Defined ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f3636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main Orchestration Logic\n",
    "\n",
    "print(\"--- Starting Main Orchestration ---\")\n",
    "\n",
    "# --- 0. Setup MLflow Experiment ---\n",
    "# Experiment ID is fetched/created using the global EXPERIMENT_PATH\n",
    "# This needs to be done once.\n",
    "try:\n",
    "    # Make sure spark session from Init cell is used if get_or_create_experiment needs it\n",
    "    experiment_id = get_or_create_experiment(EXPERIMENT_PATH, spark)\n",
    "    if experiment_id:\n",
    "        mlflow.set_experiment(experiment_id=experiment_id)\n",
    "        print(f\"MLflow experiment '{EXPERIMENT_PATH}' is set with ID: {experiment_id}\")\n",
    "    else:\n",
    "        raise Exception(\"MLflow experiment could not be set. Halting.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL: Could not initialize MLflow experiment. Please check EXPERIMENT_PATH and permissions. Error: {e}\")\n",
    "    # dbutils.notebook.exit(\"MLflow experiment setup failed\") # If in Databricks notebook and want to halt\n",
    "\n",
    "# This is a global variable that the objective function will read\n",
    "# It will be updated for each algorithm's HPO campaign.\n",
    "global HPO_PARENT_RUN_ID_FOR_OBJECTIVE\n",
    "global CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE\n",
    "\n",
    "\n",
    "# --- 1. Individual HPO for each Base Algorithm (Sequential) ---\n",
    "print(\"\\n--- Phase X.A: Individual Hyperparameter Optimization for Base Models ---\")\n",
    "best_hpo_results_per_algorithm = {} # To store best HPs and trial run_id for each algo\n",
    "\n",
    "for algo_type in BASE_ALGORITHMS_TO_RUN:\n",
    "    print(f\"\\nStarting HPO for Algorithm: {algo_type}...\")\n",
    "    if algo_type not in ALGORITHM_SEARCH_SPACES:\n",
    "        print(f\"Warning: Search space for {algo_type} not defined. Skipping HPO.\")\n",
    "        continue\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"HPO_Campaign_{algo_type}\", nested=False) as hpo_campaign_parent_run:\n",
    "        HPO_PARENT_RUN_ID_FOR_OBJECTIVE = hpo_campaign_parent_run.info.run_id\n",
    "        CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE = algo_type # Set for the objective function\n",
    "\n",
    "        mlflow.log_param(\"algorithm_being_optimized\", algo_type)\n",
    "        mlflow.log_param(\"num_hpo_trials_config\", NUM_HPO_TRIALS)\n",
    "        mlflow.log_param(\"primary_metric_config\", PRIMARY_METRIC)\n",
    "        mlflow.log_param(\"global_seed\", GLOBAL_SEED)\n",
    "        # Log the specific search space for this algorithm if possible (can be complex for hp objects)\n",
    "        # mlflow.log_dict(ALGORITHM_SEARCH_SPACES[algo_type]['model_params'], f\"search_space_{algo_type}.json\")\n",
    "\n",
    "\n",
    "        hpo_trials_database = Trials() # Sequential trials\n",
    "\n",
    "        try:\n",
    "            # The objective function now implicitly uses CURRENT_ALGORITHM_TYPE_FOR_OBJECTIVE\n",
    "            # and HPO_PARENT_RUN_ID_FOR_OBJECTIVE.\n",
    "            # It also uses global vars for data paths, label col, seed, primary metric.\n",
    "            \n",
    "            # The search space for `fmin` should be just the 'model_params' part\n",
    "            current_search_space = ALGORITHM_SEARCH_SPACES[algo_type]['model_params']\n",
    "\n",
    "            best_hyperopt_indices = fmin(\n",
    "                fn=objective_function_regression, # This now gets hyperparams for the current_algo_type\n",
    "                space=current_search_space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=NUM_HPO_TRIALS,\n",
    "                trials=hpo_trials_database,\n",
    "                rstate=np.random.default_rng(GLOBAL_SEED) # For hyperopt's internal randomness\n",
    "            )\n",
    "            \n",
    "            best_actual_params = space_eval(current_search_space, best_hyperopt_indices)\n",
    "            \n",
    "            # Find the best trial from the hpo_trials_database\n",
    "            best_trial_obj = hpo_trials_database.best_trial\n",
    "            best_trial_run_id = None\n",
    "            best_trial_loss = float('inf')\n",
    "            best_trial_attachments = {}\n",
    "\n",
    "            if best_trial_obj and best_trial_obj['result']['status'] == STATUS_OK:\n",
    "                best_trial_run_id = best_trial_obj['result'].get('run_id')\n",
    "                best_trial_loss = best_trial_obj['result']['loss']\n",
    "                best_trial_attachments = best_trial_obj['result'].get('attachments', {})\n",
    "                \n",
    "                print(f\"  Best HPO trial for {algo_type}: Loss={best_trial_loss:.4f}, Params={best_actual_params}, MLflow Run ID={best_trial_run_id}\")\n",
    "                mlflow.log_params({f\"best_hpo_{k}\": v for k,v in best_actual_params.items()})\n",
    "                mlflow.log_metric(\"best_hpo_loss\", best_trial_loss)\n",
    "                if best_trial_run_id:\n",
    "                    mlflow.set_tag(\"best_hpo_trial_run_id\", best_trial_run_id)\n",
    "                for att_k, att_v in best_trial_attachments.items():\n",
    "                     if isinstance(att_v, (int, float)) and att_k != \"model_type\": # model_type is already a param\n",
    "                        mlflow.log_metric(f\"best_hpo_trial_{att_k}\", att_v)\n",
    "\n",
    "\n",
    "                best_hpo_results_per_algorithm[algo_type] = {\n",
    "                    \"best_params\": best_actual_params,\n",
    "                    \"best_trial_run_id\": best_trial_run_id, # This is the run for the specific trial\n",
    "                    \"hpo_campaign_run_id\": HPO_PARENT_RUN_ID_FOR_OBJECTIVE,\n",
    "                    \"attachments\": best_trial_attachments\n",
    "                }\n",
    "                mlflow.set_tag(\"status_hpo_campaign\", \"success\")\n",
    "            else:\n",
    "                print(f\"  HPO for {algo_type} did not yield a successful best trial.\")\n",
    "                mlflow.set_tag(\"status_hpo_campaign\", \"no_successful_trial\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during HPO campaign for {algo_type}: {e}\")\n",
    "            mlflow.set_tag(\"status_hpo_campaign\", \"failed\")\n",
    "            mlflow.log_param(\"error_hpo_campaign\", str(e)[:250])\n",
    "\n",
    "# --- 2. OOF Generation & Final Base Model Training (Sequential) ---\n",
    "print(\"\\n--- Phase X.B: OOF Prediction Generation & Final Base Model Training ---\")\n",
    "final_base_model_details = {} # To store info about final models and their prediction paths\n",
    "\n",
    "for algo_type, hpo_result in best_hpo_results_per_algorithm.items():\n",
    "    if hpo_result and hpo_result.get(\"best_params\"):\n",
    "        print(f\"\\nGenerating OOF & Final Model for: {algo_type} with best params: {hpo_result['best_params']}\")\n",
    "        \n",
    "        oof_result = train_final_model_and_generate_oof(\n",
    "            model_type=algo_type,\n",
    "            best_hyperparams=hpo_result['best_params'],\n",
    "            train_data_path=SHARED_PROCESSED_TRAIN_PATH,\n",
    "            test_data_path=SHARED_PROCESSED_TEST_PATH,\n",
    "            label_col_name=YOUR_LABEL_COLUMN_NAME,\n",
    "            k_folds=K_FOLDS_OOF,\n",
    "            seed=GLOBAL_SEED,\n",
    "            mlflow_parent_run_name_prefix=\"MVP\" # To distinguish these runs\n",
    "        )\n",
    "        if oof_result['status'] == 'success':\n",
    "            final_base_model_details[algo_type] = oof_result\n",
    "            print(f\"  Successfully generated OOF and final model for {algo_type}.\")\n",
    "        else:\n",
    "            print(f\"  Failed to generate OOF/final model for {algo_type}: {oof_result.get('error_message')}\")\n",
    "    else:\n",
    "        print(f\"Skipping OOF for {algo_type} as no successful HPO result was found.\")\n",
    "\n",
    "\n",
    "# --- 3. Ensemble Creation ---\n",
    "print(\"\\n--- Phase X.C: Ensemble Creation ---\")\n",
    "if not final_base_model_details:\n",
    "    print(\"No base models successfully processed for OOF. Skipping ensemble creation.\")\n",
    "else:\n",
    "    # Prepare data for ensembling\n",
    "    # We need y_true_test for evaluating ensembles. Let's load it once.\n",
    "    try:\n",
    "        _, _, _, y_true_test_for_ensemble = load_processed_data_for_sklearn(\n",
    "            SHARED_PROCESSED_TRAIN_PATH, SHARED_PROCESSED_TEST_PATH, YOUR_LABEL_COLUMN_NAME\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL: Could not load test labels for ensemble evaluation. Error: {e}. Skipping ensembles.\")\n",
    "        y_true_test_for_ensemble = None\n",
    "\n",
    "    if y_true_test_for_ensemble is not None:\n",
    "        ensemble_base_model_types = list(final_base_model_details.keys())\n",
    "        \n",
    "        # This creates X_meta_train, y_meta_train, X_meta_test\n",
    "        # y_meta_train comes from the 'true_label' column in the OOF parquet files\n",
    "        meta_features_tuple = create_ensemble_features(\n",
    "            base_model_types=ensemble_base_model_types,\n",
    "            oof_dir=DBFS_UC_OOF_PREDS_DIR, # Use DBFS path for pandas\n",
    "            test_preds_dir=DBFS_UC_TEST_PREDS_DIR, # Use DBFS path for pandas\n",
    "            label_col_name=YOUR_LABEL_COLUMN_NAME,\n",
    "            train_data_path_for_true_labels=SHARED_PROCESSED_TRAIN_PATH # Only to get y_train if not in OOF df\n",
    "        )\n",
    "\n",
    "        if meta_features_tuple and meta_features_tuple[0] is not None:\n",
    "            X_meta_train, y_meta_train, X_meta_test = meta_features_tuple\n",
    "            print(f\"  Meta features created for stacking: X_meta_train shape {X_meta_train.shape}, X_meta_test shape {X_meta_test.shape}\")\n",
    "\n",
    "            # --- 3.A Weighted Ensemble ---\n",
    "            print(\"\\n  Creating Weighted Ensemble...\")\n",
    "            # Need to gather OOF metrics for weighting\n",
    "            base_model_oof_metrics_for_weighting = []\n",
    "            for algo, details in final_base_model_details.items():\n",
    "                if details['status'] == 'success':\n",
    "                    base_model_oof_metrics_for_weighting.append({\n",
    "                        'model_type': algo,\n",
    "                        'oof_rmse': details['oof_rmse'], # This was logged from OOF calculation\n",
    "                        'oof_r2': details.get('oof_r2', 1.0 - (details['oof_rmse']**2 / np.var(y_meta_train) if np.var(y_meta_train) > 0 else 0) ), # Estimate if not directly available, or log it directly\n",
    "                        'test_pred_path': details['test_predictions_path'] # Not used by current weighted function, but good to have\n",
    "                    })\n",
    "            \n",
    "            if base_model_oof_metrics_for_weighting:\n",
    "                weighted_ensemble_result = calculate_weighted_ensemble(\n",
    "                    base_model_metrics_oof=base_model_oof_metrics_for_weighting,\n",
    "                    X_meta_test=X_meta_test.copy(), # X_meta_test has columns like 'test_pred_decision_tree', etc.\n",
    "                    y_true_test=y_true_test_for_ensemble,\n",
    "                    primary_metric_config=PRIMARY_METRIC,\n",
    "                    max_metrics_config=MAX_METRICS_TO_LOG,\n",
    "                    mlflow_parent_run_name_prefix=\"MVP\"\n",
    "                )\n",
    "                if weighted_ensemble_result['status'] == 'success':\n",
    "                    print(f\"  Weighted Ensemble Test RMSE: {weighted_ensemble_result['rmse']:.4f}, R2: {weighted_ensemble_result['r2']:.4f}\")\n",
    "            else:\n",
    "                print(\"  Not enough successful base models with OOF metrics to create weighted ensemble.\")\n",
    "\n",
    "\n",
    "            # --- 3.B Stacked Ensemble ---\n",
    "            print(\"\\n  Creating Stacked Ensembles...\")\n",
    "            for meta_learner_key, meta_learner_instance in META_LEARNERS_FOR_STACKING.items():\n",
    "                print(f\"    Stacking with Meta-Learner: {meta_learner_key}\")\n",
    "                stacking_result = train_stacked_ensemble(\n",
    "                    meta_learner_name=meta_learner_key,\n",
    "                    meta_learner_model=meta_learner_instance, # Pass the actual model instance\n",
    "                    X_meta_train=X_meta_train.copy(),\n",
    "                    y_meta_train=y_meta_train.copy(),\n",
    "                    X_meta_test=X_meta_test.copy(),\n",
    "                    y_true_test=y_true_test_for_ensemble.copy(),\n",
    "                    mlflow_parent_run_name_prefix=\"MVP\",\n",
    "                    seed=GLOBAL_SEED,\n",
    "                    primary_metric_config=PRIMARY_METRIC, # Not directly used by stacker HPO, but good for consistency\n",
    "                    max_metrics_config=MAX_METRICS_TO_LOG\n",
    "                )\n",
    "                if stacking_result['status'] == 'success':\n",
    "                    print(f\"    Stacked Ensemble ({meta_learner_key}) Test RMSE: {stacking_result['rmse']:.4f}, R2: {stacking_result['r2']:.4f}\")\n",
    "                else:\n",
    "                    print(f\"    Failed to train Stacked Ensemble ({meta_learner_key}): {stacking_result.get('error_message')}\")\n",
    "        else:\n",
    "            print(\"  Failed to create meta-features for stacking. Skipping.\")\n",
    "    else:\n",
    "        print(\"Skipping ensemble creation due to failure in loading test labels for evaluation.\")\n",
    "\n",
    "print(\"\\n--- Main Orchestration Completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca82cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: (Optional) Clean up temporary files from DBFS if created directly by pandas\n",
    "# Note: Spark writes (like .write.parquet) to UC Volumes are managed by Spark.\n",
    "# If pandas created files via /dbfs/ paths, you might want to clean them.\n",
    "# Example:\n",
    "# try:\n",
    "#    if os.path.exists(DBFS_UC_OOF_PREDS_DIR):\n",
    "#        print(f\"Cleaning up OOF predictions directory: {DBFS_UC_OOF_PREDS_DIR}\")\n",
    "#        shutil.rmtree(DBFS_UC_OOF_PREDS_DIR) # Careful with this!\n",
    "#    if os.path.exists(DBFS_UC_TEST_PREDS_DIR):\n",
    "#        print(f\"Cleaning up test predictions directory: {DBFS_UC_TEST_PREDS_DIR}\")\n",
    "#        shutil.rmtree(DBFS_UC_TEST_PREDS_DIR)\n",
    "# except Exception as e:\n",
    "#    print(f\"Error during cleanup: {e}\")\n",
    "\n",
    "# If spark session is no longer needed by other cells:\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
