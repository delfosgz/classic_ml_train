{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc50fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports (Add/Update from previous full script)\n",
    "import mlflow\n",
    "# No need for mlflow.spark here for preprocessing, but keep for HPO orchestration\n",
    "# import mlflow.sklearn # Will be used in HPO and for logging preprocessing components\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib # For saving/loading Python objects like scalers, arrays\n",
    "import time\n",
    "import shutil # For cleaning up temp directories if any\n",
    "\n",
    "from sklearn.model_selection import KFold # For K-fold target encoding if implemented manually\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce # For TargetEncoder - install if not present\n",
    "\n",
    "from pyspark.sql import SparkSession # Still need SparkSession for environment context\n",
    "\n",
    "# Suppress warnings if any from category_encoders or others for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "# Ensure spark session is available\n",
    "if 'spark' not in locals():\n",
    "    spark = SparkSession.builder.appName(\"Pandas_Preprocessing_MVP\").getOrCreate()\n",
    "\n",
    "print(\"Imports for Pandas Preprocessing successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Init Cell - Global Configurations (Review and Update)\n",
    "\n",
    "# --- MLflow Configuration ---\n",
    "EXPERIMENT_PATH = \"/Users/your_username@example.com/MVP_Regression_Pandas_Preprocessing\" # CHANGE\n",
    "\n",
    "# --- Data Paths (Unity Catalog Volumes) ---\n",
    "UC_BASE_DATA_PATH = \"/Volumes/delfos/\"\n",
    "RAW_TRAIN_DATA_PATH = f\"{UC_BASE_DATA_PATH}raw_data/train.parquet\" # Path to RAW Parquet train data\n",
    "RAW_TEST_DATA_PATH = f\"{UC_BASE_DATA_PATH}raw_data/test.parquet\"   # Path to RAW Parquet test data\n",
    "\n",
    "# --- Paths for PROCESSED Data (Output of this Pandas preprocessing script) ---\n",
    "# These will be .joblib files containing dicts of NumPy arrays {'X': ..., 'y': ...}\n",
    "# Saved to UC Volume via /dbfs/ prefix for joblib\n",
    "DBFS_PROCESSED_DATA_DIR = f\"/dbfs{UC_BASE_DATA_PATH}processed_data_pandas_mvp_v1/\" # Use /dbfs for joblib\n",
    "SHARED_PROCESSED_TRAIN_PATH = os.path.join(DBFS_PROCESSED_DATA_DIR, \"train_processed_data.joblib\")\n",
    "SHARED_PROCESSED_TEST_PATH = os.path.join(DBFS_PROCESSED_DATA_DIR, \"test_processed_data.joblib\")\n",
    "\n",
    "# --- Path for saving the FITTED PREPROCESSING COMPONENTS ---\n",
    "DBFS_PREPROCESSOR_COMPONENTS_PATH = os.path.join(DBFS_PROCESSED_DATA_DIR, \"preprocessor_components.joblib\")\n",
    "\n",
    "\n",
    "# !!! IMPORTANT: SET YOUR ACTUAL LABEL COLUMN NAME (must exist in raw data) !!!\n",
    "YOUR_LABEL_COLUMN_NAME = \"target\"\n",
    "\n",
    "# --- Define your categorical and numerical columns from the RAW data ---\n",
    "# !!! IMPORTANT: UPDATE THESE LISTS BASED ON YOUR ACTUAL RAW DATASET !!!\n",
    "CATEGORICAL_COLUMNS_RAW = [\"category_feature_1\", \"category_feature_2\"]\n",
    "NUMERICAL_COLUMNS_RAW = [\"numerical_feature_1\", \"numerical_feature_2\"]\n",
    "\n",
    "# --- MLflow Configuration for saving the preprocessing \"model\" ---\n",
    "PREPROCESSING_EXPERIMENT_PATH = EXPERIMENT_PATH # Or a dedicated one\n",
    "MLFLOW_PANDAS_PREPROCESSOR_ARTIFACT_PATH = \"pandas_preprocessor\"\n",
    "\n",
    "# --- Target Encoding Configuration (for category_encoders or manual) ---\n",
    "# For category_encoders.TargetEncoder, smoothing is a parameter.\n",
    "# If doing manual K-Fold TE, this might be relevant.\n",
    "TARGET_ENCODING_SMOOTHING = 10.0 # Default for category_encoders, can be tuned\n",
    "\n",
    "# --- Reproducibility ---\n",
    "GLOBAL_SEED = 117\n",
    "\n",
    "# --- Other Global Settings from previous Init Cell ---\n",
    "# (NUM_HPO_TRIALS, PRIMARY_METRIC, BASE_ALGORITHMS_TO_RUN, K_FOLDS_OOF, etc.\n",
    "#  are for the HPO script, not strictly needed here but good for context if this cell is shared)\n",
    "\n",
    "# Ensure output directories exist\n",
    "try:\n",
    "    os.makedirs(DBFS_PROCESSED_DATA_DIR, exist_ok=True)\n",
    "    print(f\"Checked/created processed data directory: {DBFS_PROCESSED_DATA_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create directory {DBFS_PROCESSED_DATA_DIR}. Error: {e}\")\n",
    "\n",
    "print(\"--- Pandas Preprocessing Configurations Initialized ---\")\n",
    "# ... (print other relevant configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee37931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Pandas Preprocessing Functions\n",
    "\n",
    "def load_raw_data_to_pandas(uc_volume_parquet_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads a Parquet file from a UC Volume path into a Pandas DataFrame.\"\"\"\n",
    "    # For Pandas to read directly from UC Volumes, it typically needs the /dbfs/ prefix\n",
    "    dbfs_path = uc_volume_parquet_path\n",
    "    if uc_volume_parquet_path.startswith(\"/Volumes/\"):\n",
    "        dbfs_path = f\"/dbfs{uc_volume_parquet_path}\"\n",
    "    \n",
    "    print(f\"  Loading Pandas DataFrame from: {dbfs_path}\")\n",
    "    try:\n",
    "        pdf = pd.read_parquet(dbfs_path)\n",
    "        print(f\"    Successfully loaded. Shape: {pdf.shape}\")\n",
    "        return pdf\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR loading Parquet from {dbfs_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def fit_pandas_preprocessor(train_pdf: pd.DataFrame, \n",
    "                            categorical_cols: list, \n",
    "                            numerical_cols: list, \n",
    "                            label_col: str,\n",
    "                            te_smoothing: float,\n",
    "                            global_seed: int):\n",
    "    \"\"\"\n",
    "    Fits preprocessing components (imputers, target encoders, scaler) on training data.\n",
    "    Returns processed X_train_np, y_train_np, and a dictionary of fitted components.\n",
    "    \"\"\"\n",
    "    print(\"  Fitting Pandas preprocessor...\")\n",
    "    X_train = train_pdf.drop(columns=[label_col])\n",
    "    y_train = train_pdf[label_col].astype(float)\n",
    "\n",
    "    fitted_components = {}\n",
    "\n",
    "    # 1. Impute Numerical Features (Median)\n",
    "    if numerical_cols:\n",
    "        num_imputer = SimpleImputer(strategy=\"median\")\n",
    "        X_train[numerical_cols] = num_imputer.fit_transform(X_train[numerical_cols])\n",
    "        fitted_components['numerical_imputer'] = num_imputer\n",
    "        print(f\"    Fitted Numerical Imputer for: {numerical_cols}\")\n",
    "\n",
    "    # 2. Impute Categorical Features (Most Frequent or Constant)\n",
    "    if categorical_cols:\n",
    "        # Using a constant fill value is often safer for unseen categories later\n",
    "        cat_imputer_fill_value = \"__MISSING__\"\n",
    "        # SimpleImputer for categoricals needs strategy='most_frequent' or 'constant'\n",
    "        cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=cat_imputer_fill_value)\n",
    "        X_train[categorical_cols] = cat_imputer.fit_transform(X_train[categorical_cols])\n",
    "        fitted_components['categorical_imputer'] = cat_imputer\n",
    "        fitted_components['categorical_imputer_fill_value'] = cat_imputer_fill_value # Store for transform\n",
    "        print(f\"    Fitted Categorical Imputer for: {categorical_cols} (using fill_value='{cat_imputer_fill_value}')\")\n",
    "\n",
    "    # 3. Target Encoding for Categorical Features\n",
    "    # Using category_encoders library for simplicity and robustness in MVP\n",
    "    # It handles unseen categories by default (imputes with global mean of target during fit)\n",
    "    # and training set leakage with smoothing or k-fold like behavior internally if configured.\n",
    "    if categorical_cols:\n",
    "        # Note: category_encoders.TargetEncoder expects y to be passed during fit and transform of X\n",
    "        # It also needs to know which columns are categorical.\n",
    "        # Ensure categorical columns are of 'object' or 'category' dtype for category_encoders\n",
    "        for col in categorical_cols:\n",
    "            X_train[col] = X_train[col].astype('category')\n",
    "\n",
    "        target_encoder = ce.TargetEncoder(cols=categorical_cols, smoothing=te_smoothing, handle_unknown='value', handle_missing='value')\n",
    "        # handle_unknown='value' uses the overall mean of y_train for new categories\n",
    "        # handle_missing='value' uses the overall mean of y_train for NaNs seen during transform\n",
    "        \n",
    "        X_train[categorical_cols] = target_encoder.fit_transform(X_train[categorical_cols], y_train)\n",
    "        fitted_components['target_encoder'] = target_encoder\n",
    "        print(f\"    Fitted TargetEncoder for: {categorical_cols} with smoothing={te_smoothing}\")\n",
    "        # The transformed columns are now numeric.\n",
    "\n",
    "    # 4. Scale All Features (Numerical + Target Encoded Categoricals)\n",
    "    # All columns in X_train (that are features) should now be numeric.\n",
    "    feature_columns_for_scaling = numerical_cols + categorical_cols # Cat cols are now numerically encoded\n",
    "    \n",
    "    if feature_columns_for_scaling: # Check if there are any features to scale\n",
    "        scaler = StandardScaler()\n",
    "        # Ensure all columns for scaling are indeed numeric and do not contain NaNs from TE if 'value' wasn't perfect\n",
    "        # TargetEncoder with handle_missing='value' and handle_unknown='value' should prevent NaNs if y_train has no NaNs.\n",
    "        # If any NaNs persist (e.g., if a numerical column was all NaNs and imputer failed, or TE issue), scaler will fail.\n",
    "        # A check or a final SimpleImputer(strategy='median') pass on feature_columns_for_scaling could be added.\n",
    "        \n",
    "        # Make sure all columns are float before scaling\n",
    "        for col in feature_columns_for_scaling:\n",
    "            X_train[col] = X_train[col].astype(float)\n",
    "            # Final check for NaNs after TE before scaling, impute if necessary\n",
    "            if X_train[col].isnull().any():\n",
    "                print(f\"    Warning: NaNs detected in column {col} before scaling. Applying median imputation.\")\n",
    "                median_val = X_train[col].median()\n",
    "                X_train[col] = X_train[col].fillna(median_val)\n",
    "                # Store this median for transform stage if not already handled by a formal imputer for these generated features\n",
    "                if 'post_te_imputer_medians' not in fitted_components:\n",
    "                    fitted_components['post_te_imputer_medians'] = {}\n",
    "                fitted_components['post_te_imputer_medians'][col] = median_val\n",
    "\n",
    "\n",
    "        X_train_scaled_np = scaler.fit_transform(X_train[feature_columns_for_scaling])\n",
    "        fitted_components['scaler'] = scaler\n",
    "        fitted_components['feature_columns_for_scaling'] = feature_columns_for_scaling # Store order\n",
    "        print(f\"    Fitted StandardScaler for features: {feature_columns_for_scaling}\")\n",
    "    else: # No features to scale (e.g. only one categorical feature became one TE feature)\n",
    "        print(\"    No features identified for scaling. X_train might be empty or only have non-scalable features.\")\n",
    "        # Handle case where X_train might become just an empty dataframe or a single column that was a label.\n",
    "        # This logic assumes X_train[feature_columns_for_scaling] results in a valid array for scikit-learn models.\n",
    "        # If feature_columns_for_scaling is empty, X_train_scaled_np would be problematic.\n",
    "        if not feature_columns_for_scaling and not X_train.empty:\n",
    "             X_train_scaled_np = X_train.values # Or handle appropriately\n",
    "        elif X_train.empty:\n",
    "             X_train_scaled_np = np.array([]) # Or handle as error\n",
    "        else: # Should not happen if there were features\n",
    "             X_train_scaled_np = X_train[feature_columns_for_scaling].values\n",
    "\n",
    "\n",
    "    y_train_np = y_train.values\n",
    "    print(\"  Pandas preprocessor fitting complete.\")\n",
    "    return X_train_scaled_np, y_train_np, fitted_components\n",
    "\n",
    "\n",
    "def transform_pandas_preprocessor(raw_pdf: pd.DataFrame, \n",
    "                                  fitted_components: dict,\n",
    "                                  categorical_cols: list, \n",
    "                                  numerical_cols: list, \n",
    "                                  label_col: str = None, # Label col might not be in test data for prediction\n",
    "                                  is_train_data=False): # Flag to indicate if we are transforming training data (for y)\n",
    "    \"\"\"\n",
    "    Applies fitted preprocessing components to new data.\n",
    "    \"\"\"\n",
    "    print(\"  Transforming data with Pandas preprocessor...\")\n",
    "    \n",
    "    # Prepare X and y (if label_col is present)\n",
    "    if label_col and label_col in raw_pdf.columns:\n",
    "        X_data = raw_pdf.drop(columns=[label_col]).copy() # Make a copy to avoid SettingWithCopyWarning\n",
    "        y_data_np = raw_pdf[label_col].astype(float).values\n",
    "    else:\n",
    "        X_data = raw_pdf.copy()\n",
    "        y_data_np = None\n",
    "\n",
    "    # 1. Impute Numerical\n",
    "    if numerical_cols and 'numerical_imputer' in fitted_components:\n",
    "        X_data.loc[:, numerical_cols] = fitted_components['numerical_imputer'].transform(X_data[numerical_cols])\n",
    "        print(f\"    Applied Numerical Imputer for: {numerical_cols}\")\n",
    "\n",
    "    # 2. Impute Categorical\n",
    "    if categorical_cols and 'categorical_imputer' in fitted_components:\n",
    "        X_data.loc[:, categorical_cols] = fitted_components['categorical_imputer'].transform(X_data[categorical_cols])\n",
    "        print(f\"    Applied Categorical Imputer for: {categorical_cols}\")\n",
    "    \n",
    "    # 3. Target Encode Categorical\n",
    "    if categorical_cols and 'target_encoder' in fitted_components:\n",
    "        # Ensure categorical columns are of 'category' or 'object' dtype for category_encoders\n",
    "        for col in categorical_cols:\n",
    "            X_data[col] = X_data[col].astype('category')\n",
    "        \n",
    "        # For TargetEncoder's transform, y is not strictly needed if it learned from y during fit,\n",
    "        # but some versions/setups might expect it. If y_data_np is None (e.g. for true new data),\n",
    "        # category_encoders TE should use the global mean learned during fit for unknowns/missings.\n",
    "        # Pass y=None if it's not available (e.g. scoring new data).\n",
    "        # If transforming training data (is_train_data=True), y_data_np should be available.\n",
    "        # However, for consistency, TE transform should only use X.\n",
    "        X_data.loc[:, categorical_cols] = fitted_components['target_encoder'].transform(X_data[categorical_cols])\n",
    "        print(f\"    Applied TargetEncoder for: {categorical_cols}\")\n",
    "\n",
    "    # 4. Scale All Features\n",
    "    feature_columns_for_scaling = fitted_components.get('feature_columns_for_scaling', [])\n",
    "    if feature_columns_for_scaling and 'scaler' in fitted_components:\n",
    "        # Make sure all columns are float before scaling and impute any post-TE NaNs\n",
    "        for col in feature_columns_for_scaling:\n",
    "            X_data[col] = X_data[col].astype(float)\n",
    "            if col in fitted_components.get('post_te_imputer_medians', {}):\n",
    "                 if X_data[col].isnull().any():\n",
    "                    print(f\"    Post-TE Imputing NaNs in {col} with stored median before scaling.\")\n",
    "                    X_data[col] = X_data[col].fillna(fitted_components['post_te_imputer_medians'][col])\n",
    "            # If there are still NaNs and no stored median, this might indicate an issue or need for a default fill\n",
    "            if X_data[col].isnull().any():\n",
    "                print(f\"    Warning: NaNs still present in {col} before scaling and no stored median. Filling with 0.\")\n",
    "                X_data[col] = X_data[col].fillna(0)\n",
    "\n",
    "\n",
    "        X_data_scaled_np = fitted_components['scaler'].transform(X_data[feature_columns_for_scaling])\n",
    "        print(f\"    Applied StandardScaler for features: {feature_columns_for_scaling}\")\n",
    "    elif not feature_columns_for_scaling and not X_data.empty: # No scaling was fitted\n",
    "        X_data_scaled_np = X_data.values\n",
    "    elif X_data.empty :\n",
    "        X_data_scaled_np = np.array([])\n",
    "    else: # Should not happen if feature_columns_for_scaling was defined during fit\n",
    "        X_data_scaled_np = X_data[feature_columns_for_scaling].values\n",
    "\n",
    "\n",
    "    print(\"  Pandas data transformation complete.\")\n",
    "    if y_data_np is not None:\n",
    "        return X_data_scaled_np, y_data_np\n",
    "    else:\n",
    "        return X_data_scaled_np, None\n",
    "\n",
    "\n",
    "def save_processed_data_and_components(X_np, y_np, file_path, components_dict, components_path):\n",
    "    \"\"\"Saves processed NumPy arrays and fitted components using joblib.\"\"\"\n",
    "    print(f\"  Saving processed data to: {file_path}\")\n",
    "    try:\n",
    "        payload = {'X': X_np, 'y': y_np}\n",
    "        joblib.dump(payload, file_path)\n",
    "        print(f\"    Data saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR saving data to {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "    if components_dict and components_path:\n",
    "        print(f\"  Saving preprocessor components to: {components_path}\")\n",
    "        try:\n",
    "            joblib.dump(components_dict, components_path)\n",
    "            print(f\"    Preprocessor components saved successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR saving components to {components_path}: {e}\")\n",
    "            # Don't raise if components fail but data saved, or handle as critical\n",
    "            \n",
    "print(\"--- Pandas Preprocessing Utility Functions Defined ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e92ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main Preprocessing Orchestration (Pandas - Updated for Dual Save)\n",
    "\n",
    "print(\"--- Starting Pandas Preprocessing Orchestration (Dual Save) ---\")\n",
    "\n",
    "# --- 0. Set MLflow Experiment for Preprocessing ---\n",
    "try:\n",
    "    preprocessing_mlflow_experiment_id = get_or_create_experiment(PREPROCESSING_EXPERIMENT_PATH, spark)\n",
    "    if preprocessing_mlflow_experiment_id:\n",
    "        mlflow.set_experiment(experiment_id=preprocessing_mlflow_experiment_id)\n",
    "        print(f\"MLflow experiment '{PREPROCESSING_EXPERIMENT_PATH}' for preprocessing is set with ID: {preprocessing_mlflow_experiment_id}\")\n",
    "    else:\n",
    "        raise Exception(\"Preprocessing MLflow experiment could not be set. Halting.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL: Could not initialize MLflow experiment for preprocessing. Error: {e}\")\n",
    "    # Consider dbutils.notebook.exit(\"MLflow experiment setup failed\")\n",
    "\n",
    "# --- 1. Load Raw Data ---\n",
    "print(f\"\\nLoading RAW training data for Pandas preprocessing...\")\n",
    "raw_train_pdf = load_raw_data_to_pandas(RAW_TRAIN_DATA_PATH) # Uses UC Volume path\n",
    "print(f\"\\nLoading RAW test data for Pandas preprocessing...\")\n",
    "raw_test_pdf = load_raw_data_to_pandas(RAW_TEST_DATA_PATH)   # Uses UC Volume path\n",
    "\n",
    "# --- 2. Fit Preprocessor on Training Data ---\n",
    "fitted_components_dict = None\n",
    "with mlflow.start_run(run_name=\"Pandas_Preprocessor_Fit_DualSave\") as preproc_run:\n",
    "    print(f\"\\nFitting preprocessor. MLflow Run ID: {preproc_run.info.run_id}\")\n",
    "    # ... (log params as before) ...\n",
    "    mlflow.log_param(\"label_column\", YOUR_LABEL_COLUMN_NAME)\n",
    "    mlflow.log_param(\"categorical_features_raw\", \", \".join(CATEGORICAL_COLUMNS_RAW))\n",
    "    mlflow.log_param(\"numerical_features_raw\", \", \".join(NUMERICAL_COLUMNS_RAW))\n",
    "    mlflow.log_param(\"target_encoding_smoothing\", TARGET_ENCODING_SMOOTHING)\n",
    "    mlflow.log_param(\"global_seed\", GLOBAL_SEED)\n",
    "    mlflow.set_tag(\"preprocessing_type\", \"pandas_mvp_dual_save\")\n",
    "\n",
    "    try:\n",
    "        # Fit_pandas_preprocessor now returns processed_pdf_named_cols as well\n",
    "        X_train_processed_np, y_train_processed_np, \\\n",
    "        processed_train_pdf_named_cols, fitted_components_dict = fit_pandas_preprocessor(\n",
    "            raw_train_pdf, CATEGORICAL_COLUMNS_RAW, NUMERICAL_COLUMNS_RAW,\n",
    "            YOUR_LABEL_COLUMN_NAME, TARGET_ENCODING_SMOOTHING, GLOBAL_SEED\n",
    "        )\n",
    "        print(f\"  Processed X_train_np shape: {X_train_processed_np.shape}, y_train_np shape: {y_train_processed_np.shape}\")\n",
    "        print(f\"  Processed train_pdf_named_cols shape: {processed_train_pdf_named_cols.shape}\")\n",
    "\n",
    "        # Save 1: NumPy arrays as .joblib (for HPO function)\n",
    "        save_joblib_data(\n",
    "            X_train_processed_np, y_train_processed_np, \n",
    "            SHARED_PROCESSED_TRAIN_PATH_JOBLIB # Path from Init Cell\n",
    "        )\n",
    "        \n",
    "        # Save 2: Pandas DataFrame with named columns as Parquet\n",
    "        save_named_cols_parquet(\n",
    "            processed_train_pdf_named_cols,\n",
    "            SHARED_PROCESSED_TRAIN_PARQUET_NAMED_COLS_PATH # Path from Init Cell (UC Volume path for Pandas to_parquet needs /dbfs/)\n",
    "        )\n",
    "        \n",
    "        # Save fitted components\n",
    "        save_preprocessor_components(fitted_components_dict, DBFS_PREPROCESSOR_COMPONENTS_PATH)\n",
    "        \n",
    "        mlflow.log_artifact(DBFS_PREPROCESSOR_COMPONENTS_PATH, artifact_path=MLFLOW_PANDAS_PREPROCESSOR_ARTIFACT_PATH)\n",
    "        # Log paths to the saved data as params or tags for traceability\n",
    "        mlflow.set_tag(\"processed_train_joblib_path\", SHARED_PROCESSED_TRAIN_PATH_JOBLIB.replace(\"/dbfs\", \"dbfs:\"))\n",
    "        mlflow.set_tag(\"processed_train_parquet_named_path\", SHARED_PROCESSED_TRAIN_PARQUET_NAMED_COLS_PATH.replace(\"/dbfs\", \"dbfs:\"))\n",
    "        print(f\"  Logged fitted preprocessor components and data paths to MLflow.\")\n",
    "        mlflow.set_tag(\"status_fit\", \"success\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR during preprocessor fitting: {e}\")\n",
    "        mlflow.log_param(\"error_fit\", str(e)[:250])\n",
    "        mlflow.set_tag(\"status_fit\", \"failed\")\n",
    "        raise\n",
    "\n",
    "# --- 3. Transform Test Data using Fitted Preprocessor ---\n",
    "if fitted_components_dict:\n",
    "    print(\"\\nTransforming TEST data using fitted preprocessor...\")\n",
    "    try:\n",
    "        # transform_pandas_preprocessor now also returns processed_pdf_named_cols\n",
    "        X_test_processed_np, y_test_processed_np, \\\n",
    "        processed_test_pdf_named_cols = transform_pandas_preprocessor(\n",
    "            raw_test_pdf, fitted_components_dict, CATEGORICAL_COLUMNS_RAW,\n",
    "            NUMERICAL_COLUMNS_RAW, YOUR_LABEL_COLUMN_NAME, is_train_data=False\n",
    "        )\n",
    "        print(f\"  Processed X_test_np shape: {X_test_processed_np.shape}, y_test_np shape: {y_test_processed_np.shape if y_test_processed_np is not None else 'N/A'}\")\n",
    "        print(f\"  Processed test_pdf_named_cols shape: {processed_test_pdf_named_cols.shape}\")\n",
    "\n",
    "        # Save 1: NumPy arrays as .joblib\n",
    "        save_joblib_data(\n",
    "            X_test_processed_np, y_test_processed_np,\n",
    "            SHARED_PROCESSED_TEST_PATH_JOBLIB # Path from Init Cell\n",
    "        )\n",
    "        \n",
    "        # Save 2: Pandas DataFrame with named columns as Parquet\n",
    "        save_named_cols_parquet(\n",
    "            processed_test_pdf_named_cols,\n",
    "            SHARED_PROCESSED_TEST_PARQUET_NAMED_COLS_PATH # Path from Init Cell\n",
    "        )\n",
    "        # In the MLflow run for fitting, we can also log the paths to test data if desired, or handle it separately.\n",
    "        # For simplicity, just saving them here. The HPO script will expect these paths.\n",
    "        print(\"  Test data transformed and saved in both formats.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR during test data transformation: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"CRITICAL: Preprocessor fitting failed. Cannot transform test data.\")\n",
    "\n",
    "print(\"\\n--- Pandas Preprocessing Orchestration (Dual Save) Completed ---\")\n",
    "print(f\"Joblib processed training data should be at: {SHARED_PROCESSED_TRAIN_PATH_JOBLIB}\")\n",
    "print(f\"Joblib processed test data should be at: {SHARED_PROCESSED_TEST_PATH_JOBLIB}\")\n",
    "print(f\"Named Parquet processed training data should be at: {SHARED_PROCESSED_TRAIN_PARQUET_NAMED_COLS_PATH}\")\n",
    "print(f\"Named Parquet processed test data should be at: {SHARED_PROCESSED_TEST_PARQUET_NAMED_COLS_PATH}\")\n",
    "print(f\"Fitted preprocessor components should be at: {DBFS_PREPROCESSOR_COMPONENTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f52ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-------------------- CELL 1: IMPORTS -------------------->\n",
    "print(\"Cell 1: Imports - Executing...\")\n",
    "import mlflow\n",
    "# No mlflow.spark specifically needed for this Pandas preprocessing script,\n",
    "# but mlflow itself is used.\n",
    "# The HPO script will use mlflow.sklearn, mlflow.lightgbm etc.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib # For saving/loading Python objects like scalers, arrays, and our components dict\n",
    "import time\n",
    "import shutil # For cleaning up temp directories if any (not used extensively here yet)\n",
    "\n",
    "from sklearn.model_selection import KFold # Kept for potential future K-Fold TE, not used in current TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce # For TargetEncoder\n",
    "\n",
    "from pyspark.sql import SparkSession # Still useful for environment context, paths, and future Spark ML\n",
    "\n",
    "# Suppress common warnings for cleaner output during MVP development\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning) # If category_encoders uses deprecated features\n",
    "\n",
    "# Ensure spark session is available (Databricks notebooks usually provide 'spark')\n",
    "if 'spark' not in locals():\n",
    "    spark = SparkSession.builder.appName(\"Pandas_Preprocessing_MVP_Full\").getOrCreate()\n",
    "    print(\"SparkSession created.\")\n",
    "else:\n",
    "    print(\"SparkSession already exists.\")\n",
    "\n",
    "print(\"Imports successful for Pandas Preprocessing.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# <-------------------- CELL 2: INIT CELL - GLOBAL CONFIGURATIONS -------------------->\n",
    "print(\"\\nCell 2: Global Configurations - Executing...\")\n",
    "\n",
    "# --- MLflow Configuration ---\n",
    "# !!! IMPORTANT: SET YOUR MLFLOW EXPERIMENT PATH !!!\n",
    "# Example: /Users/your.email@domain.com/MyProjectExperiment_Preprocessing\n",
    "# You can get your username programmatically in Databricks:\n",
    "# current_user = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "# EXPERIMENT_PATH = f\"/Users/{current_user}/MVP_Regression_Pandas_Preprocessing_Full\"\n",
    "EXPERIMENT_PATH = \"/Users/your_username@example.com/MVP_Regression_Pandas_Preprocessing_Full\" # CHANGE THIS\n",
    "\n",
    "# --- Data Paths (Unity Catalog Volumes) ---\n",
    "# !!! IMPORTANT: SET YOUR UNITY CATALOG VOLUME BASE PATH !!!\n",
    "# Example: \"/Volumes/my_main_catalog/my_bronze_schema/my_project_volume/\"\n",
    "UC_BASE_DATA_PATH = \"/Volumes/delfos/\" # As per your input\n",
    "\n",
    "# --- Paths for RAW Data (Input to this preprocessing script) ---\n",
    "# !!! IMPORTANT: UPDATE THESE TO YOUR ACTUAL RAW DATA PATHS IN UC VOLUMES !!!\n",
    "RAW_TRAIN_DATA_PATH = f\"{UC_BASE_DATA_PATH}raw_data/train.parquet\" # Example\n",
    "RAW_TEST_DATA_PATH = f\"{UC_BASE_DATA_PATH}raw_data/test.parquet\"   # Example\n",
    "\n",
    "# --- Paths for PROCESSED Data (Output of this Pandas preprocessing script) ---\n",
    "# Using a versioned directory for processed data\n",
    "PROCESSED_DATA_VERSION = \"v1_pandas\"\n",
    "DBFS_PROCESSED_DATA_DIR_BASE = f\"/dbfs{UC_BASE_DATA_PATH}processed_data/\" # /dbfs/ prefix for Python os/joblib\n",
    "PROCESSED_DATA_DIR_VERSIONED = os.path.join(DBFS_PROCESSED_DATA_DIR_BASE, PROCESSED_DATA_VERSION)\n",
    "\n",
    "# For .joblib files (NumPy arrays for HPO objective function)\n",
    "SHARED_PROCESSED_TRAIN_PATH_JOBLIB = os.path.join(PROCESSED_DATA_DIR_VERSIONED, \"train_processed_data.joblib\")\n",
    "SHARED_PROCESSED_TEST_PATH_JOBLIB = os.path.join(PROCESSED_DATA_DIR_VERSIONED, \"test_processed_data.joblib\")\n",
    "\n",
    "# For Parquet files with named columns (for inspection, other tools)\n",
    "SHARED_PROCESSED_TRAIN_PARQUET_NAMED_COLS_PATH = os.path.join(PROCESSED_DATA_DIR_VERSIONED, \"train_processed_named_cols.parquet\")\n",
    "SHARED_PROCESSED_TEST_PARQUET_NAMED_COLS_PATH = os.path.join(PROCESSED_DATA_DIR_VERSIONED, \"test_processed_named_cols.parquet\")\n",
    "\n",
    "# --- Path for saving the FITTED PREPROCESSING COMPONENTS ---\n",
    "DBFS_PREPROCESSOR_COMPONENTS_PATH = os.path.join(PROCESSED_DATA_DIR_VERSIONED, \"preprocessor_components.joblib\")\n",
    "\n",
    "# !!! IMPORTANT: SET YOUR ACTUAL LABEL COLUMN NAME (must exist in raw data) !!!\n",
    "YOUR_LABEL_COLUMN_NAME = \"target\" # Example, change this\n",
    "\n",
    "# --- Define your categorical and numerical columns from the RAW data ---\n",
    "# !!! IMPORTANT: UPDATE THESE LISTS BASED ON YOUR ACTUAL RAW DATASET !!!\n",
    "CATEGORICAL_COLUMNS_RAW = [\"category_feature_1\", \"category_feature_2\"] # Example\n",
    "NUMERICAL_COLUMNS_RAW = [\"numerical_feature_1\", \"numerical_feature_2\", \"numerical_feature_3\"] # Example\n",
    "\n",
    "# --- MLflow Configuration for saving the preprocessing \"model\" (components) ---\n",
    "MLFLOW_PANDAS_PREPROCESSOR_ARTIFACT_PATH = \"pandas_preprocessor_components\" # Directory in MLflow artifacts\n",
    "\n",
    "# --- Target Encoding Configuration ---\n",
    "TARGET_ENCODING_SMOOTHING = 20.0 # Smoothing factor for category_encoders.TargetEncoder\n",
    "\n",
    "# --- Reproducibility ---\n",
    "GLOBAL_SEED = 117\n",
    "\n",
    "# --- Ensure output directories exist (using /dbfs/ path for os.makedirs) ---\n",
    "try:\n",
    "    os.makedirs(PROCESSED_DATA_DIR_VERSIONED, exist_ok=True)\n",
    "    print(f\"Checked/created base processed data directory: {PROCESSED_DATA_DIR_VERSIONED}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create directory {PROCESSED_DATA_DIR_VERSIONED} using os.makedirs. Error: {e}\")\n",
    "    print(\"Ensure the UC Volume path is correct and accessible if this fails.\")\n",
    "\n",
    "print(f\"--- Global Configurations Initialized (Pandas Preprocessing) ---\")\n",
    "print(f\"MLflow Experiment Path for Preprocessing: {EXPERIMENT_PATH}\")\n",
    "print(f\"Unity Catalog Base Data Path: {UC_BASE_DATA_PATH}\")\n",
    "print(f\"Raw Train Data Path: {RAW_TRAIN_DATA_PATH}\")\n",
    "print(f\"Raw Test Data Path: {RAW_TEST_DATA_PATH}\")\n",
    "print(f\"  Output Joblib Processed Train Data Path: {SHARED_PROCESSED_TRAIN_PATH_JOBLIB}\")\n",
    "print(f\"  Output Joblib Processed Test Data Path: {SHARED_PROCESSED_TEST_PATH_JOBLIB}\")\n",
    "print(f\"  Output Parquet (Named Cols) Processed Train Path: {SHARED_PROCESSED_TRAIN_PARQUET_NAMED_COLS_PATH}\")\n",
    "print(f\"  Output Parquet (Named Cols) Processed Test Path: {SHARED_PROCESSED_TEST_PARQUET_NAMED_COLS_PATH}\")\n",
    "print(f\"  Output Preprocessor Components Path: {DBFS_PREPROCESSOR_COMPONENTS_PATH}\")\n",
    "print(f\"Label Column: {YOUR_LABEL_COLUMN_NAME}\")\n",
    "print(f\"Categorical Columns (Raw): {CATEGORICAL_COLUMNS_RAW}\")\n",
    "print(f\"Numerical Columns (Raw): {NUMERICAL_COLUMNS_RAW}\")\n",
    "print(f\"Global Seed: {GLOBAL_SEED}\")\n",
    "print(f\"Target Encoding Smoothing: {TARGET_ENCODING_SMOOTHING}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# <-------------------- CELL 3: UTILITY FUNCTIONS & PANDAS PREPROCESSING LOGIC -------------------->\n",
    "print(\"\\nCell 3: Utility Functions & Pandas Preprocessing Logic - Defining...\")\n",
    "\n",
    "# --- MLflow Utility ---\n",
    "def get_or_create_experiment(experiment_name_param, spark_session_param): # Added params to avoid global clashes\n",
    "    \"\"\"Safely creates or fetches an MLflow experiment.\"\"\"\n",
    "    try:\n",
    "        # In Databricks, experiment names can be full paths\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name_param)\n",
    "        if experiment:\n",
    "            print(f\"MLflow experiment '{experiment_name_param}' found with ID: {experiment.experiment_id}\")\n",
    "            return experiment.experiment_id\n",
    "        else:\n",
    "            print(f\"MLflow experiment '{experiment_name_param}' not found. Attempting to create.\")\n",
    "            # Note: Artifact location can be specified for UC experiments if needed,\n",
    "            # e.g., f\"uc://{catalog}.{schema}.{volume_or_dir}\"\n",
    "            # For now, default artifact location.\n",
    "            experiment_id = mlflow.create_experiment(name=experiment_name_param)\n",
    "            print(f\"MLflow experiment '{experiment_name_param}' created with ID: {experiment_id}\")\n",
    "            return experiment_id\n",
    "    except mlflow.exceptions.MlflowException as e:\n",
    "        if \"RESOURCE_ALREADY_EXISTS\" in str(e) or (\"Experiment with name\" in str(e) and \"already exists\" in str(e)):\n",
    "            print(f\"Race condition or experiment '{experiment_name_param}' was created concurrently. Fetching again.\")\n",
    "            experiment = mlflow.get_experiment_by_name(experiment_name_param)\n",
    "            if experiment:\n",
    "                print(f\"Successfully fetched concurrently created experiment '{experiment_name_param}' with ID: {experiment.experiment_id}\")\n",
    "                return experiment.experiment_id\n",
    "        print(f\"MLflowException: Could not get or create experiment '{experiment_name_param}'. Error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in get_or_create_experiment for '{experiment_name_param}'. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Pandas Preprocessing Functions ---\n",
    "def load_raw_data_to_pandas_from_uc_volume(uc_volume_parquet_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads a Parquet file from a UC Volume path into a Pandas DataFrame using /dbfs/ prefix.\"\"\"\n",
    "    dbfs_path = uc_volume_parquet_path\n",
    "    if uc_volume_parquet_path.startswith(\"/Volumes/\"):\n",
    "        dbfs_path = f\"/dbfs{uc_volume_parquet_path}\"\n",
    "    else: # Assume it might already be a /dbfs/ path or other local path\n",
    "        print(f\"Warning: Path '{uc_volume_parquet_path}' does not start with /Volumes/. Assuming it's a direct /dbfs/ path or local.\")\n",
    "\n",
    "    print(f\"  Attempting to load Pandas DataFrame from: {dbfs_path}\")\n",
    "    try:\n",
    "        pdf = pd.read_parquet(dbfs_path)\n",
    "        print(f\"    Successfully loaded. Shape: {pdf.shape}\")\n",
    "        return pdf\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR loading Parquet from {dbfs_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def fit_pandas_preprocessor_mvp(train_pdf: pd.DataFrame, \n",
    "                                categorical_cols: list, \n",
    "                                numerical_cols: list, \n",
    "                                label_col: str,\n",
    "                                te_smoothing_factor: float): # Renamed for clarity\n",
    "    \"\"\"\n",
    "    Fits preprocessing components (imputers, target encoders, scaler) on training data (Pandas).\n",
    "    Returns:\n",
    "        - X_train_processed_df_named_cols: Pandas DataFrame with original feature names but transformed & scaled values.\n",
    "        - y_train_series: Pandas Series of labels.\n",
    "        - fitted_components: Dictionary of fitted components.\n",
    "    \"\"\"\n",
    "    print(\"  Fitting Pandas preprocessor (MVP version)...\")\n",
    "    X_train_pd_intermediate = train_pdf.drop(columns=[label_col], errors='ignore').copy()\n",
    "    y_train_series = train_pdf[label_col].astype(float).copy()\n",
    "\n",
    "    fitted_components = {}\n",
    "\n",
    "    # 1. Impute Numerical Features (Median)\n",
    "    if numerical_cols:\n",
    "        # Ensure only existing columns are processed\n",
    "        valid_numerical_cols = [col for col in numerical_cols if col in X_train_pd_intermediate.columns]\n",
    "        if valid_numerical_cols:\n",
    "            num_imputer = SimpleImputer(strategy=\"median\")\n",
    "            X_train_pd_intermediate[valid_numerical_cols] = num_imputer.fit_transform(X_train_pd_intermediate[valid_numerical_cols])\n",
    "            fitted_components['numerical_imputer'] = num_imputer\n",
    "            fitted_components['numerical_cols_fitted'] = valid_numerical_cols # Store actual columns used\n",
    "            print(f\"    Fitted Numerical Imputer for: {valid_numerical_cols}\")\n",
    "        else:\n",
    "            print(\"    No valid numerical columns found in data for imputation.\")\n",
    "    else:\n",
    "        print(\"    No numerical columns specified for imputation.\")\n",
    "\n",
    "\n",
    "    # 2. Impute Categorical Features (Using a constant string)\n",
    "    if categorical_cols:\n",
    "        valid_categorical_cols = [col for col in categorical_cols if col in X_train_pd_intermediate.columns]\n",
    "        if valid_categorical_cols:\n",
    "            cat_imputer_fill_value = \"__MISSING_CATEGORY__\" # More explicit fill value\n",
    "            cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=cat_imputer_fill_value)\n",
    "            X_train_pd_intermediate[valid_categorical_cols] = cat_imputer.fit_transform(X_train_pd_intermediate[valid_categorical_cols])\n",
    "            fitted_components['categorical_imputer'] = cat_imputer\n",
    "            fitted_components['categorical_cols_fitted_for_impute'] = valid_categorical_cols\n",
    "            print(f\"    Fitted Categorical Imputer for: {valid_categorical_cols} (using fill_value='{cat_imputer_fill_value}')\")\n",
    "        else:\n",
    "            print(\"    No valid categorical columns found in data for imputation.\")\n",
    "    else:\n",
    "        print(\"    No categorical columns specified for imputation.\")\n",
    "\n",
    "\n",
    "    # 3. Target Encoding for Categorical Features (using category_encoders)\n",
    "    # Ensure `categorical_cols_fitted_for_impute` or original `valid_categorical_cols` are used\n",
    "    te_input_cols = fitted_components.get('categorical_cols_fitted_for_impute', [])\n",
    "    if te_input_cols: # Only proceed if there were categorical columns after imputation check\n",
    "        # Ensure columns are object or category type for TargetEncoder\n",
    "        for col in te_input_cols:\n",
    "            X_train_pd_intermediate[col] = X_train_pd_intermediate[col].astype('category')\n",
    "\n",
    "        target_encoder = ce.TargetEncoder(\n",
    "            cols=te_input_cols, \n",
    "            smoothing=te_smoothing_factor,\n",
    "            handle_unknown='value', # Uses mean of y_train for unseen categories\n",
    "            handle_missing='value'  # Uses mean of y_train for NaNs (already imputed, but good fallback)\n",
    "        )\n",
    "        \n",
    "        # TargetEncoder transforms specified columns in place or returns only those.\n",
    "        # It should return a DataFrame with the same columns as input if cols is specified.\n",
    "        X_train_pd_intermediate[te_input_cols] = target_encoder.fit_transform(X_train_pd_intermediate[te_input_cols], y_train_series)\n",
    "        fitted_components['target_encoder'] = target_encoder\n",
    "        fitted_components['categorical_cols_target_encoded'] = te_input_cols # Store which cols were TE'd\n",
    "        print(f\"    Fitted TargetEncoder for: {te_input_cols} with smoothing={te_smoothing_factor}\")\n",
    "    else:\n",
    "        print(\"    No categorical columns available for Target Encoding.\")\n",
    "\n",
    "    # 4. Scale All Features (Original Numerical + Target Encoded Categoricals)\n",
    "    # Columns that were originally numerical + columns that were categorical (now numerically target-encoded)\n",
    "    feature_columns_for_scaling = fitted_components.get('numerical_cols_fitted', []) + \\\n",
    "                                  fitted_components.get('categorical_cols_target_encoded', [])\n",
    "    \n",
    "    X_train_processed_df_named_cols = X_train_pd_intermediate.copy() # This will hold final scaled values with names\n",
    "\n",
    "    if feature_columns_for_scaling:\n",
    "        # Ensure all columns for scaling are float and impute any new NaNs that might have emerged\n",
    "        # (e.g., if TE produced NaNs, though `handle_unknown='value'` should prevent this if y_train is clean)\n",
    "        final_impute_medians = {}\n",
    "        for col in feature_columns_for_scaling:\n",
    "            X_train_processed_df_named_cols[col] = X_train_processed_df_named_cols[col].astype(float) # Ensure float\n",
    "            if X_train_processed_df_named_cols[col].isnull().any():\n",
    "                median_val = X_train_processed_df_named_cols[col].median()\n",
    "                X_train_processed_df_named_cols[col] = X_train_processed_df_named_cols[col].fillna(median_val)\n",
    "                final_impute_medians[col] = median_val\n",
    "                print(f\"    Post-TE/Pre-Scaling Imputing NaNs in {col} with median: {median_val}\")\n",
    "        if final_impute_medians:\n",
    "            fitted_components['post_te_imputer_medians'] = final_impute_medians\n",
    "            \n",
    "        scaler = StandardScaler()\n",
    "        X_train_processed_df_named_cols[feature_columns_for_scaling] = scaler.fit_transform(X_train_processed_df_named_cols[feature_columns_for_scaling])\n",
    "        \n",
    "        fitted_components['scaler'] = scaler\n",
    "        fitted_components['feature_columns_for_scaling'] = feature_columns_for_scaling # Store order and names\n",
    "        print(f\"    Fitted StandardScaler for features: {feature_columns_for_scaling}\")\n",
    "    else:\n",
    "        print(\"    No features identified for scaling. Output X will be based on previous steps.\")\n",
    "\n",
    "    # Add label back for the Parquet output with named columns\n",
    "    X_train_processed_df_named_cols[label_col] = y_train_series.values\n",
    "\n",
    "    print(\"  Pandas preprocessor fitting complete.\")\n",
    "    return X_train_processed_df_named_cols, y_train_series, fitted_components\n",
    "\n",
    "\n",
    "def transform_pandas_preprocessor_mvp(raw_pdf: pd.DataFrame, \n",
    "                                      fitted_components: dict,\n",
    "                                      label_col: str = None): # Cat/Num cols are derived from fitted_components\n",
    "    \"\"\"\n",
    "    Applies fitted Pandas preprocessing components to new data.\n",
    "    Returns:\n",
    "        - processed_pdf_named_cols: Pandas DataFrame with transformed values under original feature names (plus label if present).\n",
    "        - y_data_series: Pandas Series of labels (if label_col present).\n",
    "    \"\"\"\n",
    "    print(\"  Transforming data with fitted Pandas preprocessor (MVP version)...\")\n",
    "    \n",
    "    X_data_pd = raw_pdf.drop(columns=[label_col], errors='ignore').copy() if label_col and label_col in raw_pdf.columns else raw_pdf.copy()\n",
    "    y_data_series = raw_pdf[label_col].astype(float).copy() if label_col and label_col in raw_pdf.columns else None\n",
    "\n",
    "    # 1. Impute Numerical\n",
    "    numerical_cols_fitted = fitted_components.get('numerical_cols_fitted', [])\n",
    "    if numerical_cols_fitted and 'numerical_imputer' in fitted_components:\n",
    "        # Ensure only existing columns are processed\n",
    "        valid_numerical_cols_to_transform = [col for col in numerical_cols_fitted if col in X_data_pd.columns]\n",
    "        if valid_numerical_cols_to_transform:\n",
    "           X_data_pd[valid_numerical_cols_to_transform] = fitted_components['numerical_imputer'].transform(X_data_pd[valid_numerical_cols_to_transform])\n",
    "           print(f\"    Applied Numerical Imputer for: {valid_numerical_cols_to_transform}\")\n",
    "    \n",
    "    # 2. Impute Categorical\n",
    "    categorical_cols_fitted_for_impute = fitted_components.get('categorical_cols_fitted_for_impute', [])\n",
    "    if categorical_cols_fitted_for_impute and 'categorical_imputer' in fitted_components:\n",
    "        valid_cat_cols_to_transform = [col for col in categorical_cols_fitted_for_impute if col in X_data_pd.columns]\n",
    "        if valid_cat_cols_to_transform:\n",
    "            X_data_pd[valid_cat_cols_to_transform] = fitted_components['categorical_imputer'].transform(X_data_pd[valid_cat_cols_to_transform])\n",
    "            print(f\"    Applied Categorical Imputer for: {valid_cat_cols_to_transform}\")\n",
    "    \n",
    "    # 3. Target Encode Categorical\n",
    "    categorical_cols_target_encoded = fitted_components.get('categorical_cols_target_encoded', [])\n",
    "    if categorical_cols_target_encoded and 'target_encoder' in fitted_components:\n",
    "        # Ensure dtype for category_encoders\n",
    "        valid_te_cols_to_transform = [col for col in categorical_cols_target_encoded if col in X_data_pd.columns]\n",
    "        if valid_te_cols_to_transform:\n",
    "            for col in valid_te_cols_to_transform:\n",
    "                X_data_pd[col] = X_data_pd[col].astype('category')\n",
    "            \n",
    "            # TargetEncoder's transform only needs X. y is ignored if provided.\n",
    "            X_data_pd[valid_te_cols_to_transform] = fitted_components['target_encoder'].transform(X_data_pd[valid_te_cols_to_transform])\n",
    "            print(f\"    Applied TargetEncoder for: {valid_te_cols_to_transform}\")\n",
    "\n",
    "    # 4. Scale All Features\n",
    "    feature_columns_for_scaling = fitted_components.get('feature_columns_for_scaling', [])\n",
    "    processed_pdf_named_cols = X_data_pd.copy()\n",
    "\n",
    "    if feature_columns_for_scaling and 'scaler' in fitted_components:\n",
    "        valid_scale_cols_to_transform = [col for col in feature_columns_for_scaling if col in processed_pdf_named_cols.columns]\n",
    "        if valid_scale_cols_to_transform:\n",
    "            # Ensure float and impute post-TE NaNs using medians learned during fit\n",
    "            post_te_imputer_medians = fitted_components.get('post_te_imputer_medians', {})\n",
    "            for col in valid_scale_cols_to_transform:\n",
    "                processed_pdf_named_cols[col] = processed_pdf_named_cols[col].astype(float)\n",
    "                if processed_pdf_named_cols[col].isnull().any():\n",
    "                    fill_val = post_te_imputer_medians.get(col, 0) # Fallback to 0 if somehow not in medians\n",
    "                    processed_pdf_named_cols[col] = processed_pdf_named_cols[col].fillna(fill_val)\n",
    "                    print(f\"    Post-TE/Pre-Scaling Imputing NaNs in {col} with: {fill_val}\")\n",
    "            \n",
    "            processed_pdf_named_cols[valid_scale_cols_to_transform] = fitted_components['scaler'].transform(processed_pdf_named_cols[valid_scale_cols_to_transform])\n",
    "            print(f\"    Applied StandardScaler for features: {valid_scale_cols_to_transform}\")\n",
    "    else:\n",
    "        print(\"    No features to scale or scaler not found in fitted_components.\")\n",
    "\n",
    "\n",
    "    if y_data_series is not None: # Add original label back if it was present\n",
    "        processed_pdf_named_cols[label_col] = y_data_series.values\n",
    "\n",
    "    print(\"  Pandas data transformation complete.\")\n",
    "    return processed_pdf_named_cols, y_data_series\n",
    "\n",
    "\n",
    "def save_processed_outputs(processed_pdf_named_cols: pd.DataFrame, \n",
    "                           label_series: pd.Series, # Can be None for test data without label\n",
    "                           feature_cols_in_order: list, # To extract X_np correctly\n",
    "                           joblib_file_path: str, \n",
    "                           parquet_file_path: str,\n",
    "                           label_col_name_in_output: str):\n",
    "    \"\"\"Saves processed data as both joblib (NumPy) and Parquet (named cols).\"\"\"\n",
    "    \n",
    "    # 1. Prepare NumPy arrays for joblib\n",
    "    if feature_cols_in_order and not processed_pdf_named_cols[feature_cols_in_order].empty:\n",
    "        X_np = processed_pdf_named_cols[feature_cols_in_order].values\n",
    "    elif not processed_pdf_named_cols.empty and not feature_cols_in_order: # E.g. if all columns were features after label drop\n",
    "        temp_X_pdf = processed_pdf_named_cols.drop(columns=[label_col_name_in_output], errors='ignore')\n",
    "        X_np = temp_X_pdf.values\n",
    "        print(f\"Warning: feature_cols_in_order was empty, derived X_np from all columns minus label. Shape: {X_np.shape}\")\n",
    "    else:\n",
    "        X_np = np.array([])\n",
    "        print(\"Warning: X_np is empty.\")\n",
    "\n",
    "    y_np = label_series.values if label_series is not None else np.array([])\n",
    "\n",
    "    # Save joblib\n",
    "    dbfs_joblib_path = joblib_file_path\n",
    "    if joblib_file_path.startswith(\"/Volumes/\"): dbfs_joblib_path = f\"/dbfs{joblib_file_path}\"\n",
    "    print(f\"  Saving joblib data (X_np shape: {X_np.shape}, y_np shape: {y_np.shape}) to: {dbfs_joblib_path}\")\n",
    "    try:\n",
    "        payload = {'X': X_np, 'y': y_np}\n",
    "        joblib.dump(payload, dbfs_joblib_path)\n",
    "        print(f\"    Joblib data saved successfully to {dbfs_joblib_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR saving joblib data to {dbfs_joblib_path}: {e}\")\n",
    "        # raise # Decide if this is a critical error to stop the flow\n",
    "\n",
    "    # Save Parquet with named columns\n",
    "    dbfs_parquet_path = parquet_file_path\n",
    "    if parquet_file_path.startswith(\"/Volumes/\"): dbfs_parquet_path = f\"/dbfs{parquet_file_path}\"\n",
    "    print(f\"  Saving named cols Parquet data (shape: {processed_pdf_named_cols.shape}) to: {dbfs_parquet_path}\")\n",
    "    try:\n",
    "        # Ensure label column is correctly named if it was added back\n",
    "        df_to_save = processed_pdf_named_cols.copy()\n",
    "        if label_series is not None and label_col_name_in_output not in df_to_save.columns:\n",
    "            df_to_save[label_col_name_in_output] = label_series.values\n",
    "        \n",
    "        df_to_save.to_parquet(dbfs_parquet_path, index=False)\n",
    "        print(f\"    Named cols Parquet data saved successfully to {dbfs_parquet_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR saving named cols Parquet to {dbfs_parquet_path}: {e}\")\n",
    "        # raise\n",
    "\n",
    "def save_fitted_components_artifact(components_dict, components_dbfs_path, mlflow_artifact_path_name):\n",
    "    \"\"\"Saves fitted components locally (via /dbfs/) and logs to MLflow.\"\"\"\n",
    "    if components_dict and components_dbfs_path:\n",
    "        print(f\"  Saving preprocessor components to: {components_dbfs_path}\")\n",
    "        try:\n",
    "            joblib.dump(components_dict, components_dbfs_path)\n",
    "            print(f\"    Preprocessor components saved successfully to {components_dbfs_path}.\")\n",
    "            # Log components as artifact in MLflow\n",
    "            mlflow.log_artifact(components_dbfs_path, artifact_path=mlflow_artifact_path_name)\n",
    "            print(f\"    Logged fitted preprocessor components to MLflow artifact path: {mlflow_artifact_path_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR saving/logging components from {components_dbfs_path}: {e}\")\n",
    "            # Decide if this is critical\n",
    "            \n",
    "print(\"--- All Pandas Preprocessing Utility Functions Defined ---\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# <-------------------- CELL 4: MAIN PREPROCESSING ORCHESTRATION (PANDAS) -------------------->\n",
    "print(\"\\nCell 4: Main Pandas Preprocessing Orchestration - Executing...\")\n",
    "\n",
    "# --- 0. Set MLflow Experiment for Preprocessing ---\n",
    "# This variable `preprocessing_mlflow_experiment_id` is used in the MLflow run context\n",
    "global preprocessing_mlflow_experiment_id # Make it global if used inside functions called later\n",
    "preprocessing_mlflow_experiment_id = None\n",
    "try:\n",
    "    preprocessing_mlflow_experiment_id = get_or_create_experiment(EXPERIMENT_PATH, spark) # Use EXPERIMENT_PATH from Init\n",
    "    if preprocessing_mlflow_experiment_id:\n",
    "        mlflow.set_experiment(experiment_id=preprocessing_mlflow_experiment_id)\n",
    "        print(f\"MLflow experiment '{EXPERIMENT_PATH}' for preprocessing is set with ID: {preprocessing_mlflow_experiment_id}\")\n",
    "    else:\n",
    "        raise Exception(\"Preprocessing MLflow experiment could not be set. Halting.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL: Could not initialize MLflow experiment for preprocessing. Error: {e}\")\n",
    "    # In a real script, you might use dbutils.notebook.exit(\"MLflow experiment setup failed\") here\n",
    "\n",
    "# Ensure execution proceeds only if experiment is set\n",
    "if preprocessing_mlflow_experiment_id:\n",
    "    # --- 1. Load Raw Data into Pandas DataFrames ---\n",
    "    print(f\"\\nLoading RAW training data for Pandas preprocessing...\")\n",
    "    raw_train_pdf = load_raw_data_to_pandas_from_uc_volume(RAW_TRAIN_DATA_PATH)\n",
    "    print(f\"RAW training data columns: {raw_train_pdf.columns.tolist()}\")\n",
    "    raw_train_pdf.info(verbose=True, show_counts=True)\n",
    "\n",
    "\n",
    "    print(f\"\\nLoading RAW test data for Pandas preprocessing...\")\n",
    "    raw_test_pdf = load_raw_data_to_pandas_from_uc_volume(RAW_TEST_DATA_PATH)\n",
    "    raw_test_pdf.info(verbose=True, show_counts=True)\n",
    "\n",
    "\n",
    "    # --- 2. Fit Preprocessor on Training Data & Save Outputs ---\n",
    "    fitted_components_dict = None # Initialize\n",
    "    with mlflow.start_run(run_name=\"Pandas_Preprocessor_Fit_Save_MVP\") as preproc_run:\n",
    "        run_id_for_preproc = preproc_run.info.run_id\n",
    "        print(f\"\\nFitting preprocessor and saving outputs. MLflow Run ID: {run_id_for_preproc}\")\n",
    "        mlflow.log_param(\"label_column\", YOUR_LABEL_COLUMN_NAME)\n",
    "        mlflow.log_param(\"categorical_features_raw\", \", \".join(CATEGORICAL_COLUMNS_RAW))\n",
    "        mlflow.log_param(\"numerical_features_raw\", \", \".join(NUMERICAL_COLUMNS_RAW))\n",
    "        mlflow.log_param(\"target_encoding_smoothing\", TARGET_ENCODING_SMOOTHING)\n",
    "        mlflow.log_param(\"global_seed_config\", GLOBAL_SEED) # Though seed is less impactful here than in model training\n",
    "        mlflow.set_tag(\"preprocessing_type\", \"pandas_mvp_full_script\")\n",
    "        mlflow.set_tag(\"data_version_processed\", PROCESSED_DATA_VERSION)\n",
    "\n",
    "        try:\n",
    "            # Fit preprocessor\n",
    "            processed_train_pdf_named_cols, y_train_series, \\\n",
    "            fitted_components_dict = fit_pandas_preprocessor_mvp(\n",
    "                raw_train_pdf,\n",
    "                CATEGORICAL_COLUMNS_RAW,\n",
    "                NUMERICAL_COLUMNS_RAW,\n",
    "                YOUR_LABEL_COLUMN_NAME,\n",
    "                TARGET_ENCODING_SMOOTHING\n",
    "            )\n",
    "            print(f\"  Fit complete. Processed train_pdf_named_cols shape: {processed_train_pdf_named_cols.shape}, y_train_series shape: {y_train_series.shape}\")\n",
    "\n",
    "            # Save processed training data (both formats)\n",
    "            save_processed_outputs(\n",
    "                processed_pdf_named_cols=processed_train_pdf_named_cols,\n",
    "                label_series=y_train_series, # Pass y_train_series to ensure label column is handled correctly for Parquet save\n",
    "                feature_cols_in_order=fitted_components_dict.get('feature_columns_for_scaling', []), # Important for X_np\n",
    "                joblib_file_path=SHARED_PROCESSED_TRAIN_PATH_JOBLIB,\n",
    "                parquet_file_path=SHARED_PROCESSED_TRAIN_PARQUET_NAMED_COLS_PATH,\n",
    "                label_col_name_in_output=YOUR_LABEL_COLUMN_NAME\n",
    "            )\n",
    "            \n",
    "            # Save fitted components and log to MLflow\n",
    "            save_fitted_components_artifact(fitted_components_dict, DBFS_PREPROCESSOR_COMPONENTS_PATH, MLFLOW_PANDAS_PREPROCESSOR_ARTIFACT_PATH)\n",
    "            \n",
    "            # Log paths to the saved data as params or tags for traceability\n",
    "            mlflow.set_tag(\"processed_train_joblib_path\", SHARED_PROCESSED_TRAIN_PATH_JOBLIB.replace(\"/dbfs\", \"dbfs:\"))\n",
    "            mlflow.set_tag(\"processed_train_parquet_named_path\", SHARED_PROCESSED_TRAIN_PARQUET_NAMED_COLS_PATH.replace(\"/dbfs\", \"dbfs:\"))\n",
    "            mlflow.set_tag(\"preprocessor_components_path\", DBFS_PREPROCESSOR_COMPONENTS_PATH.replace(\"/dbfs\", \"dbfs:\"))\n",
    "            mlflow.set_tag(\"status_fit\", \"success\")\n",
    "            print(\"  Training data processed and components saved successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR during preprocessor fitting or saving training data: {e}\")\n",
    "            mlflow.log_param(\"error_fit\", str(e)[:250]) # Log error to MLflow\n",
    "            mlflow.set_tag(\"status_fit\", \"failed\")\n",
    "            # dbutils.notebook.exit(f\"Preprocessing fitting failed: {e}\") # Halt on critical error\n",
    "            raise # Re-raise to ensure failure is noted\n",
    "\n",
    "    # --- 3. Transform Test Data using Fitted Preprocessor & Save Outputs ---\n",
    "    if fitted_components_dict: # Proceed only if fitting was successful\n",
    "        print(\"\\nTransforming TEST data using fitted preprocessor...\")\n",
    "        # No new MLflow run needed here, this is part of the same preprocessing \"job\"\n",
    "        # Or, if preferred, could be a separate run for \"transform_test_data\"\n",
    "        try:\n",
    "            processed_test_pdf_named_cols, y_test_series = transform_pandas_preprocessor_mvp(\n",
    "                raw_test_pdf,\n",
    "                fitted_components_dict,\n",
    "                label_col=YOUR_LABEL_COLUMN_NAME # Pass label_col if test set has it for consistency in y_test_series\n",
    "            )\n",
    "            print(f\"  Test data transformed. Processed test_pdf_named_cols shape: {processed_test_pdf_named_cols.shape}\")\n",
    "            if y_test_series is not None:\n",
    "                print(f\"  y_test_series shape: {y_test_series.shape}\")\n",
    "\n",
    "            # Save processed test data (both formats)\n",
    "            save_processed_outputs(\n",
    "                processed_pdf_named_cols=processed_test_pdf_named_cols,\n",
    "                label_series=y_test_series,\n",
    "                feature_cols_in_order=fitted_components_dict.get('feature_columns_for_scaling', []),\n",
    "                joblib_file_path=SHARED_PROCESSED_TEST_PATH_JOBLIB,\n",
    "                parquet_file_path=SHARED_PROCESSED_TEST_PARQUET_NAMED_COLS_PATH,\n",
    "                label_col_name_in_output=YOUR_LABEL_COLUMN_NAME\n",
    "            )\n",
    "            # Log test data paths to the same MLflow run as the preprocessor components\n",
    "            with mlflow.start_run(run_id=run_id_for_preproc, nested=False): # Re-open run if closed or use existing\n",
    "                 mlflow.set_tag(\"processed_test_joblib_path\", SHARED_PROCESSED_TEST_PATH_JOBLIB.replace(\"/dbfs\", \"dbfs:\"))\n",
    "                 mlflow.set_tag(\"processed_test_parquet_named_path\", SHARED_PROCESSED_TEST_PARQUET_NAMED_COLS_PATH.replace(\"/dbfs\", \"dbfs:\"))\n",
    "                 mlflow.set_tag(\"status_transform_test\", \"success\")\n",
    "\n",
    "            print(\"  Test data transformed and saved in both formats.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR during test data transformation or saving: {e}\")\n",
    "            with mlflow.start_run(run_id=run_id_for_preproc, nested=False): # Re-open run if closed\n",
    "                mlflow.log_param(\"error_transform_test\", str(e)[:250])\n",
    "                mlflow.set_tag(\"status_transform_test\", \"failed\")\n",
    "            # dbutils.notebook.exit(f\"Test data transformation failed: {e}\") # Halt on critical error\n",
    "            raise\n",
    "    else:\n",
    "        print(\"CRITICAL: Preprocessor fitting failed or did not produce components. Cannot transform test data.\")\n",
    "\n",
    "    print(\"\\n--- Pandas Preprocessing Orchestration (Full Script) Completed ---\")\n",
    "    print(f\"Joblib processed training data should be at: {SHARED_PROCESSED_TRAIN_PATH_JOBLIB}\")\n",
    "    print(f\"Joblib processed test data should be at: {SHARED_PROCESSED_TEST_PATH_JOBLIB}\")\n",
    "    print(f\"Named Parquet processed training data should be at: {SHARED_PROCESSED_TRAIN_PARQUET_NAMED_COLS_PATH}\")\n",
    "    print(f\"Named Parquet processed test data should be at: {SHARED_PROCESSED_TEST_PARQUET_NAMED_COLS_PATH}\")\n",
    "    print(f\"Fitted preprocessor components (transformer) saved at: {DBFS_PREPROCESSOR_COMPONENTS_PATH}\")\n",
    "    print(f\"MLflow Run ID for this preprocessing: {run_id_for_preproc}\")\n",
    "    print(\"You can now proceed to the HPO and Model Training script/cells using these outputs.\")\n",
    "\n",
    "else:\n",
    "    print(\"Halting script because MLflow experiment for preprocessing could not be set.\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# <-------------------- CELL X: EXAMPLE USAGE OF SAVED PANDAS PREPROCESSOR FOR NEW DATA (INFERENCE) -------------------->\n",
    "# This is a conceptual cell showing how you would use the saved components later for prediction.\n",
    "# You would run this in a new session/notebook.\n",
    "\n",
    "# print(\"\\nCell X: Example - Using Saved Pandas Preprocessor for New Data (Inference)...\")\n",
    "\n",
    "# # --- 1. Define Configuration (should match context of saved preprocessor) ---\n",
    "# # These paths and column names should correspond to how the preprocessor was trained.\n",
    "# PREPROC_COMPONENTS_TO_LOAD_PATH = DBFS_PREPROCESSOR_COMPONENTS_PATH # From Init Cell of this script, or from MLflow run\n",
    "# # Example of loading from MLflow if you have the run_id and artifact path:\n",
    "# # PREPROC_RUN_ID_FOR_LOADING = \"your_mlflow_run_id_where_preprocessor_was_saved\"\n",
    "# # MLFLOW_ARTIFACT_DIR_FOR_LOADING = \"pandas_preprocessor_components\" # Artifact path used during logging\n",
    "# # COMPONENTS_FILENAME = \"preprocessor_components.joblib\"\n",
    "\n",
    "# NEW_RAW_DATA_UC_PATH_FOR_PREDICTION = \"/Volumes/delfos/new_inference_data/new_data_for_prediction.parquet\" # !!! REPLACE !!!\n",
    "\n",
    "# # Feature columns must match those used during fitting the preprocessor\n",
    "# CATEGORICAL_COLS_FOR_PREDICTION = CATEGORICAL_COLUMNS_RAW # From Init Cell\n",
    "# NUMERICAL_COLS_FOR_PREDICTION = NUMERICAL_COLUMNS_RAW     # From Init Cell\n",
    "\n",
    "# # --- 2. Load the Fitted Preprocessing Components ---\n",
    "# print(\"Loading fitted preprocessor components for inference...\")\n",
    "# try:\n",
    "#     # If loading from MLflow (ensure load_components_from_mlflow function is defined or use direct client):\n",
    "#     # from Cell 3 of previous response for the function load_components_from_mlflow\n",
    "#     # loaded_components_for_inference = load_components_from_mlflow(\n",
    "#     # PREPROC_RUN_ID_FOR_LOADING,\n",
    "#     # MLFLOW_ARTIFACT_DIR_FOR_LOADING,\n",
    "#     #    COMPONENTS_FILENAME\n",
    "#     # )\n",
    "#     # Or loading directly from DBFS path:\n",
    "#     loaded_components_for_inference = joblib.load(PREPROC_COMPONENTS_TO_LOAD_PATH)\n",
    "#     print(\"Preprocessor components loaded successfully for inference.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"CRITICAL ERROR: Could not load preprocessor components for inference. {e}\")\n",
    "#     # dbutils.notebook.exit(\"Failed to load preprocessor components for inference\")\n",
    "\n",
    "# if 'loaded_components_for_inference' in locals():\n",
    "#     # --- 3. Load New Raw Data for Prediction ---\n",
    "#     print(f\"Loading new raw data for prediction from {NEW_RAW_DATA_UC_PATH_FOR_PREDICTION}...\")\n",
    "#     try:\n",
    "#         new_raw_pdf_for_prediction = load_raw_data_to_pandas_from_uc_volume(NEW_RAW_DATA_UC_PATH_FOR_PREDICTION)\n",
    "#     except Exception as e:\n",
    "#         print(f\"CRITICAL ERROR: Could not load new raw data for prediction. {e}\")\n",
    "#         # dbutils.notebook.exit(\"Failed to load new raw data for prediction\")\n",
    "\n",
    "#     if 'new_raw_pdf_for_prediction' in locals():\n",
    "#         # --- 4. Apply Preprocessing to New Data ---\n",
    "#         print(\"Applying preprocessing to new data for prediction...\")\n",
    "#         try:\n",
    "#             # The transform function returns X_processed_np, y_processed_np (y will be None if no label_col)\n",
    "#             # And also the processed_pdf_named_cols\n",
    "#             X_new_processed_np, _, processed_new_pdf_named_cols = transform_pandas_preprocessor_mvp(\n",
    "#                 raw_pdf=new_raw_pdf_for_prediction,\n",
    "#                 fitted_components=loaded_components_for_inference,\n",
    "#                 label_col=None # No label column in new data for prediction typically\n",
    "#             )\n",
    "#             print(f\"Preprocessing of new data complete. Processed feature shape (NumPy): {X_new_processed_np.shape}\")\n",
    "#             print(f\"Processed new data DataFrame shape (Named Cols): {processed_new_pdf_named_cols.shape}\")\n",
    "\n",
    "#             # X_new_processed_np is now ready for your trained ML model's predict() method\n",
    "#             # For example:\n",
    "#             # final_trained_model = mlflow.sklearn.load_model(\"runs:/<your_best_model_run_id>/model\")\n",
    "#             # predictions_on_new_data = final_trained_model.predict(X_new_processed_np)\n",
    "#             # print(f\"Sample predictions on new data: {predictions_on_new_data[:5]}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"ERROR during preprocessing or prediction on new data: {e}\")\n",
    "\n",
    "# print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
