{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc50fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports (Add/Update from previous full script)\n",
    "import mlflow\n",
    "# No need for mlflow.spark here for preprocessing, but keep for HPO orchestration\n",
    "# import mlflow.sklearn # Will be used in HPO and for logging preprocessing components\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib # For saving/loading Python objects like scalers, arrays\n",
    "import time\n",
    "import shutil # For cleaning up temp directories if any\n",
    "\n",
    "from sklearn.model_selection import KFold # For K-fold target encoding if implemented manually\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce # For TargetEncoder - install if not present\n",
    "\n",
    "from pyspark.sql import SparkSession # Still need SparkSession for environment context\n",
    "\n",
    "# Suppress warnings if any from category_encoders or others for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "# Ensure spark session is available\n",
    "if 'spark' not in locals():\n",
    "    spark = SparkSession.builder.appName(\"Pandas_Preprocessing_MVP\").getOrCreate()\n",
    "\n",
    "print(\"Imports for Pandas Preprocessing successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Init Cell - Global Configurations (Review and Update)\n",
    "\n",
    "# --- MLflow Configuration ---\n",
    "EXPERIMENT_PATH = \"/Users/your_username@example.com/MVP_Regression_Pandas_Preprocessing\" # CHANGE\n",
    "\n",
    "# --- Data Paths (Unity Catalog Volumes) ---\n",
    "UC_BASE_DATA_PATH = \"/Volumes/delfos/\"\n",
    "RAW_TRAIN_DATA_PATH = f\"{UC_BASE_DATA_PATH}raw_data/train.parquet\" # Path to RAW Parquet train data\n",
    "RAW_TEST_DATA_PATH = f\"{UC_BASE_DATA_PATH}raw_data/test.parquet\"   # Path to RAW Parquet test data\n",
    "\n",
    "# --- Paths for PROCESSED Data (Output of this Pandas preprocessing script) ---\n",
    "# These will be .joblib files containing dicts of NumPy arrays {'X': ..., 'y': ...}\n",
    "# Saved to UC Volume via /dbfs/ prefix for joblib\n",
    "DBFS_PROCESSED_DATA_DIR = f\"/dbfs{UC_BASE_DATA_PATH}processed_data_pandas_mvp_v1/\" # Use /dbfs for joblib\n",
    "SHARED_PROCESSED_TRAIN_PATH = os.path.join(DBFS_PROCESSED_DATA_DIR, \"train_processed_data.joblib\")\n",
    "SHARED_PROCESSED_TEST_PATH = os.path.join(DBFS_PROCESSED_DATA_DIR, \"test_processed_data.joblib\")\n",
    "\n",
    "# --- Path for saving the FITTED PREPROCESSING COMPONENTS ---\n",
    "DBFS_PREPROCESSOR_COMPONENTS_PATH = os.path.join(DBFS_PROCESSED_DATA_DIR, \"preprocessor_components.joblib\")\n",
    "\n",
    "\n",
    "# !!! IMPORTANT: SET YOUR ACTUAL LABEL COLUMN NAME (must exist in raw data) !!!\n",
    "YOUR_LABEL_COLUMN_NAME = \"target\"\n",
    "\n",
    "# --- Define your categorical and numerical columns from the RAW data ---\n",
    "# !!! IMPORTANT: UPDATE THESE LISTS BASED ON YOUR ACTUAL RAW DATASET !!!\n",
    "CATEGORICAL_COLUMNS_RAW = [\"category_feature_1\", \"category_feature_2\"]\n",
    "NUMERICAL_COLUMNS_RAW = [\"numerical_feature_1\", \"numerical_feature_2\"]\n",
    "\n",
    "# --- MLflow Configuration for saving the preprocessing \"model\" ---\n",
    "PREPROCESSING_EXPERIMENT_PATH = EXPERIMENT_PATH # Or a dedicated one\n",
    "MLFLOW_PANDAS_PREPROCESSOR_ARTIFACT_PATH = \"pandas_preprocessor\"\n",
    "\n",
    "# --- Target Encoding Configuration (for category_encoders or manual) ---\n",
    "# For category_encoders.TargetEncoder, smoothing is a parameter.\n",
    "# If doing manual K-Fold TE, this might be relevant.\n",
    "TARGET_ENCODING_SMOOTHING = 10.0 # Default for category_encoders, can be tuned\n",
    "\n",
    "# --- Reproducibility ---\n",
    "GLOBAL_SEED = 117\n",
    "\n",
    "# --- Other Global Settings from previous Init Cell ---\n",
    "# (NUM_HPO_TRIALS, PRIMARY_METRIC, BASE_ALGORITHMS_TO_RUN, K_FOLDS_OOF, etc.\n",
    "#  are for the HPO script, not strictly needed here but good for context if this cell is shared)\n",
    "\n",
    "# Ensure output directories exist\n",
    "try:\n",
    "    os.makedirs(DBFS_PROCESSED_DATA_DIR, exist_ok=True)\n",
    "    print(f\"Checked/created processed data directory: {DBFS_PROCESSED_DATA_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create directory {DBFS_PROCESSED_DATA_DIR}. Error: {e}\")\n",
    "\n",
    "print(\"--- Pandas Preprocessing Configurations Initialized ---\")\n",
    "# ... (print other relevant configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee37931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Pandas Preprocessing Functions\n",
    "\n",
    "def load_raw_data_to_pandas(uc_volume_parquet_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads a Parquet file from a UC Volume path into a Pandas DataFrame.\"\"\"\n",
    "    # For Pandas to read directly from UC Volumes, it typically needs the /dbfs/ prefix\n",
    "    dbfs_path = uc_volume_parquet_path\n",
    "    if uc_volume_parquet_path.startswith(\"/Volumes/\"):\n",
    "        dbfs_path = f\"/dbfs{uc_volume_parquet_path}\"\n",
    "    \n",
    "    print(f\"  Loading Pandas DataFrame from: {dbfs_path}\")\n",
    "    try:\n",
    "        pdf = pd.read_parquet(dbfs_path)\n",
    "        print(f\"    Successfully loaded. Shape: {pdf.shape}\")\n",
    "        return pdf\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR loading Parquet from {dbfs_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def fit_pandas_preprocessor(train_pdf: pd.DataFrame, \n",
    "                            categorical_cols: list, \n",
    "                            numerical_cols: list, \n",
    "                            label_col: str,\n",
    "                            te_smoothing: float,\n",
    "                            global_seed: int):\n",
    "    \"\"\"\n",
    "    Fits preprocessing components (imputers, target encoders, scaler) on training data.\n",
    "    Returns processed X_train_np, y_train_np, and a dictionary of fitted components.\n",
    "    \"\"\"\n",
    "    print(\"  Fitting Pandas preprocessor...\")\n",
    "    X_train = train_pdf.drop(columns=[label_col])\n",
    "    y_train = train_pdf[label_col].astype(float)\n",
    "\n",
    "    fitted_components = {}\n",
    "\n",
    "    # 1. Impute Numerical Features (Median)\n",
    "    if numerical_cols:\n",
    "        num_imputer = SimpleImputer(strategy=\"median\")\n",
    "        X_train[numerical_cols] = num_imputer.fit_transform(X_train[numerical_cols])\n",
    "        fitted_components['numerical_imputer'] = num_imputer\n",
    "        print(f\"    Fitted Numerical Imputer for: {numerical_cols}\")\n",
    "\n",
    "    # 2. Impute Categorical Features (Most Frequent or Constant)\n",
    "    if categorical_cols:\n",
    "        # Using a constant fill value is often safer for unseen categories later\n",
    "        cat_imputer_fill_value = \"__MISSING__\"\n",
    "        # SimpleImputer for categoricals needs strategy='most_frequent' or 'constant'\n",
    "        cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=cat_imputer_fill_value)\n",
    "        X_train[categorical_cols] = cat_imputer.fit_transform(X_train[categorical_cols])\n",
    "        fitted_components['categorical_imputer'] = cat_imputer\n",
    "        fitted_components['categorical_imputer_fill_value'] = cat_imputer_fill_value # Store for transform\n",
    "        print(f\"    Fitted Categorical Imputer for: {categorical_cols} (using fill_value='{cat_imputer_fill_value}')\")\n",
    "\n",
    "    # 3. Target Encoding for Categorical Features\n",
    "    # Using category_encoders library for simplicity and robustness in MVP\n",
    "    # It handles unseen categories by default (imputes with global mean of target during fit)\n",
    "    # and training set leakage with smoothing or k-fold like behavior internally if configured.\n",
    "    if categorical_cols:\n",
    "        # Note: category_encoders.TargetEncoder expects y to be passed during fit and transform of X\n",
    "        # It also needs to know which columns are categorical.\n",
    "        # Ensure categorical columns are of 'object' or 'category' dtype for category_encoders\n",
    "        for col in categorical_cols:\n",
    "            X_train[col] = X_train[col].astype('category')\n",
    "\n",
    "        target_encoder = ce.TargetEncoder(cols=categorical_cols, smoothing=te_smoothing, handle_unknown='value', handle_missing='value')\n",
    "        # handle_unknown='value' uses the overall mean of y_train for new categories\n",
    "        # handle_missing='value' uses the overall mean of y_train for NaNs seen during transform\n",
    "        \n",
    "        X_train[categorical_cols] = target_encoder.fit_transform(X_train[categorical_cols], y_train)\n",
    "        fitted_components['target_encoder'] = target_encoder\n",
    "        print(f\"    Fitted TargetEncoder for: {categorical_cols} with smoothing={te_smoothing}\")\n",
    "        # The transformed columns are now numeric.\n",
    "\n",
    "    # 4. Scale All Features (Numerical + Target Encoded Categoricals)\n",
    "    # All columns in X_train (that are features) should now be numeric.\n",
    "    feature_columns_for_scaling = numerical_cols + categorical_cols # Cat cols are now numerically encoded\n",
    "    \n",
    "    if feature_columns_for_scaling: # Check if there are any features to scale\n",
    "        scaler = StandardScaler()\n",
    "        # Ensure all columns for scaling are indeed numeric and do not contain NaNs from TE if 'value' wasn't perfect\n",
    "        # TargetEncoder with handle_missing='value' and handle_unknown='value' should prevent NaNs if y_train has no NaNs.\n",
    "        # If any NaNs persist (e.g., if a numerical column was all NaNs and imputer failed, or TE issue), scaler will fail.\n",
    "        # A check or a final SimpleImputer(strategy='median') pass on feature_columns_for_scaling could be added.\n",
    "        \n",
    "        # Make sure all columns are float before scaling\n",
    "        for col in feature_columns_for_scaling:\n",
    "            X_train[col] = X_train[col].astype(float)\n",
    "            # Final check for NaNs after TE before scaling, impute if necessary\n",
    "            if X_train[col].isnull().any():\n",
    "                print(f\"    Warning: NaNs detected in column {col} before scaling. Applying median imputation.\")\n",
    "                median_val = X_train[col].median()\n",
    "                X_train[col] = X_train[col].fillna(median_val)\n",
    "                # Store this median for transform stage if not already handled by a formal imputer for these generated features\n",
    "                if 'post_te_imputer_medians' not in fitted_components:\n",
    "                    fitted_components['post_te_imputer_medians'] = {}\n",
    "                fitted_components['post_te_imputer_medians'][col] = median_val\n",
    "\n",
    "\n",
    "        X_train_scaled_np = scaler.fit_transform(X_train[feature_columns_for_scaling])\n",
    "        fitted_components['scaler'] = scaler\n",
    "        fitted_components['feature_columns_for_scaling'] = feature_columns_for_scaling # Store order\n",
    "        print(f\"    Fitted StandardScaler for features: {feature_columns_for_scaling}\")\n",
    "    else: # No features to scale (e.g. only one categorical feature became one TE feature)\n",
    "        print(\"    No features identified for scaling. X_train might be empty or only have non-scalable features.\")\n",
    "        # Handle case where X_train might become just an empty dataframe or a single column that was a label.\n",
    "        # This logic assumes X_train[feature_columns_for_scaling] results in a valid array for scikit-learn models.\n",
    "        # If feature_columns_for_scaling is empty, X_train_scaled_np would be problematic.\n",
    "        if not feature_columns_for_scaling and not X_train.empty:\n",
    "             X_train_scaled_np = X_train.values # Or handle appropriately\n",
    "        elif X_train.empty:\n",
    "             X_train_scaled_np = np.array([]) # Or handle as error\n",
    "        else: # Should not happen if there were features\n",
    "             X_train_scaled_np = X_train[feature_columns_for_scaling].values\n",
    "\n",
    "\n",
    "    y_train_np = y_train.values\n",
    "    print(\"  Pandas preprocessor fitting complete.\")\n",
    "    return X_train_scaled_np, y_train_np, fitted_components\n",
    "\n",
    "\n",
    "def transform_pandas_preprocessor(raw_pdf: pd.DataFrame, \n",
    "                                  fitted_components: dict,\n",
    "                                  categorical_cols: list, \n",
    "                                  numerical_cols: list, \n",
    "                                  label_col: str = None, # Label col might not be in test data for prediction\n",
    "                                  is_train_data=False): # Flag to indicate if we are transforming training data (for y)\n",
    "    \"\"\"\n",
    "    Applies fitted preprocessing components to new data.\n",
    "    \"\"\"\n",
    "    print(\"  Transforming data with Pandas preprocessor...\")\n",
    "    \n",
    "    # Prepare X and y (if label_col is present)\n",
    "    if label_col and label_col in raw_pdf.columns:\n",
    "        X_data = raw_pdf.drop(columns=[label_col]).copy() # Make a copy to avoid SettingWithCopyWarning\n",
    "        y_data_np = raw_pdf[label_col].astype(float).values\n",
    "    else:\n",
    "        X_data = raw_pdf.copy()\n",
    "        y_data_np = None\n",
    "\n",
    "    # 1. Impute Numerical\n",
    "    if numerical_cols and 'numerical_imputer' in fitted_components:\n",
    "        X_data.loc[:, numerical_cols] = fitted_components['numerical_imputer'].transform(X_data[numerical_cols])\n",
    "        print(f\"    Applied Numerical Imputer for: {numerical_cols}\")\n",
    "\n",
    "    # 2. Impute Categorical\n",
    "    if categorical_cols and 'categorical_imputer' in fitted_components:\n",
    "        X_data.loc[:, categorical_cols] = fitted_components['categorical_imputer'].transform(X_data[categorical_cols])\n",
    "        print(f\"    Applied Categorical Imputer for: {categorical_cols}\")\n",
    "    \n",
    "    # 3. Target Encode Categorical\n",
    "    if categorical_cols and 'target_encoder' in fitted_components:\n",
    "        # Ensure categorical columns are of 'category' or 'object' dtype for category_encoders\n",
    "        for col in categorical_cols:\n",
    "            X_data[col] = X_data[col].astype('category')\n",
    "        \n",
    "        # For TargetEncoder's transform, y is not strictly needed if it learned from y during fit,\n",
    "        # but some versions/setups might expect it. If y_data_np is None (e.g. for true new data),\n",
    "        # category_encoders TE should use the global mean learned during fit for unknowns/missings.\n",
    "        # Pass y=None if it's not available (e.g. scoring new data).\n",
    "        # If transforming training data (is_train_data=True), y_data_np should be available.\n",
    "        # However, for consistency, TE transform should only use X.\n",
    "        X_data.loc[:, categorical_cols] = fitted_components['target_encoder'].transform(X_data[categorical_cols])\n",
    "        print(f\"    Applied TargetEncoder for: {categorical_cols}\")\n",
    "\n",
    "    # 4. Scale All Features\n",
    "    feature_columns_for_scaling = fitted_components.get('feature_columns_for_scaling', [])\n",
    "    if feature_columns_for_scaling and 'scaler' in fitted_components:\n",
    "        # Make sure all columns are float before scaling and impute any post-TE NaNs\n",
    "        for col in feature_columns_for_scaling:\n",
    "            X_data[col] = X_data[col].astype(float)\n",
    "            if col in fitted_components.get('post_te_imputer_medians', {}):\n",
    "                 if X_data[col].isnull().any():\n",
    "                    print(f\"    Post-TE Imputing NaNs in {col} with stored median before scaling.\")\n",
    "                    X_data[col] = X_data[col].fillna(fitted_components['post_te_imputer_medians'][col])\n",
    "            # If there are still NaNs and no stored median, this might indicate an issue or need for a default fill\n",
    "            if X_data[col].isnull().any():\n",
    "                print(f\"    Warning: NaNs still present in {col} before scaling and no stored median. Filling with 0.\")\n",
    "                X_data[col] = X_data[col].fillna(0)\n",
    "\n",
    "\n",
    "        X_data_scaled_np = fitted_components['scaler'].transform(X_data[feature_columns_for_scaling])\n",
    "        print(f\"    Applied StandardScaler for features: {feature_columns_for_scaling}\")\n",
    "    elif not feature_columns_for_scaling and not X_data.empty: # No scaling was fitted\n",
    "        X_data_scaled_np = X_data.values\n",
    "    elif X_data.empty :\n",
    "        X_data_scaled_np = np.array([])\n",
    "    else: # Should not happen if feature_columns_for_scaling was defined during fit\n",
    "        X_data_scaled_np = X_data[feature_columns_for_scaling].values\n",
    "\n",
    "\n",
    "    print(\"  Pandas data transformation complete.\")\n",
    "    if y_data_np is not None:\n",
    "        return X_data_scaled_np, y_data_np\n",
    "    else:\n",
    "        return X_data_scaled_np, None\n",
    "\n",
    "\n",
    "def save_processed_data_and_components(X_np, y_np, file_path, components_dict, components_path):\n",
    "    \"\"\"Saves processed NumPy arrays and fitted components using joblib.\"\"\"\n",
    "    print(f\"  Saving processed data to: {file_path}\")\n",
    "    try:\n",
    "        payload = {'X': X_np, 'y': y_np}\n",
    "        joblib.dump(payload, file_path)\n",
    "        print(f\"    Data saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR saving data to {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "    if components_dict and components_path:\n",
    "        print(f\"  Saving preprocessor components to: {components_path}\")\n",
    "        try:\n",
    "            joblib.dump(components_dict, components_path)\n",
    "            print(f\"    Preprocessor components saved successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR saving components to {components_path}: {e}\")\n",
    "            # Don't raise if components fail but data saved, or handle as critical\n",
    "            \n",
    "print(\"--- Pandas Preprocessing Utility Functions Defined ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e92ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main Preprocessing Orchestration (Pandas)\n",
    "\n",
    "print(\"--- Starting Pandas Preprocessing Orchestration ---\")\n",
    "\n",
    "# --- 0. Set MLflow Experiment for Preprocessing ---\n",
    "try:\n",
    "    # Make sure spark session from Init cell is used if get_or_create_experiment needs it\n",
    "    preprocessing_mlflow_experiment_id = get_or_create_experiment(PREPROCESSING_EXPERIMENT_PATH, spark)\n",
    "    if preprocessing_mlflow_experiment_id:\n",
    "        # Setting experiment for the whole notebook session for this stage\n",
    "        mlflow.set_experiment(experiment_id=preprocessing_mlflow_experiment_id)\n",
    "        print(f\"MLflow experiment '{PREPROCESSING_EXPERIMENT_PATH}' for preprocessing is set with ID: {preprocessing_mlflow_experiment_id}\")\n",
    "    else:\n",
    "        raise Exception(\"Preprocessing MLflow experiment could not be set. Halting.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL: Could not initialize MLflow experiment for preprocessing. Error: {e}\")\n",
    "    # Consider dbutils.notebook.exit(\"MLflow experiment setup failed\")\n",
    "\n",
    "# --- 1. Load Raw Data into Pandas DataFrames ---\n",
    "# Paths are from Init Cell. Ensure these Parquet files exist in UC Volumes.\n",
    "print(f\"\\nLoading RAW training data for Pandas preprocessing...\")\n",
    "raw_train_pdf = load_raw_data_to_pandas(RAW_TRAIN_DATA_PATH)\n",
    "print(f\"RAW training data features: {raw_train_pdf.columns.tolist()}\")\n",
    "raw_train_pdf.info()\n",
    "\n",
    "print(f\"\\nLoading RAW test data for Pandas preprocessing...\")\n",
    "raw_test_pdf = load_raw_data_to_pandas(RAW_TEST_DATA_PATH)\n",
    "\n",
    "# --- 2. Fit Preprocessor on Training Data ---\n",
    "# This MLflow run will log the parameters and the fitted components artifact\n",
    "fitted_components_dict = None\n",
    "with mlflow.start_run(run_name=\"Pandas_Preprocessor_Fit\") as preproc_run:\n",
    "    print(f\"\\nFitting preprocessor. MLflow Run ID: {preproc_run.info.run_id}\")\n",
    "    mlflow.log_param(\"label_column\", YOUR_LABEL_COLUMN_NAME)\n",
    "    mlflow.log_param(\"categorical_features_raw\", \", \".join(CATEGORICAL_COLUMNS_RAW))\n",
    "    mlflow.log_param(\"numerical_features_raw\", \", \".join(NUMERICAL_COLUMNS_RAW))\n",
    "    mlflow.log_param(\"target_encoding_smoothing\", TARGET_ENCODING_SMOOTHING)\n",
    "    mlflow.log_param(\"global_seed\", GLOBAL_SEED)\n",
    "    mlflow.set_tag(\"preprocessing_type\", \"pandas_mvp\")\n",
    "\n",
    "    try:\n",
    "        X_train_processed_np, y_train_processed_np, fitted_components_dict = fit_pandas_preprocessor(\n",
    "            raw_train_pdf,\n",
    "            CATEGORICAL_COLUMNS_RAW,\n",
    "            NUMERICAL_COLUMNS_RAW,\n",
    "            YOUR_LABEL_COLUMN_NAME,\n",
    "            TARGET_ENCODING_SMOOTHING,\n",
    "            GLOBAL_SEED\n",
    "        )\n",
    "        print(f\"  Processed X_train_np shape: {X_train_processed_np.shape}, y_train_np shape: {y_train_processed_np.shape}\")\n",
    "\n",
    "        # Save processed training data and the fitted components\n",
    "        save_processed_data_and_components(\n",
    "            X_train_processed_np, y_train_processed_np, \n",
    "            SHARED_PROCESSED_TRAIN_PATH, # Path from Init Cell (DBFS path for joblib)\n",
    "            fitted_components_dict, DBFS_PREPROCESSOR_COMPONENTS_PATH # Path from Init Cell\n",
    "        )\n",
    "        \n",
    "        # Log components as artifact in MLflow\n",
    "        # mlflow.log_dict(fitted_components_dict, \"fitted_preprocessing_components.json\") # This won't work for sklearn objects\n",
    "        mlflow.log_artifact(DBFS_PREPROCESSOR_COMPONENTS_PATH, artifact_path=MLFLOW_PANDAS_PREPROCESSOR_ARTIFACT_PATH)\n",
    "        print(f\"  Logged fitted preprocessor components to MLflow artifact path: {MLFLOW_PANDAS_PREPROCESSOR_ARTIFACT_PATH}\")\n",
    "        mlflow.set_tag(\"status_fit\", \"success\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR during preprocessor fitting: {e}\")\n",
    "        mlflow.log_param(\"error_fit\", str(e)[:250])\n",
    "        mlflow.set_tag(\"status_fit\", \"failed\")\n",
    "        raise # Re-raise to stop execution if fitting fails\n",
    "\n",
    "# --- 3. Transform Test Data using Fitted Preprocessor ---\n",
    "if fitted_components_dict: # Proceed only if fitting was successful\n",
    "    print(\"\\nTransforming TEST data using fitted preprocessor...\")\n",
    "    try:\n",
    "        X_test_processed_np, y_test_processed_np = transform_pandas_preprocessor(\n",
    "            raw_test_pdf,\n",
    "            fitted_components_dict,\n",
    "            CATEGORICAL_COLUMNS_RAW,\n",
    "            NUMERICAL_COLUMNS_RAW,\n",
    "            YOUR_LABEL_COLUMN_NAME, # Pass label col to get y_test_processed_np\n",
    "            is_train_data=False\n",
    "        )\n",
    "        print(f\"  Processed X_test_np shape: {X_test_processed_np.shape}, y_test_np shape: {y_test_processed_np.shape if y_test_processed_np is not None else 'N/A'}\")\n",
    "\n",
    "        # Save processed test data\n",
    "        # The y_test_processed_np is needed by HPO objective function for evaluation\n",
    "        save_processed_data_and_components(\n",
    "            X_test_processed_np, y_test_processed_np,\n",
    "            SHARED_PROCESSED_TEST_PATH, # Path from Init Cell (DBFS path for joblib)\n",
    "            None, None # Don't re-save components here\n",
    "        )\n",
    "        print(\"  Test data transformed and saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR during test data transformation: {e}\")\n",
    "        # Decide if this is critical. For now, we'll let it error out.\n",
    "        raise\n",
    "else:\n",
    "    print(\"CRITICAL: Preprocessor fitting failed or did not produce components. Cannot transform test data.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Pandas Preprocessing Orchestration Completed ---\")\n",
    "print(f\"Processed training data should be at: {SHARED_PROCESSED_TRAIN_PATH}\")\n",
    "print(f\"Processed test data should be at: {SHARED_PROCESSED_TEST_PATH}\")\n",
    "print(f\"Fitted preprocessor components should be at: {DBFS_PREPROCESSOR_COMPONENTS_PATH}\")\n",
    "print(\"You can now proceed to the HPO and Model Training script/cells.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
