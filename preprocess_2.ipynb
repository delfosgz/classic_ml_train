{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b875db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-------------------- CELL 1: IMPORTS -------------------->\n",
    "print(\"Cell 1: Imports - Executing...\")\n",
    "import mlflow\n",
    "import mlflow.pyfunc # For saving custom Python models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib # For saving the pyfunc model's components IF NOT relying solely on mlflow pickling the instance\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split # Used for month-wise splits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce # For TargetEncoder\n",
    "\n",
    "from pyspark.sql import SparkSession # Still useful for environment context\n",
    "\n",
    "# Suppress common warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', message=\"Previously subsetted data...\") # From category_encoders\n",
    "\n",
    "# Ensure spark session is available\n",
    "if 'spark' not in locals():\n",
    "    spark = SparkSession.builder.appName(\"Pandas_FE_Preprocessing_Full_MVP\").getOrCreate()\n",
    "    print(\"SparkSession created.\")\n",
    "else:\n",
    "    print(\"SparkSession already exists.\")\n",
    "\n",
    "print(\"Imports successful for Pandas Preprocessing Pipeline with Feature Engineering.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# <-------------------- CELL 2: INIT CELL - GLOBAL CONFIGURATIONS FOR PREPROCESSING -------------------->\n",
    "print(\"\\nCell 2: Global Configurations for Preprocessing - Executing...\")\n",
    "\n",
    "# --- MLflow Configuration ---\n",
    "# !!! IMPORTANT: SET YOUR MLFLOW EXPERIMENT PATH !!!\n",
    "PREPROCESSING_EXPERIMENT_PATH = \"/Users/your_username@example.com/MVP_Classification_FE_Preprocessing_Full\" # CHANGE THIS\n",
    "\n",
    "# --- Data Paths (Unity Catalog Volumes) ---\n",
    "# !!! IMPORTANT: SET YOUR UNITY CATALOG VOLUME BASE PATH !!!\n",
    "UC_BASE_DATA_PATH = \"/Volumes/delfos/\" # As per your input\n",
    "\n",
    "# --- Path to the FULL RAW input dataset ---\n",
    "# !!! IMPORTANT: UPDATE THIS TO YOUR ACTUAL FULL RAW DATASET PATH IN UC VOLUMES !!!\n",
    "FULL_RAW_DATA_PARQUET_PATH = f\"{UC_BASE_DATA_PATH}raw_data_full/full_dataset_generic.parquet\" # Example\n",
    "\n",
    "# --- Date Column for Stratified Splitting ---\n",
    "# !!! IMPORTANT: SET THE NAME OF YOUR DATE/TIMESTAMP COLUMN IN THE RAW DATA !!!\n",
    "DATE_COLUMN_FOR_SPLIT = \"date_col\" # Example: 'order_date', 'policy_start_date'\n",
    "\n",
    "# --- Output Paths for INTERMEDIATE RAW SPLIT Data (Optional, but good for traceability) ---\n",
    "# Using /dbfs/ prefix for direct Pandas/os operations on UC Volumes\n",
    "DBFS_RAW_SPLITS_DIR = f\"/dbfs{UC_BASE_DATA_PATH}raw_splits_v1_fe/\" # Unique name\n",
    "RAW_TRAIN_SPLIT_PATH = os.path.join(DBFS_RAW_SPLITS_DIR, \"raw_train_split.parquet\")\n",
    "RAW_TEST_SPLIT_PATH = os.path.join(DBFS_RAW_SPLITS_DIR, \"raw_test_split.parquet\")\n",
    "\n",
    "# --- Output Paths for FINAL PROCESSED Data (Parquet with Named Columns) ---\n",
    "PROCESSED_DATA_VERSION_FE = \"v1_pandas_fe_final\" # Versioning for processed data\n",
    "DBFS_PROCESSED_DATA_DIR_BASE_FE = f\"/dbfs{UC_BASE_DATA_PATH}processed_data/\"\n",
    "PROCESSED_DATA_DIR_VERSIONED_FE = os.path.join(DBFS_PROCESSED_DATA_DIR_BASE_FE, PROCESSED_DATA_VERSION_FE)\n",
    "\n",
    "# These paths will point to the Parquet files with named columns\n",
    "SHARED_PROCESSED_TRAIN_PATH = os.path.join(PROCESSED_DATA_DIR_VERSIONED_FE, \"train_processed_named_cols.parquet\")\n",
    "SHARED_PROCESSED_TEST_PATH = os.path.join(PROCESSED_DATA_DIR_VERSIONED_FE, \"test_processed_named_cols.parquet\")\n",
    "\n",
    "# --- MLflow artifact path for the saved pyfunc preprocessor model ---\n",
    "MLFLOW_PYFUNC_PREPROCESSOR_ARTIFACT_PATH = \"pandas_full_preprocessor\"\n",
    "\n",
    "# !!! IMPORTANT: SET YOUR ACTUAL BINARY TARGET LABEL COLUMN NAME (must exist in raw data) !!!\n",
    "YOUR_TARGET_COLUMN_NAME = \"target_binary\" # Example: 0 or 1\n",
    "\n",
    "# --- Define your categorical and numerical columns from the RAW data ---\n",
    "# These are columns read from FULL_RAW_DATA_PARQUET_PATH *before* any FE in the PyFunc model.\n",
    "# The PyFunc model will then create new features based on these (e.g. from date_col, age_col, premium_col).\n",
    "# !!! IMPORTANT: UPDATE THESE LISTS BASED ON YOUR ACTUAL RAW DATASET !!!\n",
    "CATEGORICAL_COLUMNS_RAW = [\"cat_feat_1\", \"cat_feat_2\", \"region_col\"] # Example\n",
    "NUMERICAL_COLUMNS_RAW = [\"num_feat_1\", \"age_col\", \"premium_col\", \"interactions_col\"] # Example\n",
    "\n",
    "# --- Preprocessing Configuration ---\n",
    "TEST_SET_SPLIT_RATIO = 0.20\n",
    "TARGET_ENCODING_SMOOTHING = 20.0 # For category_encoders.TargetEncoder\n",
    "CATEGORICAL_IMPUTE_CONSTANT = \"__MISSING_CAT__\" # For SimpleImputer\n",
    "NUMERICAL_IMPUTE_STRATEGY = \"median\" # For SimpleImputer\n",
    "\n",
    "# --- Reproducibility ---\n",
    "GLOBAL_SEED = 117\n",
    "\n",
    "# --- Feature Engineering Configuration (Example) ---\n",
    "# These original column names are used as basis for FE. They must be in NUMERICAL_COLUMNS_RAW or DATE_COLUMN_FOR_SPLIT.\n",
    "AGE_COL_FOR_FE = \"age_col\" # Must be in NUMERICAL_COLUMNS_RAW\n",
    "PREMIUM_COL_FOR_FE = \"premium_col\" # Must be in NUMERICAL_COLUMNS_RAW\n",
    "INTERACTIONS_COL_FOR_FE = \"interactions_col\" # Example, must be in NUMERICAL_COLUMNS_RAW\n",
    "\n",
    "# For binning age (example)\n",
    "AGE_BINS = [0, 18, 30, 45, 60, 120]\n",
    "AGE_BIN_LABELS = ['0-18', '19-30', '31-45', '46-60', '60+']\n",
    "\n",
    "# Ensure output directories exist (using /dbfs/ prefix for os.makedirs)\n",
    "try:\n",
    "    os.makedirs(DBFS_RAW_SPLITS_DIR, exist_ok=True)\n",
    "    os.makedirs(PROCESSED_DATA_DIR_VERSIONED_FE, exist_ok=True)\n",
    "    print(f\"Checked/created raw splits directory: {DBFS_RAW_SPLITS_DIR}\")\n",
    "    print(f\"Checked/created processed data directory: {PROCESSED_DATA_DIR_VERSIONED_FE}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create directory. Ensure UC Volume '{UC_BASE_DATA_PATH}' exists and you have write permissions. Error: {e}\")\n",
    "\n",
    "print(f\"--- Preprocessing Global Configurations (with FE) Initialized ---\")\n",
    "print(f\"MLflow Experiment Path for Preprocessing: {PREPROCESSING_EXPERIMENT_PATH}\")\n",
    "print(f\"Full Raw Data Input Path: {FULL_RAW_DATA_PARQUET_PATH}\")\n",
    "print(f\"Date Column for Split: {DATE_COLUMN_FOR_SPLIT}\")\n",
    "print(f\"Target Column: {YOUR_TARGET_COLUMN_NAME}\")\n",
    "print(f\"  Output Processed Train Data Path (Parquet Named Cols): {SHARED_PROCESSED_TRAIN_PATH}\")\n",
    "print(f\"  Output Processed Test Data Path (Parquet Named Cols): {SHARED_PROCESSED_TEST_PATH}\")\n",
    "print(f\"  MLflow Pyfunc Preprocessor Artifact Path: {MLFLOW_PYFUNC_PREPROCESSOR_ARTIFACT_PATH}\")\n",
    "print(f\"Categorical Columns (Raw): {CATEGORICAL_COLUMNS_RAW}\")\n",
    "print(f\"Numerical Columns (Raw): {NUMERICAL_COLUMNS_RAW}\")\n",
    "print(f\"Global Seed: {GLOBAL_SEED}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# <-------------------- CELL 3: PREPROCESSING LOGIC & MLFLOW PYFUNC MODEL CLASS -------------------->\n",
    "print(\"\\nCell 3: Preprocessing Logic & Pyfunc Model Class - Defining (with Custom Initial Formatting & Feature Engineering)...\")\n",
    "\n",
    "# --- MLflow Utility ---\n",
    "def get_or_create_experiment(experiment_name_param, spark_session_param=None): # spark_session_param optional\n",
    "    try:\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name_param)\n",
    "        if experiment:\n",
    "            print(f\"MLflow experiment '{experiment_name_param}' found with ID: {experiment.experiment_id}\")\n",
    "            return experiment.experiment_id\n",
    "        else:\n",
    "            print(f\"MLflow experiment '{experiment_name_param}' not found. Attempting to create.\")\n",
    "            experiment_id = mlflow.create_experiment(name=experiment_name_param)\n",
    "            print(f\"MLflow experiment '{experiment_name_param}' created with ID: {experiment_id}\")\n",
    "            return experiment_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_or_create_experiment for '{experiment_name_param}'. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Data Splitting Function ---\n",
    "def split_by_month_and_stratify(full_pdf: pd.DataFrame, \n",
    "                                date_col: str, \n",
    "                                target_col: str, \n",
    "                                test_size: float, \n",
    "                                random_state: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    print(f\"  Starting stratified split by month from column '{date_col}' and target '{target_col}'...\")\n",
    "    if date_col not in full_pdf.columns:\n",
    "        raise ValueError(f\"Date column '{date_col}' not found in DataFrame.\")\n",
    "    if target_col not in full_pdf.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in DataFrame.\")\n",
    "\n",
    "    try:\n",
    "        temp_df = full_pdf.copy()\n",
    "        temp_df[date_col] = pd.to_datetime(temp_df[date_col])\n",
    "        temp_df['year_month_group'] = temp_df[date_col].dt.to_period('M')\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not convert column '{date_col}' to datetime or extract year_month. Error: {e}\")\n",
    "\n",
    "    train_dfs_list = []\n",
    "    test_dfs_list = []\n",
    "    \n",
    "    for month_period, group_data in temp_df.groupby('year_month_group'):\n",
    "        print(f\"    Splitting for month-year: {month_period}, group size: {len(group_data)}\")\n",
    "        if len(group_data) < 2:\n",
    "             print(f\"      Group for {month_period} is too small ({len(group_data)}). Assigning based on ratio if possible, else to train.\")\n",
    "             if len(group_data) == 1: # Single sample goes to train\n",
    "                train_dfs_list.append(group_data)\n",
    "             elif np.random.RandomState(random_state).rand() > test_size : # Use seeded random for consistency\n",
    "                 train_dfs_list.append(group_data)\n",
    "             else:\n",
    "                 test_dfs_list.append(group_data)\n",
    "             continue\n",
    "        \n",
    "        target_counts = group_data[target_col].value_counts()\n",
    "        # Check if any class has fewer samples than required for a split (typically 2 for train_test_split)\n",
    "        # Or if only one class is present in the group\n",
    "        min_samples_per_class_needed = 2 # For train_test_split to be able to make a split for each class\n",
    "        \n",
    "        if len(target_counts) < 2 or target_counts.min() < min_samples_per_class_needed:\n",
    "            print(f\"      Not enough class diversity or samples in {month_period} for stratification (counts: {target_counts.to_dict()}). Performing random split.\")\n",
    "            month_train_df, month_test_df = train_test_split(\n",
    "                group_data, test_size=test_size, random_state=random_state, shuffle=True\n",
    "            )\n",
    "        else:\n",
    "            try:\n",
    "                month_train_df, month_test_df = train_test_split(\n",
    "                    group_data, test_size=test_size, random_state=random_state, \n",
    "                    stratify=group_data[target_col], shuffle=True\n",
    "                )\n",
    "            except ValueError as ve: # Handles \"The least populated class in y has only X members\"\n",
    "                print(f\"      Stratification failed for {month_period} (Error: {ve}). Performing random split.\")\n",
    "                month_train_df, month_test_df = train_test_split(\n",
    "                    group_data, test_size=test_size, random_state=random_state, shuffle=True\n",
    "                )\n",
    "        train_dfs_list.append(month_train_df)\n",
    "        if not month_test_df.empty: # Only append if test_df is not empty\n",
    "             test_dfs_list.append(month_test_df)\n",
    "\n",
    "    final_train_df = pd.concat(train_dfs_list).drop(columns=['year_month_group']) if train_dfs_list else pd.DataFrame(columns=full_pdf.columns)\n",
    "    final_test_df = pd.concat(test_dfs_list).drop(columns=['year_month_group']) if test_dfs_list else pd.DataFrame(columns=full_pdf.columns)\n",
    "\n",
    "\n",
    "    print(f\"  Splitting complete. Train shape: {final_train_df.shape}, Test shape: {final_test_df.shape}\")\n",
    "    if not final_train_df.empty: print(f\"  Train target distribution:\\n{final_train_df[target_col].value_counts(normalize=True, dropna=False)}\")\n",
    "    if not final_test_df.empty: print(f\"  Test target distribution:\\n{final_test_df[target_col].value_counts(normalize=True, dropna=False)}\")\n",
    "    return final_train_df, final_test_df\n",
    "\n",
    "\n",
    "# --- Pandas Preprocessor as an mlflow.pyfunc.PythonModel (with Feature Engineering) ---\n",
    "class PandasFeatureEngineeringPreprocessor(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 raw_categorical_cols, raw_numerical_cols, \n",
    "                 date_col_for_fe, age_col_for_fe, premium_col_for_fe, interactions_col_for_fe,\n",
    "                 age_bins_for_fe, age_bin_labels_for_fe,\n",
    "                 label_col_for_te_fitting, \n",
    "                 te_smoothing_factor, \n",
    "                 cat_impute_constant, num_impute_strategy,\n",
    "                 global_seed):\n",
    "        \n",
    "        self.raw_categorical_cols_config = list(raw_categorical_cols)\n",
    "        self.raw_numerical_cols_config = list(raw_numerical_cols)\n",
    "        self.date_col_for_fe = date_col_for_fe\n",
    "        self.age_col_for_fe = age_col_for_fe\n",
    "        self.premium_col_for_fe = premium_col_for_fe\n",
    "        self.interactions_col_for_fe = interactions_col_for_fe\n",
    "        self.age_bins_for_fe = age_bins_for_fe\n",
    "        self.age_bin_labels_for_fe = age_bin_labels_for_fe\n",
    "        self.label_col_for_te_fitting = label_col_for_te_fitting # Only used for fitting TE\n",
    "        self.te_smoothing_factor = te_smoothing_factor\n",
    "        self.cat_impute_constant = cat_impute_constant\n",
    "        self.num_impute_strategy = num_impute_strategy\n",
    "        self.global_seed = global_seed\n",
    "\n",
    "        # Fitted components and dynamic column lists will be stored here after `fit`\n",
    "        self.fitted_components = {} # Store all imputers, encoders, scalers\n",
    "        self.feature_engineering_details = {} # Store names of engineered features\n",
    "        self.final_feature_columns_in_order = [] # Defines the output columns and their order\n",
    "\n",
    "    def _get_valid_cols(self, df, col_list):\n",
    "        return [col for col in col_list if col in df.columns and col != self.label_col_for_te_fitting]\n",
    "\n",
    "    def _apply_initial_custom_formatting(self, df_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df_input.copy()\n",
    "        print(\"    Custom Preprocessing Step 1: Applying initial custom data formatting...\")\n",
    "        # !!! REPLACE THE CONTENT BELOW WITH YOUR ACTUAL CUSTOM FORMATTING LOGIC !!!\n",
    "        # This function should take the raw DataFrame (without the label if separated early)\n",
    "        # and return the formatted DataFrame.\n",
    "        # The columns listed in raw_categorical_cols_config and raw_numerical_cols_config\n",
    "        # should exist AFTER this step, or this step should create them.\n",
    "        \n",
    "        # Example (ensure this doesn't conflict with your actual column names):\n",
    "        # if 'some_text_column' in df.columns:\n",
    "        #     df['some_text_column'] = df['some_text_column'].astype(str).str.lower().str.strip()\n",
    "        #     print(\"      Example custom formatting: 'some_text_column' processed.\")\n",
    "        # else:\n",
    "        #     print(\"      Example custom formatting: 'some_text_column' not found.\")\n",
    "        print(\"    Custom Preprocessing Step 1: Initial custom data formatting complete.\")\n",
    "        return df\n",
    "\n",
    "    def _engineer_features(self, df_input: pd.DataFrame, is_fitting_phase: bool) -> pd.DataFrame:\n",
    "        df = df_input.copy()\n",
    "        print(\"    Feature Engineering Step 2: Creating new features...\")\n",
    "        \n",
    "        engineered_categoricals_temp = []\n",
    "        engineered_numericals_temp = []\n",
    "\n",
    "        # Date/Time Features\n",
    "        if self.date_col_for_fe and self.date_col_for_fe in df.columns:\n",
    "            try:\n",
    "                s_date = pd.to_datetime(df[self.date_col_for_fe], errors='coerce')\n",
    "                if not s_date.isnull().all(): # Proceed if some dates are valid\n",
    "                    df['fe_month'] = s_date.dt.month.fillna(-1).astype(int).astype(str) # Impute NaT with -1 then to str\n",
    "                    df['fe_day_of_week'] = s_date.dt.dayofweek.fillna(-1).astype(int).astype(str)\n",
    "                    df['fe_is_weekend'] = s_date.dt.dayofweek.isin([5,6]).astype(int).astype(str)\n",
    "                    engineered_categoricals_temp.extend(['fe_month', 'fe_day_of_week', 'fe_is_weekend'])\n",
    "                    print(f\"      FE: Created date features: month, day_of_week, is_weekend from {self.date_col_for_fe}\")\n",
    "            except Exception as e_date: print(f\"      Warning: Could not create date features from {self.date_col_for_fe}. Error: {e_date}\")\n",
    "        \n",
    "        # Numerical Transformations\n",
    "        for col_name, new_col_prefix in [\n",
    "            (self.premium_col_for_fe, \"fe_premium\"), \n",
    "            (self.age_col_for_fe, \"fe_age\"), \n",
    "            (self.interactions_col_for_fe, \"fe_interactions\") # Example\n",
    "        ]:\n",
    "            if col_name and col_name in df.columns:\n",
    "                df[col_name] = pd.to_numeric(df[col_name], errors='coerce') # Ensure numeric\n",
    "                df[f'{new_col_prefix}_log1p'] = np.log1p(df[col_name].fillna(0).clip(lower=0))\n",
    "                df[f'{new_col_prefix}_sq'] = df[col_name].fillna(0)**2\n",
    "                engineered_numericals_temp.extend([f'{new_col_prefix}_log1p', f'{new_col_prefix}_sq'])\n",
    "                print(f\"      FE: Created log1p and squared features for {col_name}\")\n",
    "\n",
    "        # Binning Age\n",
    "        if self.age_col_for_fe and self.age_col_for_fe in df.columns and self.age_bins_for_fe and self.age_bin_labels_for_fe:\n",
    "            age_col_binned_name = f'fe_{self.age_col_for_fe}_binned'\n",
    "            # Ensure age column is numeric before binning\n",
    "            df[self.age_col_for_fe] = pd.to_numeric(df[self.age_col_for_fe], errors='coerce')\n",
    "            df[age_col_binned_name] = pd.cut(df[self.age_col_for_fe], \n",
    "                                             bins=self.age_bins_for_fe, \n",
    "                                             labels=self.age_bin_labels_for_fe, \n",
    "                                             right=False, include_lowest=True)\n",
    "            df[age_col_binned_name] = df[age_col_binned_name].astype(str).fillna(self.cat_impute_constant) # Handle NaNs from binning then to string\n",
    "            engineered_categoricals_temp.append(age_col_binned_name)\n",
    "            print(f\"      FE: Created binned age feature: {age_col_binned_name}\")\n",
    "\n",
    "        if is_fitting_phase:\n",
    "            self.feature_engineering_details['engineered_categorical_cols'] = list(set(engineered_categoricals_temp))\n",
    "            self.feature_engineering_details['engineered_numerical_cols'] = list(set(engineered_numericals_temp))\n",
    "        return df\n",
    "\n",
    "    def _create_interaction_features_post_te(self, df_input: pd.DataFrame, is_fitting_phase: bool) -> pd.DataFrame:\n",
    "        df = df_input.copy()\n",
    "        print(\"    Feature Engineering Step 4: Creating interaction features (post-target encoding)...\")\n",
    "        newly_created_interactions_temp = []\n",
    "\n",
    "        # Use log_premium if available, else original premium (ensure it exists)\n",
    "        premium_col_for_interact = f'fe_log_{self.premium_col_for_fe}' if f'fe_log_{self.premium_col_for_fe}' in df.columns else self.premium_col_for_fe\n",
    "        if premium_col_for_interact not in df.columns:\n",
    "            print(f\"      FE Interaction: Base premium column '{premium_col_for_interact}' for interactions not found. Skipping.\")\n",
    "            if is_fitting_phase: self.feature_engineering_details['engineered_interaction_cols'] = []\n",
    "            return df\n",
    "        \n",
    "        # Ensure premium column for interaction is numeric\n",
    "        df[premium_col_for_interact] = pd.to_numeric(df[premium_col_for_interact], errors='coerce').fillna(0)\n",
    "\n",
    "        # Example: Interaction with target-encoded binned age\n",
    "        # The original binned age (e.g., 'fe_age_col_binned') was target encoded.\n",
    "        # Its name remains the same after TE by category_encoders.TargetEncoder.\n",
    "        age_binned_col_name = f'fe_{self.age_col_for_fe}_binned'\n",
    "        if age_binned_col_name in df.columns: # This column is now numeric (target-encoded)\n",
    "            interaction_col_name = f'fe_inter_{premium_col_for_interact}_x_{age_binned_col_name}'\n",
    "            df[interaction_col_name] = df[premium_col_for_interact] * df[age_binned_col_name]\n",
    "            newly_created_interactions_temp.append(interaction_col_name)\n",
    "            print(f\"      FE Interaction: Created {interaction_col_name}\")\n",
    "        \n",
    "        # Example: Interaction with target-encoded raw categorical 'customer_segment'\n",
    "        # (Assuming 'customer_segment' is in self.raw_categorical_cols_config)\n",
    "        customer_segment_col_name = \"customer_segment\" # Example from raw_categorical_cols_config\n",
    "        if customer_segment_col_name in self.raw_categorical_cols_config and customer_segment_col_name in df.columns:\n",
    "            interaction_col_name_seg = f'fe_inter_{premium_col_for_interact}_x_{customer_segment_col_name}'\n",
    "            df[interaction_col_name_seg] = df[premium_col_for_interact] * df[customer_segment_col_name] # Assumes customer_segment is now numeric post-TE\n",
    "            newly_created_interactions_temp.append(interaction_col_name_seg)\n",
    "            print(f\"      FE Interaction: Created {interaction_col_name_seg}\")\n",
    "\n",
    "        if is_fitting_phase:\n",
    "            self.feature_engineering_details['engineered_interaction_cols'] = list(set(newly_created_interactions_temp))\n",
    "        return df\n",
    "\n",
    "\n",
    "    def fit(self, train_pdf: pd.DataFrame):\n",
    "        print(\"    Fitting PandasFeatureEngineeringPreprocessor...\")\n",
    "        if self.label_col_for_te_fitting not in train_pdf.columns:\n",
    "            raise ValueError(f\"Label column '{self.label_col_for_te_fitting}' for Target Encoder fitting not found in training DataFrame.\")\n",
    "        \n",
    "        # --- Step 0: Initial Custom Formatting ---\n",
    "        X_fit_custom_formatted = self._apply_initial_custom_formatting(\n",
    "            train_pdf.drop(columns=[self.label_col_for_te_fitting], errors='ignore')\n",
    "        )\n",
    "        y_fit_series = train_pdf[self.label_col_for_te_fitting].astype(float).copy() # Used by TargetEncoder\n",
    "\n",
    "        # --- Step 1: Create Base Engineered Features ---\n",
    "        X_fit_fe_engineered = self._engineer_features(X_fit_custom_formatted, is_fitting_phase=True)\n",
    "        \n",
    "        # --- Define columns for imputation based on raw and engineered ---\n",
    "        # Valid columns present after initial FE\n",
    "        current_cols_in_X = X_fit_fe_engineered.columns.tolist()\n",
    "        active_numerical_cols = self._get_valid_cols(X_fit_fe_engineered, self.raw_numerical_cols_config + self.feature_engineering_details.get('engineered_numerical_cols', []))\n",
    "        active_categorical_cols = self._get_valid_cols(X_fit_fe_engineered, self.raw_categorical_cols_config + self.feature_engineering_details.get('engineered_categorical_cols', []))\n",
    "        \n",
    "        self.fitted_components['active_numerical_cols_for_impute'] = active_numerical_cols\n",
    "        self.fitted_components['active_categorical_cols_for_impute'] = active_categorical_cols\n",
    "\n",
    "        # --- Step 2: Impute Numerical Features ---\n",
    "        if active_numerical_cols:\n",
    "            num_imputer = SimpleImputer(strategy=self.num_impute_strategy)\n",
    "            X_fit_fe_engineered[active_numerical_cols] = num_imputer.fit_transform(X_fit_fe_engineered[active_numerical_cols])\n",
    "            self.fitted_components['numerical_imputer'] = num_imputer\n",
    "            print(f\"      Fitted Numerical Imputer for: {active_numerical_cols}\")\n",
    "\n",
    "        # --- Step 3: Impute Categorical Features ---\n",
    "        if active_categorical_cols:\n",
    "            cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=self.cat_impute_constant)\n",
    "            X_fit_fe_engineered[active_categorical_cols] = cat_imputer.fit_transform(X_fit_fe_engineered[active_categorical_cols])\n",
    "            self.fitted_components['categorical_imputer'] = cat_imputer\n",
    "            print(f\"      Fitted Categorical Imputer for: {active_categorical_cols}\")\n",
    "            \n",
    "        # --- Step 4: Target Encoding ---\n",
    "        # Target encode all active categorical columns (original raw + newly engineered categoricals)\n",
    "        cols_for_te_fit = list(active_categorical_cols) # Use the imputed ones\n",
    "        target_encoded_output_names = [] # Will be same as input names for category_encoders.TargetEncoder\n",
    "        if cols_for_te_fit:\n",
    "            for col in cols_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55276369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-------------------- CELL 1: IMPORTS -------------------->\n",
    "print(\"Cell 1: Imports - Executing...\")\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', message=\"Previously subsetted data...\")\n",
    "\n",
    "if 'spark' not in locals():\n",
    "    spark = SparkSession.builder.appName(\"Pandas_FE_Preprocessing_UC_Volumes\").getOrCreate()\n",
    "    print(\"SparkSession created.\")\n",
    "else:\n",
    "    print(\"SparkSession already exists.\")\n",
    "\n",
    "print(\"Imports successful for Pandas Preprocessing Pipeline with Feature Engineering (UC Volumes).\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# <-------------------- CELL 2: INIT CELL - GLOBAL CONFIGURATIONS FOR PREPROCESSING -------------------->\n",
    "print(\"\\nCell 2: Global Configurations for Preprocessing - Executing...\")\n",
    "\n",
    "# --- MLflow Configuration ---\n",
    "PREPROCESSING_EXPERIMENT_PATH = \"/Users/your_username@example.com/MVP_Classification_FE_Preprocessing_UCV\" # !!! CHANGE THIS !!!\n",
    "\n",
    "# --- Data Paths (Unity Catalog Volumes) ---\n",
    "# All paths will now be direct /Volumes/ paths\n",
    "UC_BASE_DATA_PATH = \"/Volumes/delfos/\" # !!! From your input: /Volumes/<catalog>/<schema>/<volume>/ !!!\n",
    "\n",
    "FULL_RAW_DATA_PARQUET_PATH = f\"{UC_BASE_DATA_PATH}raw_data_full/full_dataset_generic.parquet\" # !!! UPDATE !!!\n",
    "\n",
    "DATE_COLUMN_FOR_SPLIT = \"date_col\" # !!! GENERIC NAME - UPDATE !!!\n",
    "YOUR_TARGET_COLUMN_NAME = \"target_binary\" # !!! GENERIC NAME - UPDATE (0 or 1) !!!\n",
    "\n",
    "CATEGORICAL_COLUMNS_RAW = [\"cat_feat_1\", \"cat_feat_2\", \"region_col\"] # !!! UPDATE !!!\n",
    "NUMERICAL_COLUMNS_RAW = [\"num_feat_1\", \"age_col\", \"premium_col\", \"interactions_col\"] # !!! UPDATE !!!\n",
    "\n",
    "# --- Output Paths for INTERMEDIATE RAW SPLIT Data (Optional) ---\n",
    "RAW_SPLITS_DIR_UCV = os.path.join(UC_BASE_DATA_PATH, \"raw_splits_v1_fe_ucv\") # Direct UC Volume path\n",
    "RAW_TRAIN_SPLIT_PATH_UCV = os.path.join(RAW_SPLITS_DIR_UCV, \"raw_train_split.parquet\")\n",
    "RAW_TEST_SPLIT_PATH_UCV = os.path.join(RAW_SPLITS_DIR_UCV, \"raw_test_split.parquet\")\n",
    "\n",
    "# --- Output Paths for FINAL PROCESSED Data (Parquet with Named Columns) ---\n",
    "PROCESSED_DATA_VERSION_FE_UCV = \"v1_pandas_fe_ucv_parquet_only\"\n",
    "PROCESSED_DATA_DIR_BASE_FE_UCV = os.path.join(UC_BASE_DATA_PATH, \"processed_data\")\n",
    "PROCESSED_DATA_DIR_VERSIONED_FE_UCV = os.path.join(PROCESSED_DATA_DIR_BASE_FE_UCV, PROCESSED_DATA_VERSION_FE_UCV)\n",
    "\n",
    "SHARED_PROCESSED_TRAIN_PATH = os.path.join(PROCESSED_DATA_DIR_VERSIONED_FE_UCV, \"train_processed_named_cols.parquet\")\n",
    "SHARED_PROCESSED_TEST_PATH = os.path.join(PROCESSED_DATA_DIR_VERSIONED_FE_UCV, \"test_processed_named_cols.parquet\")\n",
    "\n",
    "# --- Path for saving the FITTED PREPROCESSING COMPONENTS for pyfunc model (joblib) ---\n",
    "# This path will be used by joblib.dump for the pyfunc's internal components if not relying on MLflow's auto-pickling.\n",
    "# For MLflow pyfunc, the components are pickled with the model instance. We'll use this path for MLflow to log *from*.\n",
    "# So, this path is a temporary local path on the driver that MLflow can access to pick up the components.\n",
    "# Or, if joblib can write directly to /Volumes/ for MLflow to pick up, we can use that.\n",
    "# Let's assume for `mlflow.pyfunc.log_model`, internal components of the PythonModel instance are pickled.\n",
    "# If we needed to save components separately and tell pyfunc to load them from artifacts, this would be different.\n",
    "# For now, the PythonModel will store fitted components as attributes.\n",
    "\n",
    "MLFLOW_PYFUNC_PREPROCESSOR_ARTIFACT_PATH = \"pandas_classification_fe_preprocessor_ucv\" # Name in MLflow artifacts\n",
    "\n",
    "TEST_SET_SPLIT_RATIO = 0.20\n",
    "TARGET_ENCODING_SMOOTHING = 20.0\n",
    "CATEGORICAL_IMPUTE_CONSTANT = \"__MISSING_CAT__\"\n",
    "NUMERICAL_IMPUTE_STRATEGY = \"median\"\n",
    "GLOBAL_SEED = 117\n",
    "\n",
    "AGE_COL_FOR_FE = \"age_col\"\n",
    "PREMIUM_COL_FOR_FE = \"premium_col\"\n",
    "INTERACTIONS_COL_FOR_FE = \"interactions_col\"\n",
    "AGE_BINS = [0, 18, 30, 45, 60, 120]\n",
    "AGE_BIN_LABELS = ['0-18', '19-30', '31-45', '46-60', '60+']\n",
    "\n",
    "# Ensure output directories exist using os.makedirs with /Volumes/ paths\n",
    "# This assumes your Databricks environment allows direct os operations on /Volumes/\n",
    "try:\n",
    "    os.makedirs(RAW_SPLITS_DIR_UCV, exist_ok=True)\n",
    "    os.makedirs(PROCESSED_DATA_DIR_VERSIONED_FE_UCV, exist_ok=True)\n",
    "    print(f\"Checked/created raw splits directory: {RAW_SPLITS_DIR_UCV}\")\n",
    "    print(f\"Checked/created processed data directory: {PROCESSED_DATA_DIR_VERSIONED_FE_UCV}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create directories using os.makedirs with /Volumes/ paths. Error: {e}\")\n",
    "    print(\"Ensure the UC Volume path is correct, mounted, and you have write permissions from the driver.\")\n",
    "\n",
    "print(f\"--- Preprocessing Global Configurations (UC Volumes Exclusive) Initialized ---\")\n",
    "print(f\"MLflow Experiment Path for Preprocessing: {PREPROCESSING_EXPERIMENT_PATH}\")\n",
    "print(f\"Full Raw Data Input Path: {FULL_RAW_DATA_PARQUET_PATH}\")\n",
    "print(f\"  Output Processed Train Data Path (Parquet Named Cols): {SHARED_PROCESSED_TRAIN_PATH}\")\n",
    "print(f\"  Output Processed Test Data Path (Parquet Named Cols): {SHARED_PROCESSED_TEST_PATH}\")\n",
    "print(f\"  MLflow Pyfunc Preprocessor Artifact Path: {MLFLOW_PYFUNC_PREPROCESSOR_ARTIFACT_PATH}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# <-------------------- CELL 3: PREPROCESSING LOGIC & MLFLOW PYFUNC MODEL CLASS -------------------->\n",
    "print(\"\\nCell 3: Preprocessing Logic & Pyfunc Model Class - Defining (UC Volumes Exclusive)...\")\n",
    "\n",
    "# --- MLflow Utility (get_or_create_experiment - same as before) ---\n",
    "def get_or_create_experiment(experiment_name_param, spark_session_param=None):\n",
    "    try:\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name_param)\n",
    "        if experiment: print(f\"MLflow experiment '{experiment_name_param}' found with ID: {experiment.experiment_id}\"); return experiment.experiment_id\n",
    "        else:\n",
    "            print(f\"MLflow experiment '{experiment_name_param}' not found. Creating.\"); experiment_id = mlflow.create_experiment(name=experiment_name_param)\n",
    "            print(f\"MLflow experiment '{experiment_name_param}' created with ID: {experiment_id}\"); return experiment_id\n",
    "    except Exception as e: print(f\"Error in get_or_create_experiment for '{experiment_name_param}': {e}\"); return None\n",
    "\n",
    "# --- Data Splitting Function (split_by_month_and_stratify - same as before) ---\n",
    "def split_by_month_and_stratify(full_pdf: pd.DataFrame, date_col: str, target_col: str, test_size: float, random_state: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # ... (Full implementation from the previous detailed response) ...\n",
    "    print(f\"  Starting stratified split by month from column '{date_col}' and target '{target_col}'...\")\n",
    "    if date_col not in full_pdf.columns: raise ValueError(f\"Date column '{date_col}' not found.\")\n",
    "    if target_col not in full_pdf.columns: raise ValueError(f\"Target column '{target_col}' not found.\")\n",
    "    try:\n",
    "        temp_df = full_pdf.copy(); temp_df[date_col] = pd.to_datetime(temp_df[date_col]); temp_df['year_month_group'] = temp_df[date_col].dt.to_period('M')\n",
    "    except Exception as e: raise ValueError(f\"Could not process date column '{date_col}'. Error: {e}\")\n",
    "    train_dfs_list, test_dfs_list = [], []\n",
    "    for month_period, group_data in temp_df.groupby('year_month_group'):\n",
    "        if len(group_data) < 2:\n",
    "             if len(group_data) == 1: train_dfs_list.append(group_data)\n",
    "             elif np.random.RandomState(random_state).rand() > test_size : train_dfs_list.append(group_data)\n",
    "             else: test_dfs_list.append(group_data)\n",
    "             continue\n",
    "        target_counts = group_data[target_col].value_counts()\n",
    "        min_samples_per_class_needed = 2 \n",
    "        if len(target_counts) < 2 or target_counts.min() < min_samples_per_class_needed:\n",
    "            month_train_df, month_test_df = train_test_split(group_data, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "        else:\n",
    "            try: month_train_df, month_test_df = train_test_split(group_data, test_size=test_size, random_state=random_state, stratify=group_data[target_col], shuffle=True)\n",
    "            except ValueError as ve: month_train_df, month_test_df = train_test_split(group_data, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "        train_dfs_list.append(month_train_df)\n",
    "        if not month_test_df.empty: test_dfs_list.append(month_test_df)\n",
    "    final_train_df = pd.concat(train_dfs_list).drop(columns=['year_month_group']) if train_dfs_list else pd.DataFrame(columns=full_pdf.columns)\n",
    "    final_test_df = pd.concat(test_dfs_list).drop(columns=['year_month_group']) if test_dfs_list else pd.DataFrame(columns=full_pdf.columns)\n",
    "    print(f\"  Splitting complete. Train shape: {final_train_df.shape}, Test shape: {final_test_df.shape}\")\n",
    "    return final_train_df, final_test_df\n",
    "\n",
    "# --- Pandas Preprocessor as an mlflow.pyfunc.PythonModel (with Feature Engineering) ---\n",
    "class PandasFeatureEngineeringPreprocessor(mlflow.pyfunc.PythonModel):\n",
    "    # ... (Full implementation of the class from the previous response, no changes needed to its internal logic) ...\n",
    "    # The key is that its `fit` method stores components as self.fitted_components['component_name']\n",
    "    # and self.final_feature_columns_in_order.\n",
    "    # Its `predict` method uses these self.fitted_components and self.final_feature_columns_in_order.\n",
    "    # MLflow's `log_model` will pickle this instance, thus saving the fitted state.\n",
    "    def __init__(self, raw_categorical_cols, raw_numerical_cols, date_col_for_fe, age_col_for_fe, premium_col_for_fe, interactions_col_for_fe, age_bins_for_fe, age_bin_labels_for_fe, label_col_for_te_fitting, te_smoothing_factor, cat_impute_constant, num_impute_strategy, global_seed):\n",
    "        self.raw_categorical_cols_config = list(raw_categorical_cols); self.raw_numerical_cols_config = list(raw_numerical_cols)\n",
    "        self.date_col_for_fe = date_col_for_fe; self.age_col_for_fe = age_col_for_fe; self.premium_col_for_fe = premium_col_for_fe; self.interactions_col_for_fe = interactions_col_for_fe\n",
    "        self.age_bins_for_fe = age_bins_for_fe; self.age_bin_labels_for_fe = age_bin_labels_for_fe\n",
    "        self.label_col_for_te_fitting = label_col_for_te_fitting; self.te_smoothing_factor = te_smoothing_factor\n",
    "        self.cat_impute_constant = cat_impute_constant; self.num_impute_strategy = num_impute_strategy; self.global_seed = global_seed\n",
    "        self.fitted_components = {}; self.feature_engineering_details = {}; self.final_feature_columns_in_order = []\n",
    "    def _get_valid_cols(self, df, col_list): return [col for col in col_list if col in df.columns and col != self.label_col_for_te_fitting]\n",
    "    def _apply_initial_custom_formatting(self, df_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df_input.copy(); print(\"    Custom Preprocessing Step 0: Applying initial custom data formatting (User to implement)...\")\n",
    "        # !!! REPLACE THIS WITH YOUR LOGIC !!!\n",
    "        # Example: if 'example_col_to_clean' in df.columns: df['example_col_to_clean'] = df['example_col_to_clean'].str.strip().str.lower()\n",
    "        return df\n",
    "    def _engineer_features(self, df_input: pd.DataFrame, is_fitting_phase: bool) -> pd.DataFrame:\n",
    "        df = df_input.copy(); print(\"    Feature Engineering Step 1: Creating base new features...\")\n",
    "        engineered_categoricals_temp, engineered_numericals_temp = [], []\n",
    "        if self.date_col_for_fe and self.date_col_for_fe in df.columns:\n",
    "            try: s_date = pd.to_datetime(df[self.date_col_for_fe], errors='coerce')\n",
    "            if not s_date.isnull().all():\n",
    "                df['fe_month'] = s_date.dt.month.fillna(-1).astype(int).astype(str); df['fe_day_of_week'] = s_date.dt.dayofweek.fillna(-1).astype(int).astype(str)\n",
    "                df['fe_is_weekend'] = s_date.dt.dayofweek.isin([5,6]).astype(int).astype(str); engineered_categoricals_temp.extend(['fe_month', 'fe_day_of_week', 'fe_is_weekend'])\n",
    "            except Exception as e_date: print(f\"      Warning: Date FE error for {self.date_col_for_fe}: {e_date}\")\n",
    "        for col_name, new_col_prefix in [(self.premium_col_for_fe, \"fe_premium\"), (self.age_col_for_fe, \"fe_age\"), (self.interactions_col_for_fe, \"fe_interactions\")]:\n",
    "            if col_name and col_name in df.columns:\n",
    "                df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "                df[f'{new_col_prefix}_log1p'] = np.log1p(df[col_name].fillna(0).clip(lower=0)); df[f'{new_col_prefix}_sq'] = df[col_name].fillna(0)**2\n",
    "                engineered_numericals_temp.extend([f'{new_col_prefix}_log1p', f'{new_col_prefix}_sq'])\n",
    "        if self.age_col_for_fe and self.age_col_for_fe in df.columns and self.age_bins_for_fe and self.age_bin_labels_for_fe:\n",
    "            age_binned_name = f'fe_{self.age_col_for_fe}_binned'\n",
    "            df[self.age_col_for_fe] = pd.to_numeric(df[self.age_col_for_fe], errors='coerce')\n",
    "            df[age_binned_name] = pd.cut(df[self.age_col_for_fe], bins=self.age_bins_for_fe, labels=self.age_bin_labels_for_fe, right=False, include_lowest=True)\n",
    "            df[age_binned_name] = df[age_binned_name].astype(str).fillna(self.cat_impute_constant); engineered_categoricals_temp.append(age_binned_name)\n",
    "        if is_fitting_phase: self.feature_engineering_details['engineered_categorical_cols'] = list(set(engineered_categoricals_temp)); self.feature_engineering_details['engineered_numerical_cols'] = list(set(engineered_numericals_temp))\n",
    "        return df\n",
    "    def _create_interaction_features_post_te(self, df_input: pd.DataFrame, is_fitting_phase: bool) -> pd.DataFrame:\n",
    "        df = df_input.copy(); print(\"    Feature Engineering Step 3: Creating interaction features (post-target encoding)...\")\n",
    "        newly_created_interactions_temp = []\n",
    "        premium_col_for_interact = f'fe_log_{self.premium_col_for_fe}' if f'fe_log_{self.premium_col_for_fe}' in df.columns else self.premium_col_for_fe\n",
    "        if premium_col_for_interact not in df.columns:\n",
    "            if is_fitting_phase: self.feature_engineering_details['engineered_interaction_cols'] = []\n",
    "            return df\n",
    "        df[premium_col_for_interact] = pd.to_numeric(df[premium_col_for_interact], errors='coerce').fillna(0)\n",
    "        age_binned_col_name = f'fe_{self.age_col_for_fe}_binned' \n",
    "        if age_binned_col_name in df.columns:\n",
    "            df[f'fe_inter_{premium_col_for_interact}_x_age_binned'] = df[premium_col_for_interact] * pd.to_numeric(df[age_binned_col_name], errors='coerce').fillna(0)\n",
    "            newly_created_interactions_temp.append(f'fe_inter_{premium_col_for_interact}_x_age_binned')\n",
    "        customer_segment_col_name = \"customer_segment\" # Example\n",
    "        if customer_segment_col_name in self.raw_categorical_cols_config and customer_segment_col_name in df.columns:\n",
    "            df[f'fe_inter_{premium_col_for_interact}_x_{customer_segment_col_name}'] = df[premium_col_for_interact] * pd.to_numeric(df[customer_segment_col_name], errors='coerce').fillna(0)\n",
    "            newly_created_interactions_temp.append(f'fe_inter_{premium_col_for_interact}_x_{customer_segment_col_name}')\n",
    "        if is_fitting_phase: self.feature_engineering_details['engineered_interaction_cols'] = list(set(newly_created_interactions_temp))\n",
    "        return df\n",
    "    def fit(self, train_pdf: pd.DataFrame):\n",
    "        print(\"    Fitting PandasFeatureEngineeringPreprocessor...\"); X_fit_no_label = train_pdf.drop(columns=[self.label_col_for_te_fitting], errors='ignore').copy()\n",
    "        y_fit_series = train_pdf[self.label_col_for_te_fitting].astype(float).copy()\n",
    "        X_fit_custom = self._apply_initial_custom_formatting(X_fit_no_label)\n",
    "        X_fit_fe = self._engineer_features(X_fit_custom, is_fitting_phase=True)\n",
    "        active_num_cols = self._get_valid_cols(X_fit_fe, self.raw_numerical_cols_config + self.feature_engineering_details.get('engineered_numerical_cols', []))\n",
    "        active_cat_cols = self._get_valid_cols(X_fit_fe, self.raw_categorical_cols_config + self.feature_engineering_details.get('engineered_categorical_cols', []))\n",
    "        self.fitted_components['active_numerical_cols_for_impute'] = active_num_cols; self.fitted_components['active_categorical_cols_for_impute'] = active_cat_cols\n",
    "        if active_num_cols:\n",
    "            num_imputer = SimpleImputer(strategy=self.num_impute_strategy); X_fit_fe[active_num_cols] = num_imputer.fit_transform(X_fit_fe[active_num_cols])\n",
    "            self.fitted_components['numerical_imputer'] = num_imputer\n",
    "        if active_cat_cols:\n",
    "            cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=self.cat_impute_constant); X_fit_fe[active_cat_cols] = cat_imputer.fit_transform(X_fit_fe[active_cat_cols])\n",
    "            self.fitted_components['categorical_imputer'] = cat_imputer\n",
    "        cols_for_te_fit = list(active_cat_cols); target_encoded_output_names = []\n",
    "        if cols_for_te_fit:\n",
    "            for col in cols_for_te_fit: X_fit_fe[col] = X_fit_fe[col].astype('category')\n",
    "            target_encoder = ce.TargetEncoder(cols=cols_for_te_fit, smoothing=self.te_smoothing_factor, handle_unknown='value', handle_missing='value')\n",
    "            X_fit_te_transformed = target_encoder.fit_transform(X_fit_fe[cols_for_te_fit], y_fit_series)\n",
    "            for col in cols_for_te_fit: X_fit_fe[col] = X_fit_te_transformed[col]\n",
    "            self.fitted_components['target_encoder'] = target_encoder; target_encoded_output_names = list(cols_for_te_fit)\n",
    "            self.fitted_components['target_encoded_cols_list'] = target_encoded_output_names\n",
    "        X_fit_interactions = self._create_interaction_features_post_te(X_fit_fe, is_fitting_phase=True)\n",
    "        self.all_numerical_cols_for_scaling = list(set(active_num_cols + target_encoded_output_names + self.feature_engineering_details.get('engineered_interaction_cols', [])))\n",
    "        self.final_feature_columns_in_order = [col for col in self.all_numerical_cols_for_scaling if col in X_fit_interactions.columns]\n",
    "        final_impute_medians_before_scale = {}\n",
    "        if self.final_feature_columns_in_order:\n",
    "            for col in self.final_feature_columns_in_order:\n",
    "                X_fit_interactions[col] = pd.to_numeric(X_fit_interactions[col], errors='coerce')\n",
    "                if X_fit_interactions[col].isnull().any(): median_val = X_fit_interactions[col].median(); X_fit_interactions[col] = X_fit_interactions[col].fillna(median_val); final_impute_medians_before_scale[col] = median_val\n",
    "            if final_impute_medians_before_scale: self.fitted_components['final_impute_medians_before_scale'] = final_impute_medians_before_scale\n",
    "            scaler = StandardScaler(); X_fit_interactions[self.final_feature_columns_in_order] = scaler.fit_transform(X_fit_interactions[self.final_feature_columns_in_order])\n",
    "            self.fitted_components['scaler'] = scaler\n",
    "        print(\"    PandasFeatureEngineeringPreprocessor fitting complete.\"); return self\n",
    "    def predict(self, context, model_input_pdf: pd.DataFrame):\n",
    "        print(f\"  Pyfunc Preprocessor: Applying full transformation to input DataFrame with shape {model_input_pdf.shape}...\")\n",
    "        X_data_pd_orig_cols = model_input_pdf.copy()\n",
    "        if not hasattr(self, 'final_feature_columns_in_order') or not self.final_feature_columns_in_order: raise RuntimeError(\"Preprocessor not fitted.\")\n",
    "        X_data_pd_custom = self._apply_initial_custom_formatting(X_data_pd_orig_cols)\n",
    "        X_data_pd_fe = self._engineer_features(X_data_pd_custom, is_fitting_phase=False)\n",
    "        active_num_cols = self._get_valid_cols(X_data_pd_fe, self.fitted_components.get('active_numerical_cols_for_impute', []))\n",
    "        if active_num_cols and self.fitted_components.get('numerical_imputer'): X_data_pd_fe[active_num_cols] = self.fitted_components['numerical_imputer'].transform(X_data_pd_fe[active_num_cols])\n",
    "        active_cat_cols = self._get_valid_cols(X_data_pd_fe, self.fitted_components.get('active_categorical_cols_for_impute', []))\n",
    "        if active_cat_cols and self.fitted_components.get('categorical_imputer'): X_data_pd_fe[active_cat_cols] = self.fitted_components['categorical_imputer'].transform(X_data_pd_fe[active_cat_cols])\n",
    "        te_cols_to_transform = self._get_valid_cols(X_data_pd_fe, self.fitted_components.get('target_encoded_cols_list', []))\n",
    "        if te_cols_to_transform and self.fitted_components.get('target_encoder'):\n",
    "            for col in te_cols_to_transform: X_data_pd_fe[col] = X_data_pd_fe[col].astype('category')\n",
    "            transformed_te_cols = self.fitted_components['target_encoder'].transform(X_data_pd_fe[te_cols_to_transform])\n",
    "            for col in te_cols_to_transform: X_data_pd_fe[col] = transformed_te_cols[col]\n",
    "        X_data_pd_interactions = self._create_interaction_features_post_te(X_data_pd_fe, is_fitting_phase=False)\n",
    "        output_df_features_only = X_data_pd_interactions\n",
    "        if self.final_feature_columns_in_order and self.fitted_components.get('scaler'):\n",
    "            final_impute_medians = self.fitted_components.get('final_impute_medians_before_scale', {})\n",
    "            temp_df_for_scaling = pd.DataFrame(index=output_df_features_only.index)\n",
    "            for col in self.final_feature_columns_in_order:\n",
    "                if col in output_df_features_only.columns: temp_df_for_scaling[col] = pd.to_numeric(output_df_features_only[col], errors='coerce').fillna(final_impute_medians.get(col, 0))\n",
    "                else: temp_df_for_scaling[col] = final_impute_medians.get(col, 0)\n",
    "            scaled_values = self.fitted_components['scaler'].transform(temp_df_for_scaling[self.final_feature_columns_in_order])\n",
    "            output_df_features_only = pd.DataFrame(scaled_values, columns=self.final_feature_columns_in_order, index=output_df_features_only.index)\n",
    "        elif self.final_feature_columns_in_order : output_df_features_only = output_df_features_only[self.final_feature_columns_in_order]\n",
    "        else: # Should not happen if fit was successful\n",
    "            print(\"    Error/Warning: final_feature_columns_in_order is empty in predict. Returning empty DataFrame.\")\n",
    "            return pd.DataFrame(index=model_input_pdf.index)\n",
    "        print(f\"  Pyfunc Preprocessor: Transformation complete. Output shape {output_df_features_only.shape}\")\n",
    "        return output_df_features_only\n",
    "\n",
    "# --- Helper to Save Processed Outputs (Parquet Only) ---\n",
    "def save_processed_pandas_outputs_parquet_only(\n",
    "    processed_features_pdf: pd.DataFrame, # Contains ONLY processed features\n",
    "    original_label_series: pd.Series,     # Original labels corresponding to the features\n",
    "    parquet_file_path: str,               # Full /Volumes/... path for saving\n",
    "    label_col_name_in_output: str\n",
    "    ):\n",
    "    \"\"\"Saves processed features and labels as a single Parquet file.\"\"\"\n",
    "    df_to_save_parquet = processed_features_pdf.copy()\n",
    "    \n",
    "    # Add label back if it was provided (e.g., for train/test sets)\n",
    "    if original_label_series is not None:\n",
    "        # Ensure index alignment if possible, though for ML often just need values aligned\n",
    "        if len(df_to_save_parquet) == len(original_label_series):\n",
    "            df_to_save_parquet[label_col_name_in_output] = original_label_series.values\n",
    "        else:\n",
    "            print(f\"  Warning: Length mismatch between processed features ({len(df_to_save_parquet)}) and labels ({len(original_label_series)}). Label not added to Parquet.\")\n",
    "            \n",
    "    print(f\"  Saving named cols Parquet data (shape: {df_to_save_parquet.shape}) to: {parquet_file_path}\")\n",
    "    try:\n",
    "        # For direct Python I/O to UC Volumes, ensure the path is usable by pandas.\n",
    "        # If parquet_file_path is already /dbfs/Volumes/..., it's fine.\n",
    "        # If it's /Volumes/..., pandas might need /dbfs/ explicitly for some environments.\n",
    "        # However, for consistency with user request, we'll use /Volumes/... directly.\n",
    "        # Ensure parent directory exists\n",
    "        os.makedirs(os.path.dirname(parquet_file_path), exist_ok=True)\n",
    "        df_to_save_parquet.to_parquet(parquet_file_path, index=False)\n",
    "        print(f\"    Named cols Parquet data saved successfully to {parquet_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR saving named cols Parquet to {parquet_file_path}: {e}\")\n",
    "        print(f\"    Attempted path was: {parquet_file_path}. If this fails, ensure the path is accessible for direct Python I/O or try prefixing with /dbfs for UC Volumes.\")\n",
    "        raise\n",
    "            \n",
    "print(\"--- Pandas Preprocessing Pyfunc Model Class and Helpers (with Full FE & Parquet Only Save, UCV Paths) Defined ---\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# <-------------------- CELL 4: MAIN PREPROCESSING ORCHESTRATION (with FE & Parquet Only Save, UCV Paths) -------------------->\n",
    "print(\"\\nCell 4: Main Preprocessing Orchestration - Executing (UC Volumes Exclusive)...\")\n",
    "\n",
    "global main_preprocessing_mlflow_experiment_id_ucv # Make it global if used inside functions\n",
    "main_preprocessing_mlflow_experiment_id_ucv = None\n",
    "try:\n",
    "    main_preprocessing_mlflow_experiment_id_ucv = get_or_create_experiment(PREPROCESSING_EXPERIMENT_PATH, spark)\n",
    "    if main_preprocessing_mlflow_experiment_id_ucv:\n",
    "        mlflow.set_experiment(experiment_id=main_preprocessing_mlflow_experiment_id_ucv)\n",
    "        print(f\"MLflow experiment '{PREPROCESSING_EXPERIMENT_PATH}' for preprocessing is set with ID: {main_preprocessing_mlflow_experiment_id_ucv}\")\n",
    "    else:\n",
    "        raise Exception(\"Preprocessing MLflow experiment could not be set. Halting.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL: Could not initialize MLflow experiment for preprocessing. Error: {e}\")\n",
    "    # dbutils.notebook.exit(\"MLflow experiment setup failed\") # Halt\n",
    "\n",
    "if main_preprocessing_mlflow_experiment_id_ucv:\n",
    "    print(f\"\\nLoading FULL RAW data for Pandas preprocessing from: {FULL_RAW_DATA_PARQUET_PATH}\")\n",
    "    # Assuming load_raw_data_to_pandas_from_uc_volume is defined (it was in Cell 3 of previous response)\n",
    "    # For completeness, let's redefine it here or ensure it uses /Volumes/ directly.\n",
    "    def load_raw_data_pandas_ucv(uc_volume_parquet_path: str) -> pd.DataFrame:\n",
    "        print(f\"  Attempting to load Pandas DataFrame directly from UC Volume path: {uc_volume_parquet_path}\")\n",
    "        try:\n",
    "            # Forcing use of /Volumes/ path directly for pandas\n",
    "            pdf = pd.read_parquet(uc_volume_parquet_path)\n",
    "            print(f\"    Successfully loaded. Shape: {pdf.shape}\")\n",
    "            return pdf\n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR loading Parquet from {uc_volume_parquet_path}: {e}\")\n",
    "            print(f\"    Ensure path is correct and accessible. If issues persist, try prepending /dbfs to the path for pandas.\")\n",
    "            raise\n",
    "    full_raw_pdf = load_raw_data_pandas_ucv(FULL_RAW_DATA_PARQUET_PATH)\n",
    "    \n",
    "    print(\"\\nPerforming stratified train-test split by month and target...\")\n",
    "    raw_train_pdf, raw_test_pdf = split_by_month_and_stratify(\n",
    "        full_pdf=full_raw_pdf, date_col=DATE_COLUMN_FOR_SPLIT,\n",
    "        target_col=YOUR_TARGET_COLUMN_NAME, test_size=TEST_SET_SPLIT_RATIO,\n",
    "        random_state=GLOBAL_SEED\n",
    "    )\n",
    "    \n",
    "    # Optional: Save intermediate raw splits to UC Volumes using direct /Volumes/ path\n",
    "    if not raw_train_pdf.empty: \n",
    "        os.makedirs(os.path.dirname(RAW_TRAIN_SPLIT_PATH_UCV), exist_ok=True)\n",
    "        raw_train_pdf.to_parquet(RAW_TRAIN_SPLIT_PATH_UCV, index=False)\n",
    "        print(f\"  Intermediate raw train split ({raw_train_pdf.shape}) saved to: {RAW_TRAIN_SPLIT_PATH_UCV}\")\n",
    "    if not raw_test_pdf.empty: \n",
    "        os.makedirs(os.path.dirname(RAW_TEST_SPLIT_PATH_UCV), exist_ok=True)\n",
    "        raw_test_pdf.to_parquet(RAW_TEST_SPLIT_PATH_UCV, index=False)\n",
    "        print(f\"  Intermediate raw test split ({raw_test_pdf.shape}) saved to: {RAW_TEST_SPLIT_PATH_UCV}\")\n",
    "\n",
    "\n",
    "    pyfunc_model_uri_saved_final = None\n",
    "    fitted_preprocessor_to_log = None\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Pandas_FullFE_Pyfunc_UCV_FitLog\") as preproc_run_final:\n",
    "        run_id_main_preproc_final = preproc_run_final.info.run_id\n",
    "        print(f\"\\nFitting and Logging Full FE Pandas Pyfunc Preprocessor. MLflow Run ID: {run_id_main_preproc_final}\")\n",
    "        \n",
    "        # Log key configurations (as before)\n",
    "        mlflow.log_params({\n",
    "            \"label_column\": YOUR_TARGET_COLUMN_NAME, \"raw_cat_cols\": \", \".join(CATEGORICAL_COLUMNS_RAW),\n",
    "            \"raw_num_cols\": \", \".join(NUMERICAL_COLUMNS_RAW), \"date_col_fe\": DATE_COLUMN_FOR_SPLIT, \n",
    "            \"age_col_fe\": AGE_COL_FOR_FE, \"premium_col_fe\": PREMIUM_COL_FOR_FE, \n",
    "            \"interactions_col_fe\": INTERACTIONS_COL_FOR_FE, \"age_bins_config\": str(AGE_BINS), \n",
    "            \"te_smoothing\": TARGET_ENCODING_SMOOTHING, \"cat_impute_const\": CATEGORICAL_IMPUTE_CONSTANT,\n",
    "            \"num_impute_strat\": NUMERICAL_IMPUTE_STRATEGY, \"test_split_ratio\": TEST_SET_SPLIT_RATIO,\n",
    "            \"data_version_proc\": PROCESSED_DATA_VERSION_FE_UCV\n",
    "        })\n",
    "        mlflow.set_tag(\"GLOBAL_SEED\", GLOBAL_SEED); mlflow.set_tag(\"preprocessing_type\", \"pandas_pyfunc_full_fe_ucv\")\n",
    "        mlflow.log_param(\"raw_train_split_path_used\", RAW_TRAIN_SPLIT_PATH_UCV) # Log /Volumes/ path\n",
    "\n",
    "        try:\n",
    "            preprocessor_instance_final = PandasFeatureEngineeringPreprocessor(\n",
    "                raw_categorical_cols=CATEGORICAL_COLUMNS_RAW, raw_numerical_cols=NUMERICAL_COLUMNS_RAW,\n",
    "                date_col_for_fe=DATE_COLUMN_FOR_SPLIT, age_col_for_fe=AGE_COL_FOR_FE, \n",
    "                premium_col_for_fe=PREMIUM_COL_FOR_FE, interactions_col_for_fe=INTERACTIONS_COL_FOR_FE,\n",
    "                age_bins_for_fe=AGE_BINS, age_bin_labels_for_fe=AGE_BIN_LABELS,\n",
    "                label_col_for_te_fitting=YOUR_TARGET_COLUMN_NAME,\n",
    "                te_smoothing_factor=TARGET_ENCODING_SMOOTHING,\n",
    "                cat_impute_constant=CATEGORICAL_IMPUTE_CONSTANT,\n",
    "                num_impute_strategy=NUMERICAL_IMPUTE_STRATEGY,\n",
    "                global_seed=GLOBAL_SEED\n",
    "            )\n",
    "            \n",
    "            fitted_preprocessor_to_log = preprocessor_instance_final.fit(raw_train_pdf)\n",
    "            print(\"  Full FE Preprocessor instance fitted successfully.\")\n",
    "\n",
    "            conda_env_final_fe = {\n",
    "                'channels': ['conda-forge', 'defaults'],\n",
    "                'dependencies': [\n",
    "                    f'python={pd.__version__.split(\".\")[0]}.{pd.__version__.split(\".\")[1]}', 'pip',\n",
    "                    {'pip': [\n",
    "                        f'mlflow>={mlflow.__version__}', f'pandas>={pd.__version__}',\n",
    "                        f'numpy>={np.__version__}', f'scikit-learn>={sklearn.__version__}',\n",
    "                        f'category-encoders>={ce.__version__}', f'joblib>={joblib.__version__}',\n",
    "                        'pyarrow'\n",
    "                    ],},\n",
    "                ],'name': 'pandas_full_fe_preprocessor_env_ucv'\n",
    "            }\n",
    "            \n",
    "            input_example_df_final = raw_train_pdf.drop(columns=[YOUR_TARGET_COLUMN_NAME], errors='ignore').head(5) if not raw_train_pdf.empty else None\n",
    "            signature_final = None\n",
    "            if input_example_df_final is not None and not input_example_df_final.empty:\n",
    "                try:\n",
    "                    output_example_for_pyfunc_final = fitted_preprocessor_to_log.predict(None, input_example_df_final)\n",
    "                    signature_final = mlflow.models.infer_signature(input_example_df_final, output_example_for_pyfunc_final)\n",
    "                    print(\"  Signature inferred for Pyfunc model.\")\n",
    "                except Exception as sig_e: print(f\"  Warning: Could not infer signature for Pyfunc model. Error: {sig_e}\")\n",
    "\n",
    "            mlflow.pyfunc.log_model(\n",
    "                artifact_path=MLFLOW_PYFUNC_PREPROCESSOR_ARTIFACT_PATH,\n",
    "                python_model=fitted_preprocessor_to_log,\n",
    "                conda_env=conda_env_final_fe,\n",
    "                input_example=input_example_df_main, # Renamed variable from prev response\n",
    "                signature=signature_main # Renamed variable from prev response\n",
    "            )\n",
    "            pyfunc_model_uri_saved_final = f\"runs:/{run_id_main_preproc_final}/{MLFLOW_PYFUNC_PREPROCESSOR_ARTIFACT_PATH}\"\n",
    "            mlflow.set_tag(\"pyfunc_model_uri\", pyfunc_model_uri_saved_final)\n",
    "            \n",
    "            if hasattr(fitted_preprocessor_to_log, 'final_feature_columns_in_order'):\n",
    "                 mlflow.log_param(\"final_feature_names_count\", len(fitted_preprocessor_to_log.final_feature_columns_in_order))\n",
    "                 # To log all features (might be long):\n",
    "                 # mlflow.log_text(\",\".join(fitted_preprocessor_to_log.final_feature_columns_in_order), \"final_feature_names.txt\")\n",
    "            mlflow.set_tag(\"status_fit_log_pyfunc\", \"success\")\n",
    "            print(f\"  Fitted Pyfunc Preprocessor with Full FE saved to MLflow: {pyfunc_model_uri_saved_final}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR during Pyfunc Preprocessor (Full FE) fitting or logging: {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "            mlflow.log_param(\"error_fit_pyfunc\", str(e)[:250])\n",
    "            mlflow.set_tag(\"status_fit_log_pyfunc\", \"failed\")\n",
    "            raise\n",
    "\n",
    "    # --- 4. Transform Train and Test Data using the FITTED instance & Save Outputs (Parquet Only) ---\n",
    "    if fitted_preprocessor_to_log:\n",
    "        print(\"\\nTransforming TRAIN data using fitted preprocessor instance (Full FE)...\")\n",
    "        try:\n",
    "            # The predict method of our Pyfunc model returns only the feature DataFrame\n",
    "            processed_train_features_pdf = fitted_preprocessor_to_log.predict(None, raw_train_pdf.drop(columns=[YOUR_TARGET_COLUMN_NAME], errors='ignore'))\n",
    "            \n",
    "            # Re-attach the original label for saving\n",
    "            processed_train_pdf_to_save = processed_train_features_pdf.copy()\n",
    "            # Ensure index is aligned if raw_train_pdf was manipulated, or just use values\n",
    "            if len(processed_train_pdf_to_save) == len(raw_train_pdf):\n",
    "                 processed_train_pdf_to_save[YOUR_TARGET_COLUMN_NAME] = raw_train_pdf[YOUR_TARGET_COLUMN_NAME].values\n",
    "            else:\n",
    "                 print(f\"Warning: Length mismatch when re-attaching train labels. Train features shape {processed_train_features_pdf.shape}, raw_train_pdf shape {raw_train_pdf.shape}\")\n",
    "                 # Fallback or error\n",
    "            \n",
    "            save_processed_pandas_outputs_parquet_only( # Defined in Cell 3\n",
    "                processed_pdf=processed_train_pdf_to_save,\n",
    "                parquet_file_path=SHARED_PROCESSED_TRAIN_PATH # Path from Init Cell (now direct /Volumes/)\n",
    "            )\n",
    "            with mlflow.start_run(run_id=run_id_main_preproc_final, nested=False):\n",
    "                 mlflow.set_tag(\"output_train_parquet_path\", SHARED_PROCESSED_TRAIN_PATH) # Log direct /Volumes/\n",
    "            print(f\"  Processed TRAIN data with Full FE saved as Parquet to: {SHARED_PROCESSED_TRAIN_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR during TRAIN data transformation or saving (Full FE): {e}\")\n",
    "            with mlflow.start_run(run_id=run_id_main_preproc_final, nested=False): mlflow.log_param(\"error_transform_train_fe\", str(e)[:250])\n",
    "            raise\n",
    "\n",
    "        print(\"\\nTransforming TEST data using fitted preprocessor instance (Full FE)...\")\n",
    "        try:\n",
    "            processed_test_features_pdf = fitted_preprocessor_to_log.predict(None, raw_test_pdf.drop(columns=[YOUR_TARGET_COLUMN_NAME], errors='ignore'))\n",
    "            processed_test_pdf_to_save = processed_test_features_pdf.copy()\n",
    "            if YOUR_TARGET_COLUMN_NAME in raw_test_pdf.columns:\n",
    "                if len(processed_test_pdf_to_save) == len(raw_test_pdf):\n",
    "                     processed_test_pdf_to_save[YOUR_TARGET_COLUMN_NAME] = raw_test_pdf[YOUR_TARGET_COLUMN_NAME].values\n",
    "                else:\n",
    "                     print(f\"Warning: Length mismatch when re-attaching test labels. Test features shape {processed_test_features_pdf.shape}, raw_test_pdf shape {raw_test_pdf.shape}\")\n",
    "\n",
    "\n",
    "            save_processed_pandas_outputs_parquet_only( # Defined in Cell 3\n",
    "                processed_pdf=processed_test_pdf_to_save,\n",
    "                parquet_file_path=SHARED_PROCESSED_TEST_PATH\n",
    "            )\n",
    "            with mlflow.start_run(run_id=run_id_main_preproc_final, nested=False):\n",
    "                 mlflow.set_tag(\"output_test_parquet_path\", SHARED_PROCESSED_TEST_PATH)\n",
    "            print(f\"  Processed TEST data with Full FE saved as Parquet to: {SHARED_PROCESSED_TEST_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR during TEST data transformation or saving (Full FE): {e}\")\n",
    "            with mlflow.start_run(run_id=run_id_main_preproc_final, nested=False): mlflow.log_param(\"error_transform_test_fe\", str(e)[:250])\n",
    "            raise\n",
    "    else:\n",
    "        print(\"CRITICAL: Preprocessor (Full FE) fitting/logging failed. Cannot transform train/test data.\")\n",
    "\n",
    "    print(\"\\n--- Pandas Preprocessing Orchestration with Full Feature Engineering (UC Volumes Exclusive) Completed ---\")\n",
    "    if pyfunc_model_uri_saved_final: print(f\"Fitted Pyfunc Preprocessor MLflow URI: {pyfunc_model_uri_saved_final}\")\n",
    "    print(f\"Named Parquet processed training data saved at: {SHARED_PROCESSED_TRAIN_PATH}\")\n",
    "    print(f\"Named Parquet processed test data saved at: {SHARED_PROCESSED_TEST_PATH}\")\n",
    "else:\n",
    "    print(\"Halting script because MLflow experiment for preprocessing could not be set.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# <-------------------- CELL 5: EXAMPLE USAGE OF SAVED PANDAS PREPROCESSOR FOR NEW DATA (INFERENCE) -------------------->\n",
    "# This cell is conceptual and shows how to use the logged Pyfunc model.\n",
    "# print(\"\\nCell 5: Example - Using Saved Pandas Preprocessor for New Data (Inference)...\")\n",
    "\n",
    "# # --- 1. Define Path to New Raw Data and MLflow URI of the Preprocessor ---\n",
    "# # pyfunc_preprocessor_uri_to_load = pyfunc_model_uri_saved_final # From Cell 4 output if run in same session\n",
    "# # OR \"runs:/<RUN_ID_FROM_CELL_4>/pandas_classification_fe_preprocessor_ucv\" \n",
    "# # NEW_RAW_DATA_FOR_INFERENCE_UCV_PATH = \"/Volumes/delfos/new_inference_data/new_predict_data.parquet\" # !!! REPLACE !!!\n",
    "\n",
    "# # --- 2. Load the Pyfunc Preprocessor Model ---\n",
    "# # print(f\"Loading Pyfunc preprocessor from: {pyfunc_preprocessor_uri_to_load}\")\n",
    "# # try:\n",
    "# #     loaded_pyfunc_preprocessor_instance = mlflow.pyfunc.load_model(pyfunc_preprocessor_uri_to_load)\n",
    "# #     print(\"Pyfunc preprocessor loaded successfully for inference.\")\n",
    "# # except Exception as e:\n",
    "# #     print(f\"CRITICAL ERROR: Could not load Pyfunc preprocessor model. {e}\")\n",
    "# #     # dbutils.notebook.exit(\"Failed to load Pyfunc preprocessor\")\n",
    "\n",
    "# # if 'loaded_pyfunc_preprocessor_instance' in locals():\n",
    "# #     # --- 3. Load New Raw Data for Prediction ---\n",
    "# #     print(f\"Loading new raw data for prediction from {NEW_RAW_DATA_FOR_INFERENCE_UCV_PATH}...\")\n",
    "# #     try:\n",
    "# #         new_raw_pdf_to_predict = load_raw_data_pandas_ucv(NEW_RAW_DATA_FOR_INFERENCE_UCV_PATH) # Function from Cell 3\n",
    "# #     except Exception as e:\n",
    "# #         print(f\"CRITICAL ERROR: Could not load new raw data for prediction. {e}\")\n",
    "# #         # dbutils.notebook.exit(\"Failed to load new raw data\")\n",
    "\n",
    "# #     if 'new_raw_pdf_to_predict' in locals() and not new_raw_pdf_to_predict.empty:\n",
    "# #         # --- 4. Apply Preprocessing to New Data ---\n",
    "# #         print(\"Applying Pyfunc preprocessing to new data for prediction...\")\n",
    "# #         try:\n",
    "# #             # The .predict() method of our pyfunc model takes the raw Pandas DF\n",
    "# #             # (it expects features only, so drop label if present in new_raw_pdf_to_predict, though usually not)\n",
    "# #             raw_features_for_pyfunc_predict = new_raw_pdf_to_predict.drop(columns=[YOUR_TARGET_COLUMN_NAME], errors='ignore')\n",
    "            \n",
    "# #             processed_features_for_prediction_pdf = loaded_pyfunc_preprocessor_instance.predict(raw_features_for_pyfunc_predict)\n",
    "            \n",
    "# #             print(f\"Preprocessing of new data complete. Processed feature DataFrame shape: {processed_features_for_prediction_pdf.shape}\")\n",
    "# #             print(\"First 5 rows of processed features for prediction:\")\n",
    "# #             print(processed_features_for_prediction_pdf.head())\n",
    "\n",
    "# #             # This processed_features_for_prediction_pdf (or its .values) can then be fed to your trained ML model.\n",
    "# #             # For example:\n",
    "# #             # final_ml_model = mlflow.sklearn.load_model(\"runs:/<your_ml_model_run_id>/model\")\n",
    "# #             # final_predictions = final_ml_model.predict(processed_features_for_prediction_pdf.values)\n",
    "# #             # print(f\"Sample final predictions on new data: {final_predictions[:5]}\")\n",
    "\n",
    "# #         except Exception as e:\n",
    "# #             print(f\"ERROR during preprocessing or prediction on new data: {e}\")\n",
    "# #             import traceback\n",
    "# #             traceback.print_exc()\n",
    "# #     else:\n",
    "# #         print(\"New raw data for prediction is empty or failed to load.\")\n",
    "# # else:\n",
    "# #     print(\"Pyfunc preprocessor was not loaded. Cannot proceed with inference example.\")\n",
    "# print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
