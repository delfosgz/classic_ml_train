{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b875db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-------------------- CELL 1: IMPORTS -------------------->\n",
    "print(\"Cell 1: Imports - Executing...\")\n",
    "import mlflow\n",
    "import mlflow.pyfunc # For saving custom Python models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib # For saving the pyfunc model's components IF NOT relying solely on mlflow pickling the instance\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split # Used for month-wise splits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce # For TargetEncoder\n",
    "\n",
    "from pyspark.sql import SparkSession # Still useful for environment context\n",
    "\n",
    "# Suppress common warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', message=\"Previously subsetted data...\") # From category_encoders\n",
    "\n",
    "# Ensure spark session is available\n",
    "if 'spark' not in locals():\n",
    "    spark = SparkSession.builder.appName(\"Pandas_FE_Preprocessing_Full_MVP\").getOrCreate()\n",
    "    print(\"SparkSession created.\")\n",
    "else:\n",
    "    print(\"SparkSession already exists.\")\n",
    "\n",
    "print(\"Imports successful for Pandas Preprocessing Pipeline with Feature Engineering.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# <-------------------- CELL 2: INIT CELL - GLOBAL CONFIGURATIONS FOR PREPROCESSING -------------------->\n",
    "print(\"\\nCell 2: Global Configurations for Preprocessing - Executing...\")\n",
    "\n",
    "# --- MLflow Configuration ---\n",
    "# !!! IMPORTANT: SET YOUR MLFLOW EXPERIMENT PATH !!!\n",
    "PREPROCESSING_EXPERIMENT_PATH = \"/Users/your_username@example.com/MVP_Classification_FE_Preprocessing_Full\" # CHANGE THIS\n",
    "\n",
    "# --- Data Paths (Unity Catalog Volumes) ---\n",
    "# !!! IMPORTANT: SET YOUR UNITY CATALOG VOLUME BASE PATH !!!\n",
    "UC_BASE_DATA_PATH = \"/Volumes/delfos/\" # As per your input\n",
    "\n",
    "# --- Path to the FULL RAW input dataset ---\n",
    "# !!! IMPORTANT: UPDATE THIS TO YOUR ACTUAL FULL RAW DATASET PATH IN UC VOLUMES !!!\n",
    "FULL_RAW_DATA_PARQUET_PATH = f\"{UC_BASE_DATA_PATH}raw_data_full/full_dataset_generic.parquet\" # Example\n",
    "\n",
    "# --- Date Column for Stratified Splitting ---\n",
    "# !!! IMPORTANT: SET THE NAME OF YOUR DATE/TIMESTAMP COLUMN IN THE RAW DATA !!!\n",
    "DATE_COLUMN_FOR_SPLIT = \"date_col\" # Example: 'order_date', 'policy_start_date'\n",
    "\n",
    "# --- Output Paths for INTERMEDIATE RAW SPLIT Data (Optional, but good for traceability) ---\n",
    "# Using /dbfs/ prefix for direct Pandas/os operations on UC Volumes\n",
    "DBFS_RAW_SPLITS_DIR = f\"/dbfs{UC_BASE_DATA_PATH}raw_splits_v1_fe/\" # Unique name\n",
    "RAW_TRAIN_SPLIT_PATH = os.path.join(DBFS_RAW_SPLITS_DIR, \"raw_train_split.parquet\")\n",
    "RAW_TEST_SPLIT_PATH = os.path.join(DBFS_RAW_SPLITS_DIR, \"raw_test_split.parquet\")\n",
    "\n",
    "# --- Output Paths for FINAL PROCESSED Data (Parquet with Named Columns) ---\n",
    "PROCESSED_DATA_VERSION_FE = \"v1_pandas_fe_final\" # Versioning for processed data\n",
    "DBFS_PROCESSED_DATA_DIR_BASE_FE = f\"/dbfs{UC_BASE_DATA_PATH}processed_data/\"\n",
    "PROCESSED_DATA_DIR_VERSIONED_FE = os.path.join(DBFS_PROCESSED_DATA_DIR_BASE_FE, PROCESSED_DATA_VERSION_FE)\n",
    "\n",
    "# These paths will point to the Parquet files with named columns\n",
    "SHARED_PROCESSED_TRAIN_PATH = os.path.join(PROCESSED_DATA_DIR_VERSIONED_FE, \"train_processed_named_cols.parquet\")\n",
    "SHARED_PROCESSED_TEST_PATH = os.path.join(PROCESSED_DATA_DIR_VERSIONED_FE, \"test_processed_named_cols.parquet\")\n",
    "\n",
    "# --- MLflow artifact path for the saved pyfunc preprocessor model ---\n",
    "MLFLOW_PYFUNC_PREPROCESSOR_ARTIFACT_PATH = \"pandas_full_preprocessor\"\n",
    "\n",
    "# !!! IMPORTANT: SET YOUR ACTUAL BINARY TARGET LABEL COLUMN NAME (must exist in raw data) !!!\n",
    "YOUR_TARGET_COLUMN_NAME = \"target_binary\" # Example: 0 or 1\n",
    "\n",
    "# --- Define your categorical and numerical columns from the RAW data ---\n",
    "# These are columns read from FULL_RAW_DATA_PARQUET_PATH *before* any FE in the PyFunc model.\n",
    "# The PyFunc model will then create new features based on these (e.g. from date_col, age_col, premium_col).\n",
    "# !!! IMPORTANT: UPDATE THESE LISTS BASED ON YOUR ACTUAL RAW DATASET !!!\n",
    "CATEGORICAL_COLUMNS_RAW = [\"cat_feat_1\", \"cat_feat_2\", \"region_col\"] # Example\n",
    "NUMERICAL_COLUMNS_RAW = [\"num_feat_1\", \"age_col\", \"premium_col\", \"interactions_col\"] # Example\n",
    "\n",
    "# --- Preprocessing Configuration ---\n",
    "TEST_SET_SPLIT_RATIO = 0.20\n",
    "TARGET_ENCODING_SMOOTHING = 20.0 # For category_encoders.TargetEncoder\n",
    "CATEGORICAL_IMPUTE_CONSTANT = \"__MISSING_CAT__\" # For SimpleImputer\n",
    "NUMERICAL_IMPUTE_STRATEGY = \"median\" # For SimpleImputer\n",
    "\n",
    "# --- Reproducibility ---\n",
    "GLOBAL_SEED = 117\n",
    "\n",
    "# --- Feature Engineering Configuration (Example) ---\n",
    "# These original column names are used as basis for FE. They must be in NUMERICAL_COLUMNS_RAW or DATE_COLUMN_FOR_SPLIT.\n",
    "AGE_COL_FOR_FE = \"age_col\" # Must be in NUMERICAL_COLUMNS_RAW\n",
    "PREMIUM_COL_FOR_FE = \"premium_col\" # Must be in NUMERICAL_COLUMNS_RAW\n",
    "INTERACTIONS_COL_FOR_FE = \"interactions_col\" # Example, must be in NUMERICAL_COLUMNS_RAW\n",
    "\n",
    "# For binning age (example)\n",
    "AGE_BINS = [0, 18, 30, 45, 60, 120]\n",
    "AGE_BIN_LABELS = ['0-18', '19-30', '31-45', '46-60', '60+']\n",
    "\n",
    "# Ensure output directories exist (using /dbfs/ prefix for os.makedirs)\n",
    "try:\n",
    "    os.makedirs(DBFS_RAW_SPLITS_DIR, exist_ok=True)\n",
    "    os.makedirs(PROCESSED_DATA_DIR_VERSIONED_FE, exist_ok=True)\n",
    "    print(f\"Checked/created raw splits directory: {DBFS_RAW_SPLITS_DIR}\")\n",
    "    print(f\"Checked/created processed data directory: {PROCESSED_DATA_DIR_VERSIONED_FE}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create directory. Ensure UC Volume '{UC_BASE_DATA_PATH}' exists and you have write permissions. Error: {e}\")\n",
    "\n",
    "print(f\"--- Preprocessing Global Configurations (with FE) Initialized ---\")\n",
    "print(f\"MLflow Experiment Path for Preprocessing: {PREPROCESSING_EXPERIMENT_PATH}\")\n",
    "print(f\"Full Raw Data Input Path: {FULL_RAW_DATA_PARQUET_PATH}\")\n",
    "print(f\"Date Column for Split: {DATE_COLUMN_FOR_SPLIT}\")\n",
    "print(f\"Target Column: {YOUR_TARGET_COLUMN_NAME}\")\n",
    "print(f\"  Output Processed Train Data Path (Parquet Named Cols): {SHARED_PROCESSED_TRAIN_PATH}\")\n",
    "print(f\"  Output Processed Test Data Path (Parquet Named Cols): {SHARED_PROCESSED_TEST_PATH}\")\n",
    "print(f\"  MLflow Pyfunc Preprocessor Artifact Path: {MLFLOW_PYFUNC_PREPROCESSOR_ARTIFACT_PATH}\")\n",
    "print(f\"Categorical Columns (Raw): {CATEGORICAL_COLUMNS_RAW}\")\n",
    "print(f\"Numerical Columns (Raw): {NUMERICAL_COLUMNS_RAW}\")\n",
    "print(f\"Global Seed: {GLOBAL_SEED}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# <-------------------- CELL 3: PREPROCESSING LOGIC & MLFLOW PYFUNC MODEL CLASS -------------------->\n",
    "print(\"\\nCell 3: Preprocessing Logic & Pyfunc Model Class - Defining (with Custom Initial Formatting & Feature Engineering)...\")\n",
    "\n",
    "# --- MLflow Utility ---\n",
    "def get_or_create_experiment(experiment_name_param, spark_session_param=None): # spark_session_param optional\n",
    "    try:\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name_param)\n",
    "        if experiment:\n",
    "            print(f\"MLflow experiment '{experiment_name_param}' found with ID: {experiment.experiment_id}\")\n",
    "            return experiment.experiment_id\n",
    "        else:\n",
    "            print(f\"MLflow experiment '{experiment_name_param}' not found. Attempting to create.\")\n",
    "            experiment_id = mlflow.create_experiment(name=experiment_name_param)\n",
    "            print(f\"MLflow experiment '{experiment_name_param}' created with ID: {experiment_id}\")\n",
    "            return experiment_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_or_create_experiment for '{experiment_name_param}'. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Data Splitting Function ---\n",
    "def split_by_month_and_stratify(full_pdf: pd.DataFrame, \n",
    "                                date_col: str, \n",
    "                                target_col: str, \n",
    "                                test_size: float, \n",
    "                                random_state: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    print(f\"  Starting stratified split by month from column '{date_col}' and target '{target_col}'...\")\n",
    "    if date_col not in full_pdf.columns:\n",
    "        raise ValueError(f\"Date column '{date_col}' not found in DataFrame.\")\n",
    "    if target_col not in full_pdf.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in DataFrame.\")\n",
    "\n",
    "    try:\n",
    "        temp_df = full_pdf.copy()\n",
    "        temp_df[date_col] = pd.to_datetime(temp_df[date_col])\n",
    "        temp_df['year_month_group'] = temp_df[date_col].dt.to_period('M')\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not convert column '{date_col}' to datetime or extract year_month. Error: {e}\")\n",
    "\n",
    "    train_dfs_list = []\n",
    "    test_dfs_list = []\n",
    "    \n",
    "    for month_period, group_data in temp_df.groupby('year_month_group'):\n",
    "        print(f\"    Splitting for month-year: {month_period}, group size: {len(group_data)}\")\n",
    "        if len(group_data) < 2:\n",
    "             print(f\"      Group for {month_period} is too small ({len(group_data)}). Assigning based on ratio if possible, else to train.\")\n",
    "             if len(group_data) == 1: # Single sample goes to train\n",
    "                train_dfs_list.append(group_data)\n",
    "             elif np.random.RandomState(random_state).rand() > test_size : # Use seeded random for consistency\n",
    "                 train_dfs_list.append(group_data)\n",
    "             else:\n",
    "                 test_dfs_list.append(group_data)\n",
    "             continue\n",
    "        \n",
    "        target_counts = group_data[target_col].value_counts()\n",
    "        # Check if any class has fewer samples than required for a split (typically 2 for train_test_split)\n",
    "        # Or if only one class is present in the group\n",
    "        min_samples_per_class_needed = 2 # For train_test_split to be able to make a split for each class\n",
    "        \n",
    "        if len(target_counts) < 2 or target_counts.min() < min_samples_per_class_needed:\n",
    "            print(f\"      Not enough class diversity or samples in {month_period} for stratification (counts: {target_counts.to_dict()}). Performing random split.\")\n",
    "            month_train_df, month_test_df = train_test_split(\n",
    "                group_data, test_size=test_size, random_state=random_state, shuffle=True\n",
    "            )\n",
    "        else:\n",
    "            try:\n",
    "                month_train_df, month_test_df = train_test_split(\n",
    "                    group_data, test_size=test_size, random_state=random_state, \n",
    "                    stratify=group_data[target_col], shuffle=True\n",
    "                )\n",
    "            except ValueError as ve: # Handles \"The least populated class in y has only X members\"\n",
    "                print(f\"      Stratification failed for {month_period} (Error: {ve}). Performing random split.\")\n",
    "                month_train_df, month_test_df = train_test_split(\n",
    "                    group_data, test_size=test_size, random_state=random_state, shuffle=True\n",
    "                )\n",
    "        train_dfs_list.append(month_train_df)\n",
    "        if not month_test_df.empty: # Only append if test_df is not empty\n",
    "             test_dfs_list.append(month_test_df)\n",
    "\n",
    "    final_train_df = pd.concat(train_dfs_list).drop(columns=['year_month_group']) if train_dfs_list else pd.DataFrame(columns=full_pdf.columns)\n",
    "    final_test_df = pd.concat(test_dfs_list).drop(columns=['year_month_group']) if test_dfs_list else pd.DataFrame(columns=full_pdf.columns)\n",
    "\n",
    "\n",
    "    print(f\"  Splitting complete. Train shape: {final_train_df.shape}, Test shape: {final_test_df.shape}\")\n",
    "    if not final_train_df.empty: print(f\"  Train target distribution:\\n{final_train_df[target_col].value_counts(normalize=True, dropna=False)}\")\n",
    "    if not final_test_df.empty: print(f\"  Test target distribution:\\n{final_test_df[target_col].value_counts(normalize=True, dropna=False)}\")\n",
    "    return final_train_df, final_test_df\n",
    "\n",
    "\n",
    "# --- Pandas Preprocessor as an mlflow.pyfunc.PythonModel (with Feature Engineering) ---\n",
    "class PandasFeatureEngineeringPreprocessor(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 raw_categorical_cols, raw_numerical_cols, \n",
    "                 date_col_for_fe, age_col_for_fe, premium_col_for_fe, interactions_col_for_fe,\n",
    "                 age_bins_for_fe, age_bin_labels_for_fe,\n",
    "                 label_col_for_te_fitting, \n",
    "                 te_smoothing_factor, \n",
    "                 cat_impute_constant, num_impute_strategy,\n",
    "                 global_seed):\n",
    "        \n",
    "        self.raw_categorical_cols_config = list(raw_categorical_cols)\n",
    "        self.raw_numerical_cols_config = list(raw_numerical_cols)\n",
    "        self.date_col_for_fe = date_col_for_fe\n",
    "        self.age_col_for_fe = age_col_for_fe\n",
    "        self.premium_col_for_fe = premium_col_for_fe\n",
    "        self.interactions_col_for_fe = interactions_col_for_fe\n",
    "        self.age_bins_for_fe = age_bins_for_fe\n",
    "        self.age_bin_labels_for_fe = age_bin_labels_for_fe\n",
    "        self.label_col_for_te_fitting = label_col_for_te_fitting # Only used for fitting TE\n",
    "        self.te_smoothing_factor = te_smoothing_factor\n",
    "        self.cat_impute_constant = cat_impute_constant\n",
    "        self.num_impute_strategy = num_impute_strategy\n",
    "        self.global_seed = global_seed\n",
    "\n",
    "        # Fitted components and dynamic column lists will be stored here after `fit`\n",
    "        self.fitted_components = {} # Store all imputers, encoders, scalers\n",
    "        self.feature_engineering_details = {} # Store names of engineered features\n",
    "        self.final_feature_columns_in_order = [] # Defines the output columns and their order\n",
    "\n",
    "    def _get_valid_cols(self, df, col_list):\n",
    "        return [col for col in col_list if col in df.columns and col != self.label_col_for_te_fitting]\n",
    "\n",
    "    def _apply_initial_custom_formatting(self, df_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df_input.copy()\n",
    "        print(\"    Custom Preprocessing Step 1: Applying initial custom data formatting...\")\n",
    "        # !!! REPLACE THE CONTENT BELOW WITH YOUR ACTUAL CUSTOM FORMATTING LOGIC !!!\n",
    "        # This function should take the raw DataFrame (without the label if separated early)\n",
    "        # and return the formatted DataFrame.\n",
    "        # The columns listed in raw_categorical_cols_config and raw_numerical_cols_config\n",
    "        # should exist AFTER this step, or this step should create them.\n",
    "        \n",
    "        # Example (ensure this doesn't conflict with your actual column names):\n",
    "        # if 'some_text_column' in df.columns:\n",
    "        #     df['some_text_column'] = df['some_text_column'].astype(str).str.lower().str.strip()\n",
    "        #     print(\"      Example custom formatting: 'some_text_column' processed.\")\n",
    "        # else:\n",
    "        #     print(\"      Example custom formatting: 'some_text_column' not found.\")\n",
    "        print(\"    Custom Preprocessing Step 1: Initial custom data formatting complete.\")\n",
    "        return df\n",
    "\n",
    "    def _engineer_features(self, df_input: pd.DataFrame, is_fitting_phase: bool) -> pd.DataFrame:\n",
    "        df = df_input.copy()\n",
    "        print(\"    Feature Engineering Step 2: Creating new features...\")\n",
    "        \n",
    "        engineered_categoricals_temp = []\n",
    "        engineered_numericals_temp = []\n",
    "\n",
    "        # Date/Time Features\n",
    "        if self.date_col_for_fe and self.date_col_for_fe in df.columns:\n",
    "            try:\n",
    "                s_date = pd.to_datetime(df[self.date_col_for_fe], errors='coerce')\n",
    "                if not s_date.isnull().all(): # Proceed if some dates are valid\n",
    "                    df['fe_month'] = s_date.dt.month.fillna(-1).astype(int).astype(str) # Impute NaT with -1 then to str\n",
    "                    df['fe_day_of_week'] = s_date.dt.dayofweek.fillna(-1).astype(int).astype(str)\n",
    "                    df['fe_is_weekend'] = s_date.dt.dayofweek.isin([5,6]).astype(int).astype(str)\n",
    "                    engineered_categoricals_temp.extend(['fe_month', 'fe_day_of_week', 'fe_is_weekend'])\n",
    "                    print(f\"      FE: Created date features: month, day_of_week, is_weekend from {self.date_col_for_fe}\")\n",
    "            except Exception as e_date: print(f\"      Warning: Could not create date features from {self.date_col_for_fe}. Error: {e_date}\")\n",
    "        \n",
    "        # Numerical Transformations\n",
    "        for col_name, new_col_prefix in [\n",
    "            (self.premium_col_for_fe, \"fe_premium\"), \n",
    "            (self.age_col_for_fe, \"fe_age\"), \n",
    "            (self.interactions_col_for_fe, \"fe_interactions\") # Example\n",
    "        ]:\n",
    "            if col_name and col_name in df.columns:\n",
    "                df[col_name] = pd.to_numeric(df[col_name], errors='coerce') # Ensure numeric\n",
    "                df[f'{new_col_prefix}_log1p'] = np.log1p(df[col_name].fillna(0).clip(lower=0))\n",
    "                df[f'{new_col_prefix}_sq'] = df[col_name].fillna(0)**2\n",
    "                engineered_numericals_temp.extend([f'{new_col_prefix}_log1p', f'{new_col_prefix}_sq'])\n",
    "                print(f\"      FE: Created log1p and squared features for {col_name}\")\n",
    "\n",
    "        # Binning Age\n",
    "        if self.age_col_for_fe and self.age_col_for_fe in df.columns and self.age_bins_for_fe and self.age_bin_labels_for_fe:\n",
    "            age_col_binned_name = f'fe_{self.age_col_for_fe}_binned'\n",
    "            # Ensure age column is numeric before binning\n",
    "            df[self.age_col_for_fe] = pd.to_numeric(df[self.age_col_for_fe], errors='coerce')\n",
    "            df[age_col_binned_name] = pd.cut(df[self.age_col_for_fe], \n",
    "                                             bins=self.age_bins_for_fe, \n",
    "                                             labels=self.age_bin_labels_for_fe, \n",
    "                                             right=False, include_lowest=True)\n",
    "            df[age_col_binned_name] = df[age_col_binned_name].astype(str).fillna(self.cat_impute_constant) # Handle NaNs from binning then to string\n",
    "            engineered_categoricals_temp.append(age_col_binned_name)\n",
    "            print(f\"      FE: Created binned age feature: {age_col_binned_name}\")\n",
    "\n",
    "        if is_fitting_phase:\n",
    "            self.feature_engineering_details['engineered_categorical_cols'] = list(set(engineered_categoricals_temp))\n",
    "            self.feature_engineering_details['engineered_numerical_cols'] = list(set(engineered_numericals_temp))\n",
    "        return df\n",
    "\n",
    "    def _create_interaction_features_post_te(self, df_input: pd.DataFrame, is_fitting_phase: bool) -> pd.DataFrame:\n",
    "        df = df_input.copy()\n",
    "        print(\"    Feature Engineering Step 4: Creating interaction features (post-target encoding)...\")\n",
    "        newly_created_interactions_temp = []\n",
    "\n",
    "        # Use log_premium if available, else original premium (ensure it exists)\n",
    "        premium_col_for_interact = f'fe_log_{self.premium_col_for_fe}' if f'fe_log_{self.premium_col_for_fe}' in df.columns else self.premium_col_for_fe\n",
    "        if premium_col_for_interact not in df.columns:\n",
    "            print(f\"      FE Interaction: Base premium column '{premium_col_for_interact}' for interactions not found. Skipping.\")\n",
    "            if is_fitting_phase: self.feature_engineering_details['engineered_interaction_cols'] = []\n",
    "            return df\n",
    "        \n",
    "        # Ensure premium column for interaction is numeric\n",
    "        df[premium_col_for_interact] = pd.to_numeric(df[premium_col_for_interact], errors='coerce').fillna(0)\n",
    "\n",
    "        # Example: Interaction with target-encoded binned age\n",
    "        # The original binned age (e.g., 'fe_age_col_binned') was target encoded.\n",
    "        # Its name remains the same after TE by category_encoders.TargetEncoder.\n",
    "        age_binned_col_name = f'fe_{self.age_col_for_fe}_binned'\n",
    "        if age_binned_col_name in df.columns: # This column is now numeric (target-encoded)\n",
    "            interaction_col_name = f'fe_inter_{premium_col_for_interact}_x_{age_binned_col_name}'\n",
    "            df[interaction_col_name] = df[premium_col_for_interact] * df[age_binned_col_name]\n",
    "            newly_created_interactions_temp.append(interaction_col_name)\n",
    "            print(f\"      FE Interaction: Created {interaction_col_name}\")\n",
    "        \n",
    "        # Example: Interaction with target-encoded raw categorical 'customer_segment'\n",
    "        # (Assuming 'customer_segment' is in self.raw_categorical_cols_config)\n",
    "        customer_segment_col_name = \"customer_segment\" # Example from raw_categorical_cols_config\n",
    "        if customer_segment_col_name in self.raw_categorical_cols_config and customer_segment_col_name in df.columns:\n",
    "            interaction_col_name_seg = f'fe_inter_{premium_col_for_interact}_x_{customer_segment_col_name}'\n",
    "            df[interaction_col_name_seg] = df[premium_col_for_interact] * df[customer_segment_col_name] # Assumes customer_segment is now numeric post-TE\n",
    "            newly_created_interactions_temp.append(interaction_col_name_seg)\n",
    "            print(f\"      FE Interaction: Created {interaction_col_name_seg}\")\n",
    "\n",
    "        if is_fitting_phase:\n",
    "            self.feature_engineering_details['engineered_interaction_cols'] = list(set(newly_created_interactions_temp))\n",
    "        return df\n",
    "\n",
    "\n",
    "    def fit(self, train_pdf: pd.DataFrame):\n",
    "        print(\"    Fitting PandasFeatureEngineeringPreprocessor...\")\n",
    "        if self.label_col_for_te_fitting not in train_pdf.columns:\n",
    "            raise ValueError(f\"Label column '{self.label_col_for_te_fitting}' for Target Encoder fitting not found in training DataFrame.\")\n",
    "        \n",
    "        # --- Step 0: Initial Custom Formatting ---\n",
    "        X_fit_custom_formatted = self._apply_initial_custom_formatting(\n",
    "            train_pdf.drop(columns=[self.label_col_for_te_fitting], errors='ignore')\n",
    "        )\n",
    "        y_fit_series = train_pdf[self.label_col_for_te_fitting].astype(float).copy() # Used by TargetEncoder\n",
    "\n",
    "        # --- Step 1: Create Base Engineered Features ---\n",
    "        X_fit_fe_engineered = self._engineer_features(X_fit_custom_formatted, is_fitting_phase=True)\n",
    "        \n",
    "        # --- Define columns for imputation based on raw and engineered ---\n",
    "        # Valid columns present after initial FE\n",
    "        current_cols_in_X = X_fit_fe_engineered.columns.tolist()\n",
    "        active_numerical_cols = self._get_valid_cols(X_fit_fe_engineered, self.raw_numerical_cols_config + self.feature_engineering_details.get('engineered_numerical_cols', []))\n",
    "        active_categorical_cols = self._get_valid_cols(X_fit_fe_engineered, self.raw_categorical_cols_config + self.feature_engineering_details.get('engineered_categorical_cols', []))\n",
    "        \n",
    "        self.fitted_components['active_numerical_cols_for_impute'] = active_numerical_cols\n",
    "        self.fitted_components['active_categorical_cols_for_impute'] = active_categorical_cols\n",
    "\n",
    "        # --- Step 2: Impute Numerical Features ---\n",
    "        if active_numerical_cols:\n",
    "            num_imputer = SimpleImputer(strategy=self.num_impute_strategy)\n",
    "            X_fit_fe_engineered[active_numerical_cols] = num_imputer.fit_transform(X_fit_fe_engineered[active_numerical_cols])\n",
    "            self.fitted_components['numerical_imputer'] = num_imputer\n",
    "            print(f\"      Fitted Numerical Imputer for: {active_numerical_cols}\")\n",
    "\n",
    "        # --- Step 3: Impute Categorical Features ---\n",
    "        if active_categorical_cols:\n",
    "            cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=self.cat_impute_constant)\n",
    "            X_fit_fe_engineered[active_categorical_cols] = cat_imputer.fit_transform(X_fit_fe_engineered[active_categorical_cols])\n",
    "            self.fitted_components['categorical_imputer'] = cat_imputer\n",
    "            print(f\"      Fitted Categorical Imputer for: {active_categorical_cols}\")\n",
    "            \n",
    "        # --- Step 4: Target Encoding ---\n",
    "        # Target encode all active categorical columns (original raw + newly engineered categoricals)\n",
    "        cols_for_te_fit = list(active_categorical_cols) # Use the imputed ones\n",
    "        target_encoded_output_names = [] # Will be same as input names for category_encoders.TargetEncoder\n",
    "        if cols_for_te_fit:\n",
    "            for col in cols_"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
