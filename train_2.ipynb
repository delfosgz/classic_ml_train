{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598e2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-------------------- CELL 1: IMPORTS -------------------->\n",
    "print(\"Cell 1: Imports - Executing...\")\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import mlflow.sklearn\n",
    "import mlflow.lightgbm\n",
    "import mlflow.xgboost\n",
    "import mlflow.catboost\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import joblib \n",
    "from typing import List, Dict, Any, Tuple, Union, cast\n",
    "import itertools # For ensemble combinations\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss, brier_score_loss, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
    "from scipy.optimize import minimize \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', message=\"Previously subsetted data...\")\n",
    "warnings.filterwarnings('ignore', message=\"Using `tqdm.autonotebook.tqdm`\") \n",
    "warnings.filterwarnings('ignore', message=\"The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator\") # From UMAP in SHAP if used\n",
    "\n",
    "if 'spark' not in locals():\n",
    "    spark = SparkSession.builder.appName(\"Classification_Training_Ensemble_MVP\").getOrCreate()\n",
    "    print(\"SparkSession created.\")\n",
    "else:\n",
    "    print(\"SparkSession already exists.\")\n",
    "\n",
    "print(\"Imports successful for Training & Ensembling Pipeline.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# <-------------------- CELL 2: INIT CELL - GLOBAL CONFIGURATIONS FOR TRAINING -------------------->\n",
    "print(\"\\nCell 2: Global Configurations for Training - Executing...\")\n",
    "\n",
    "# --- MLflow Configuration ---\n",
    "# !!! IMPORTANT: SET YOUR MLFLOW EXPERIMENT PATH !!!\n",
    "MLFLOW_EXPERIMENT_PATH = \"/Users/your_username@example.com/MVP_Conversion_PriceSensitivity_Training_Full\" # CHANGE THIS\n",
    "\n",
    "# --- Data Paths (Unity Catalog Volumes - Processed Data from Preprocessing Script) ---\n",
    "# These paths MUST point to the Parquet files with named, transformed columns\n",
    "# created by your Pandas preprocessing script.\n",
    "# !!! IMPORTANT: VERIFY THESE PATHS EXACTLY MATCH THE OUTPUT OF YOUR PREPROCESSING SCRIPT !!!\n",
    "UC_BASE_DATA_PATH_TRAINING = \"/Volumes/delfos/\" # From your spec, ensure this is the base\n",
    "PROCESSED_DATA_VERSION_TRAINING = \"v1_pandas_fe_final_output\" # Ensure matches preproc output version\n",
    "PROCESSED_DATA_SUBDIR_TRAINING = \"processed_data\" # Ensure matches preproc output subdir\n",
    "\n",
    "PROCESSED_DATA_DIR_VERSIONED_TRAINING = os.path.join(UC_BASE_DATA_PATH_TRAINING, PROCESSED_DATA_SUBDIR_TRAINING, PROCESSED_DATA_VERSION_TRAINING)\n",
    "\n",
    "SHARED_PROCESSED_TRAIN_PATH = os.path.join(PROCESSED_DATA_DIR_VERSIONED_TRAINING, \"train_processed_named_cols.parquet\")\n",
    "SHARED_PROCESSED_TEST_PATH = os.path.join(PROCESSED_DATA_DIR_VERSIONED_TRAINING, \"test_processed_named_cols.parquet\")\n",
    "\n",
    "# --- Target and Key Feature Names ---\n",
    "# !!! IMPORTANT: SET YOUR ACTUAL TARGET & PREMIUM COLUMN NAMES (as they appear in the processed Parquet) !!!\n",
    "TARGET_COLUMN_NAME = \"target_binary\" # This is your y\n",
    "PREMIUM_COLUMN_NAME = \"premium_col\" # This is one of your X features (processed)\n",
    "\n",
    "# --- Output Paths for OOF/Test Predictions (Parquet) from Base Models ---\n",
    "OOF_PREDS_SUBDIR_TRAINING = \"oof_predictions_classification_v1_mvp\"\n",
    "TEST_PREDS_SUBDIR_TRAINING = \"test_predictions_classification_v1_mvp\"\n",
    "OOF_PREDS_DIR_UCV = os.path.join(UC_BASE_DATA_PATH_TRAINING, OOF_PREDS_SUBDIR_TRAINING)\n",
    "TEST_PREDS_DIR_UCV = os.path.join(UC_BASE_DATA_PATH_TRAINING, TEST_PREDS_SUBDIR_TRAINING)\n",
    "\n",
    "# --- HPO Configuration ---\n",
    "NUM_HPO_TRIALS_PER_ALGO = 10 # !!! START LOW (e.g., 3-5) FOR TESTING, INCREASE LATER (e.g., 25-50) !!!\n",
    "HPO_OPTIMIZATION_METRIC = \"auc_roc\" # Options: 'auc_roc', 'auc_pr', 'log_loss', 'brier_score'. Loss will be -metric if maximizing.\n",
    "\n",
    "# --- Base Algorithms to Run ---\n",
    "BASE_ALGORITHMS_TO_TRAIN = ['lightgbm', 'xgboost', 'catboost'] # As requested\n",
    "\n",
    "# --- Cross-Validation for OOF ---\n",
    "K_FOLDS_OOF = 5\n",
    "\n",
    "# --- Reproducibility ---\n",
    "GLOBAL_SEED = 117\n",
    "\n",
    "# --- Ensemble Configuration ---\n",
    "# Will generate combinations from BASE_ALGORITHMS_TO_TRAIN\n",
    "\n",
    "# --- Other Global Settings ---\n",
    "MAX_METRICS_TO_LOG_PER_RUN = 7\n",
    "PDP_N_GRID_POINTS = 20\n",
    "PDP_PERCENTILES = (0.05, 0.95) # For PDP grid range\n",
    "\n",
    "# Ensure output directories for OOF/Test predictions exist\n",
    "try:\n",
    "    os.makedirs(OOF_PREDS_DIR_UCV, exist_ok=True)\n",
    "    os.makedirs(TEST_PREDS_DIR_UCV, exist_ok=True)\n",
    "    print(f\"Checked/created OOF directory: {OOF_PREDS_DIR_UCV}\")\n",
    "    print(f\"Checked/created Test Preds directory: {TEST_PREDS_DIR_UCV}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create OOF/Test Preds directories. Error: {e}\")\n",
    "\n",
    "print(f\"--- Training Global Configurations Initialized ---\")\n",
    "# ... (Print key configs)\n",
    "print(f\"MLflow Experiment Path: {MLFLOW_EXPERIMENT_PATH}\")\n",
    "print(f\"Processed Train Data Path: {SHARED_PROCESSED_TRAIN_PATH}\")\n",
    "print(f\"Processed Test Data Path: {SHARED_PROCESSED_TEST_PATH}\")\n",
    "print(f\"Base Algorithms: {BASE_ALGORITHMS_TO_TRAIN}\")\n",
    "print(f\"HPO Trials per Algo: {NUM_HPO_TRIALS_PER_ALGO}\")\n",
    "print(f\"HPO Optimization Metric: {HPO_OPTIMIZATION_METRIC.upper()}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# <-------------------- CELL 3: UTILITY FUNCTIONS & CORE LOGIC (HPO, OOF, ENSEMBLE) -------------------->\n",
    "print(\"\\nCell 3: Utility Functions & Core Logic - Defining...\")\n",
    "\n",
    "# --- MLflow Utility ---\n",
    "def get_or_create_experiment(experiment_name_param, spark_session_param=None):\n",
    "    try:\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name_param)\n",
    "        if experiment: print(f\"MLflow experiment '{experiment_name_param}' found with ID: {experiment.experiment_id}\"); return experiment.experiment_id\n",
    "        else:\n",
    "            print(f\"MLflow experiment '{experiment_name_param}' not found. Creating.\"); experiment_id = mlflow.create_experiment(name=experiment_name_param)\n",
    "            print(f\"MLflow experiment '{experiment_name_param}' created with ID: {experiment_id}\"); return experiment_id\n",
    "    except Exception as e: print(f\"Error in get_or_create_experiment for '{experiment_name_param}': {e}\"); return None\n",
    "\n",
    "# --- Algorithm Search Spaces (Classifiers) ---\n",
    "ALGORITHM_CLASSIFIER_SEARCH_SPACES = {\n",
    "    'lightgbm': { 'model_params': {\n",
    "            'n_estimators': hp.quniform('lgbm_n_estimators', 50, 250, 25), 'learning_rate': hp.loguniform('lgbm_learning_rate', np.log(0.01), np.log(0.1)),\n",
    "            'num_leaves': hp.quniform('lgbm_num_leaves', 20, 100, 5), 'max_depth': hp.quniform('lgbm_max_depth', 3, 10, 1),\n",
    "            'subsample': hp.uniform('lgbm_subsample', 0.6, 1.0), 'colsample_bytree': hp.uniform('lgbm_colsample_bytree', 0.6, 1.0),\n",
    "            'reg_alpha': hp.uniform('lgbm_reg_alpha', 0.0, 0.5), 'reg_lambda': hp.uniform('lgbm_reg_lambda', 0.0, 0.5),\n",
    "            'class_weight': hp.choice('lgbm_class_weight', [None, 'balanced'])\n",
    "    }},\n",
    "    'xgboost': { 'model_params': {\n",
    "            'n_estimators': hp.quniform('xgb_n_estimators', 50, 250, 25), 'learning_rate': hp.loguniform('xgb_learning_rate', np.log(0.01), np.log(0.1)),\n",
    "            'max_depth': hp.quniform('xgb_max_depth', 3, 10, 1), 'subsample': hp.uniform('xgb_subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': hp.uniform('xgb_colsample_bytree', 0.6, 1.0), 'gamma': hp.uniform('xgb_gamma', 0, 0.3),\n",
    "            'reg_alpha': hp.uniform('xgb_reg_alpha', 0.0, 0.5),'reg_lambda': hp.uniform('xgb_reg_lambda', 0.0, 0.5),\n",
    "            'scale_pos_weight': hp.quniform('xgb_scale_pos_weight', 1, 10, 1) # For imbalanced classes\n",
    "    }},\n",
    "    'catboost': { 'model_params': {\n",
    "            'iterations': hp.quniform('cb_iterations', 50, 250, 25), 'learning_rate': hp.loguniform('cb_learning_rate', np.log(0.01), np.log(0.1)),\n",
    "            'depth': hp.quniform('cb_depth', 3, 10, 1), 'l2_leaf_reg': hp.loguniform('cb_l2_leaf_reg', np.log(1), np.log(9)),\n",
    "            'border_count': hp.quniform('cb_border_count', 32, 254, 32), # Max 254 for CPU\n",
    "            'subsample': hp.uniform('cb_subsample', 0.6, 1.0) # If bootstrap_type supports it\n",
    "    }}\n",
    "}\n",
    "print(\"Classifier Search spaces defined.\")\n",
    "\n",
    "# --- Helper to Load Processed Parquet and Prepare Sklearn X, y ---\n",
    "def load_processed_parquet_to_sklearn(parquet_path: str, label_col: str) -> Tuple[pd.DataFrame, pd.Series, List[str]]:\n",
    "    print(f\"    Loading Parquet for sklearn: {parquet_path}\")\n",
    "    pdf = pd.read_parquet(parquet_path)\n",
    "    if label_col not in pdf.columns:\n",
    "        print(f\"    Label column '{label_col}' not found. Assuming all columns in Parquet are features.\")\n",
    "        X_pdf = pdf; y_series = None\n",
    "    else:\n",
    "        y_series = pdf[label_col].astype(int); X_pdf = pdf.drop(columns=[label_col])\n",
    "    feature_names_ordered = X_pdf.columns.tolist()\n",
    "    print(f\"      Loaded X_pdf shape: {X_pdf.shape}, y_series shape: {y_series.shape if y_series is not None else 'N/A'}, Features: {len(feature_names_ordered)}\")\n",
    "    return X_pdf, y_series, feature_names_ordered\n",
    "\n",
    "# --- HPO Objective Function (Classification) ---\n",
    "global HPO_PARENT_RUN_ID_HPO_OBJ, CURRENT_ALGORITHM_TYPE_HPO_OBJ, FEATURE_NAMES_FOR_HPO_OBJ\n",
    "HPO_PARENT_RUN_ID_HPO_OBJ = None; CURRENT_ALGORITHM_TYPE_HPO_OBJ = None; FEATURE_NAMES_FOR_HPO_OBJ = []\n",
    "\n",
    "def objective_function_classification(hyperparams_from_hyperopt):\n",
    "    global HPO_PARENT_RUN_ID_HPO_OBJ, CURRENT_ALGORITHM_TYPE_HPO_OBJ, FEATURE_NAMES_FOR_HPO_OBJ\n",
    "    global SHARED_PROCESSED_TRAIN_PATH, SHARED_PROCESSED_TEST_PATH, TARGET_COLUMN_NAME, PREMIUM_COLUMN_NAME\n",
    "    global HPO_OPTIMIZATION_METRIC, GLOBAL_SEED, MAX_METRICS_TO_LOG_PER_RUN\n",
    "\n",
    "    sanitized_hyperparams = {k: v.item() if isinstance(v, np.generic) else int(v) if k in ['max_depth', 'depth', 'n_estimators', 'num_leaves', 'iterations', 'border_count', 'scale_pos_weight'] and v is not None else (int(v) if isinstance(v,float) and v.is_integer() else v) for k,v in hyperparams_from_hyperopt.items()}\n",
    "    if 'max_depth' in sanitized_hyperparams and sanitized_hyperparams['max_depth'] is None: pass # Allow None for max_depth\n",
    "    elif 'max_depth' in sanitized_hyperparams: sanitized_hyperparams['max_depth'] = int(sanitized_hyperparams['max_depth'])\n",
    "    if 'depth' in sanitized_hyperparams: sanitized_hyperparams['depth'] = int(sanitized_hyperparams['depth'])\n",
    "\n",
    "\n",
    "    trial_run_name = f\"Trial_{CURRENT_ALGORITHM_TYPE_HPO_OBJ}_{time.strftime('%Y%m%d-%H%M%S')}_{os.urandom(4).hex()}\"\n",
    "    with mlflow.start_run(run_name=trial_run_name, nested=True) as trial_run:\n",
    "        if HPO_PARENT_RUN_ID_HPO_OBJ: mlflow.set_tag(\"parent_hpo_campaign_run_id\", HPO_PARENT_RUN_ID_HPO_OBJ)\n",
    "        mlflow.log_param(\"model_type_trial\", CURRENT_ALGORITHM_TYPE_HPO_OBJ); mlflow.log_params(sanitized_hyperparams)\n",
    "        mlflow.set_tag(\"seed\", GLOBAL_SEED); mlflow.log_param(\"train_data_hpo\", SHARED_PROCESSED_TRAIN_PATH); mlflow.log_param(\"test_data_hpo\", SHARED_PROCESSED_TEST_PATH)\n",
    "\n",
    "        try:\n",
    "            X_train_pdf, y_train_series, train_feature_names = load_processed_parquet_to_sklearn(SHARED_PROCESSED_TRAIN_PATH, TARGET_COLUMN_NAME)\n",
    "            X_test_pdf, y_test_series, _ = load_processed_parquet_to_sklearn(SHARED_PROCESSED_TEST_PATH, TARGET_COLUMN_NAME) # Test feature names should match train\n",
    "            if y_test_series is None: raise ValueError(\"Test labels required for HPO evaluation.\")\n",
    "            \n",
    "            X_train_np, y_train_np = X_train_pdf.values, y_train_series.values\n",
    "            X_test_np, y_test_np = X_test_pdf.values, y_test_series.values\n",
    "            FEATURE_NAMES_FOR_HPO_OBJ = list(train_feature_names) # Capture for this trial scope for PDP\n",
    "\n",
    "            monotone_constraints_val = None\n",
    "            if PREMIUM_COLUMN_NAME in FEATURE_NAMES_FOR_HPO_OBJ:\n",
    "                premium_idx = FEATURE_NAMES_FOR_HPO_OBJ.index(PREMIUM_COLUMN_NAME)\n",
    "                monotone_constraints_val = [0] * len(FEATURE_NAMES_FOR_HPO_OBJ)\n",
    "                monotone_constraints_val[premium_idx] = -1\n",
    "                mlflow.log_param(\"monotonicity_applied\", f\"{PREMIUM_COLUMN_NAME}_idx{premium_idx}_is_-1\")\n",
    "            \n",
    "            model_params_for_fit = sanitized_hyperparams.copy()\n",
    "            model = None\n",
    "            if CURRENT_ALGORITHM_TYPE_HPO_OBJ == 'lightgbm':\n",
    "                if monotone_constraints_val: model_params_for_fit['monotone_constraints'] = monotone_constraints_val\n",
    "                model = lgb.LGBMClassifier(**model_params_for_fit, random_state=GLOBAL_SEED, n_jobs=-1, verbose=-1)\n",
    "            elif CURRENT_ALGORITHM_TYPE_HPO_OBJ == 'xgboost':\n",
    "                if monotone_constraints_val: model_params_for_fit['monotone_constraints'] = tuple(monotone_constraints_val)\n",
    "                model = xgb.XGBClassifier(**model_params_for_fit, random_state=GLOBAL_SEED, use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n",
    "            elif CURRENT_ALGORITHM_TYPE_HPO_OBJ == 'catboost':\n",
    "                if monotone_constraints_val: model_params_for_fit['monotone_constraints'] = monotone_constraints_val\n",
    "                model = cb.CatBoostClassifier(**model_params_for_fit, random_state=GLOBAL_SEED, verbose=0, allow_writing_files=False)\n",
    "            else: raise ValueError(f\"Unsupported model type: {CURRENT_ALGORITHM_TYPE_HPO_OBJ}\")\n",
    "            \n",
    "            model.fit(X_train_np, y_train_np)\n",
    "            pred_proba = model.predict_proba(X_test_np)[:, 1]\n",
    "            pred_labels = (pred_proba >= 0.5).astype(int)\n",
    "\n",
    "            auc_roc = roc_auc_score(y_test_np, pred_proba); auc_pr_val = average_precision_score(y_test_np, pred_proba)\n",
    "            logloss = log_loss(y_test_np, pred_proba); brier = brier_score_loss(y_test_np, pred_proba)\n",
    "            f1 = f1_score(y_test_np, pred_labels, zero_division=0); precision = precision_score(y_test_np, pred_labels, zero_division=0)\n",
    "            recall = recall_score(y_test_np, pred_labels, zero_division=0); accuracy = accuracy_score(y_test_np, pred_labels)\n",
    "            \n",
    "            metrics_to_log = {\"auc_roc\": auc_roc, \"auc_pr\": auc_pr_val, \"logloss\": logloss, \"brier_score\": brier, \"f1\": f1, \"precision\": precision, \"recall\": recall, \"accuracy\": accuracy}\n",
    "            for i, (m_name, m_val) in enumerate(sorted(metrics_to_log.items())):\n",
    "                if i < MAX_METRICS_TO_LOG_PER_RUN: mlflow.log_metric(m_name, m_val)\n",
    "            \n",
    "            model_signature = mlflow.models.infer_signature(X_test_np, pd.Series(pred_proba, name=TARGET_COLUMN_NAME))\n",
    "            if CURRENT_ALGORITHM_TYPE_HPO_OBJ == 'lightgbm': mlflow.lightgbm.log_model(model, \"model\", signature=model_signature)\n",
    "            elif CURRENT_ALGORITHM_TYPE_HPO_OBJ == 'xgboost': mlflow.xgboost.log_model(model, \"model\", signature=model_signature)\n",
    "            elif CURRENT_ALGORITHM_TYPE_HPO_OBJ == 'catboost': mlflow.catboost.log_model(model, \"model\", signature=model_signature)\n",
    "            \n",
    "            mlflow.set_tag(\"status\", \"success\")\n",
    "            loss = float('inf')\n",
    "            if HPO_OPTIMIZATION_METRIC == 'auc_roc': loss = -auc_roc\n",
    "            elif HPO_OPTIMIZATION_METRIC == 'auc_pr': loss = -auc_pr_val\n",
    "            elif HPO_OPTIMIZATION_METRIC == 'log_loss': loss = logloss\n",
    "            elif HPO_OPTIMIZATION_METRIC == 'brier_score': loss = brier\n",
    "            else: raise ValueError(f\"Unsupported HPO Metric: {HPO_OPTIMIZATION_METRIC}\")\n",
    "            return {'loss': loss, 'status': STATUS_OK, 'run_id': trial_run.info.run_id, 'attachments': metrics_to_log, 'model_instance': model, 'X_test_pdf_for_pdp': X_test_pdf.copy()} # Pass model and X for PDP\n",
    "\n",
    "        except Exception as e: # ... (Error handling as before) ...\n",
    "            mlflow.set_tag(\"status\", \"failed\"); print(f\"TRIAL ERROR: {e}\"); import traceback; traceback.print_exc();\n",
    "            return {'loss': float('inf'), 'status': 'fail', 'run_id': trial_run.info.run_id if 'trial_run' in locals() and hasattr(trial_run,'info') else None, 'error_message': str(e)[:250]}\n",
    "print(\"Objective function for classification defined.\")\n",
    "\n",
    "# --- Function to plot and log PDP for scikit-learn compatible models ---\n",
    "def plot_and_log_pdp_sklearn(model_to_plot, X_data_for_pdp_df: pd.DataFrame, feature_name_for_pdp: str, \n",
    "                             model_name_pdp: str, target_name_pdp_plot: str, \n",
    "                             pdp_grid_points: int, pdp_percentiles_range: tuple):\n",
    "    if X_data_for_pdp_df.empty or feature_name_for_pdp not in X_data_for_pdp_df.columns:\n",
    "        print(f\"  PDP: Skipping for {feature_name_for_pdp} in {model_name_pdp}, data empty or feature missing.\"); return\n",
    "    print(f\"  PDP: Generating for '{feature_name_for_pdp}' of model '{model_name_pdp}'...\")\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        # For classifiers, PDP by default shows one line per class if predict_proba is available.\n",
    "        # We are interested in P(Conversion=1), which is typically class 1.\n",
    "        # The `from_estimator` method usually handles this by plotting for each class or for the positive class's probability.\n",
    "        # We might need to ensure that the model's `predict_proba` is used and we plot for the positive class (index 1).\n",
    "        # If `target_idx=1` or similar is not available, we might plot all or manually get proba for class 1.\n",
    "        # For most sklearn classifiers, `kind='average'` with `predict_proba` should give P(class=1) if it's binary.\n",
    "        PartialDependenceDisplay.from_estimator(\n",
    "            model_to_plot, X_data_for_pdp_df, features=[feature_name_for_pdp],\n",
    "            # For classifiers and predict_proba, it often plots for each class.\n",
    "            # We might need to target class 1. If `response_method='predict_proba'`, \n",
    "            # then the display might show lines for each class.\n",
    "            # Let's assume for now it plots for the positive class or we handle it.\n",
    "            # Often, it's simpler to get P(class=1) and plot that directly if PDP function is tricky.\n",
    "            # However, from_estimator should be able to handle it.\n",
    "            # We are interested in the probability of class 1.\n",
    "            # For binary classification, if model.classes_ = [0, 1], then target_idx=1 for P(Y=1).\n",
    "            # If from_estimator plots multiple lines for classes, we might need to pick one.\n",
    "            # For now, let it plot what it defaults to for classifiers (often P(class=1) or both).\n",
    "            kind='average', n_cols=1, ax=ax,\n",
    "            n_jobs=-1, grid_resolution=pdp_grid_points, percentiles=pdp_percentiles_range\n",
    "        )\n",
    "        ax.set_title(f\"PDP: {feature_name_for_pdp} vs. P({target_name_pdp_plot}=1)\\nModel: {model_name_pdp}\", fontsize=14)\n",
    "        ax.set_xlabel(feature_name_for_pdp, fontsize=12); ax.set_ylabel(f\"Avg. Predicted P({target_name_pdp_plot}=1)\", fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7); fig.tight_layout()\n",
    "        mlflow.log_figure(fig, f\"pdp_{model_name_pdp}_{feature_name_for_pdp}.png\"); plt.close(fig)\n",
    "        print(f\"    PDP for {feature_name_for_pdp} of {model_name_pdp} logged.\")\n",
    "    except Exception as e_pdp: print(f\"    ERROR PDP for {feature_name_for_pdp} of {model_name_pdp}: {e_pdp}\")\n",
    "\n",
    "# --- Function to plot and log Feature Importance ---\n",
    "def plot_and_log_feature_importance(model_fi, feature_names_fi: List[str], model_name_fi: str, top_n: int = 20):\n",
    "    print(f\"  FeatureImp: Generating for model '{model_name_fi}'...\")\n",
    "    try:\n",
    "        if hasattr(model_fi, 'feature_importances_'):\n",
    "            importances = model_fi.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1][:top_n]\n",
    "            sorted_feature_names = [feature_names_fi[i] for i in indices]\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(12, max(6, top_n * 0.3))) # Adjust height\n",
    "            ax.barh(range(len(indices)), importances[indices][::-1], align='center', color='lightgreen') # Plot sorted\n",
    "            ax.set_yticks(range(len(indices)))\n",
    "            ax.set_yticklabels(sorted_feature_names[::-1]) # Match bar order\n",
    "            ax.set_xlabel(\"Feature Importance Score\", fontsize=12)\n",
    "            ax.set_ylabel(\"Feature\", fontsize=12)\n",
    "            ax.set_title(f\"Top {top_n} Feature Importances - {model_name_fi}\", fontsize=14)\n",
    "            fig.tight_layout()\n",
    "            mlflow.log_figure(fig, f\"feature_importance_{model_name_fi}.png\"); plt.close(fig)\n",
    "            print(f\"    Feature importance plot for {model_name_fi} logged.\")\n",
    "        elif hasattr(model_fi, 'coef_'): # For linear models (meta-learner)\n",
    "            if model_fi.coef_.ndim > 1: # e.g. LogisticRegression with multi_class='ovr' might have >1 set of coefs\n",
    "                importances = np.mean(np.abs(model_fi.coef_), axis=0) # Take mean of abs coefs for simplicity\n",
    "            else:\n",
    "                importances = np.abs(model_fi.coef_)\n",
    "            indices = np.argsort(importances)[::-1][:top_n]\n",
    "            sorted_feature_names = [feature_names_fi[i] for i in indices]\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, max(6, top_n * 0.3)))\n",
    "            ax.barh(range(len(indices)), importances[indices][::-1], align='center', color='lightgreen')\n",
    "            ax.set_yticks(range(len(indices))); ax.set_yticklabels(sorted_feature_names[::-1])\n",
    "            ax.set_xlabel(\"Absolute Coefficient Value\", fontsize=12); ax.set_ylabel(\"Feature (Base Model Prediction)\", fontsize=12)\n",
    "            ax.set_title(f\"Top {top_n} Feature Importances (Coefficients) - {model_name_fi}\", fontsize=14)\n",
    "            fig.tight_layout()\n",
    "            mlflow.log_figure(fig, f\"feature_importance_coeffs_{model_name_fi}.png\"); plt.close(fig)\n",
    "            print(f\"    Feature importance (coefficients) plot for {model_name_fi} logged.\")\n",
    "        else:\n",
    "            print(f\"    Model {model_name_fi} does not have 'feature_importances_' or 'coef_' attribute. Skipping FI plot.\")\n",
    "    except Exception as e_fi: print(f\"    ERROR FeatureImp for {model_name_fi}: {e_fi}\")\n",
    "\n",
    "\n",
    "# --- OOF Generation and Final Model Training Function (Classification) ---\n",
    "def train_final_model_and_generate_oof_classif(\n",
    "                                     model_type_oof, best_hyperparams_oof,\n",
    "                                     train_parquet_path_oof, test_parquet_path_oof, \n",
    "                                     label_col_name_oof, premium_col_name_oof,\n",
    "                                     k_folds_oof_val, seed_oof, mlflow_parent_run_name_prefix_oof,\n",
    "                                     oof_output_dir_ucv_oof, test_preds_output_dir_ucv_oof):\n",
    "    # ... (Sanitize hyperparams as in objective_function) ...\n",
    "    sanitized_best_hyperparams = {k: v.item() if isinstance(v, np.generic) else int(v) if k in ['max_depth', 'depth', 'n_estimators', 'num_leaves', 'iterations', 'border_count', 'scale_pos_weight'] and v is not None else (int(v) if isinstance(v,float) and v.is_integer() else v) for k,v in best_hyperparams_oof.items()}\n",
    "    if 'max_depth' in sanitized_best_hyperparams and sanitized_best_hyperparams['max_depth'] is None: pass\n",
    "    elif 'max_depth' in sanitized_best_hyperparams: sanitized_best_hyperparams['max_depth'] = int(sanitized_best_hyperparams['max_depth'])\n",
    "    if 'depth' in sanitized_best_hyperparams: sanitized_best_hyperparams['depth'] = int(sanitized_best_hyperparams['depth'])\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{mlflow_parent_run_name_prefix_oof}_{model_type_oof}_OOF_FinalModel\", nested=False) as oof_parent_run:\n",
    "        mlflow.log_params(sanitized_best_hyperparams); mlflow.log_param(\"model_type\", model_type_oof); mlflow.log_param(\"k_folds_for_oof\", k_folds_oof_val)\n",
    "        mlflow.set_tag(\"seed\", seed_oof); mlflow.log_param(\"train_data_oof\", train_parquet_path_oof); mlflow.log_param(\"test_data_oof\", test_parquet_path_oof)\n",
    "        final_model_run_id = oof_parent_run.info.run_id\n",
    "        print(f\"Starting OOF & Final Model for {model_type_oof}. MLflow Run ID: {final_model_run_id}\")\n",
    "\n",
    "        try:\n",
    "            X_full_train_pdf, y_full_train_series, train_feature_names = load_processed_parquet_to_sklearn(train_parquet_path_oof, label_col_name_oof)\n",
    "            X_test_pdf, y_test_series, test_feature_names = load_processed_parquet_to_sklearn(test_parquet_path_oof, label_col_name_oof)\n",
    "            \n",
    "            X_full_train_np, y_full_train_np = X_full_train_pdf.values, y_full_train_series.values\n",
    "            X_test_np, y_test_np = X_test_pdf.values, y_test_series.values if y_test_series is not None else None\n",
    "\n",
    "            # Monotonicity for OOF/Final model\n",
    "            oof_monotone_constraints_val = None\n",
    "            if premium_col_name_oof in train_feature_names:\n",
    "                premium_idx_oof = train_feature_names.index(premium_col_name_oof)\n",
    "                oof_monotone_constraints_val = [0] * len(train_feature_names); oof_monotone_constraints_val[premium_idx_oof] = -1\n",
    "                mlflow.log_param(\"final_model_mono_constraint_on\", f\"{premium_col_name_oof}_idx{premium_idx_oof}_is_-1\")\n",
    "            \n",
    "            oof_pred_probas_np = np.zeros_like(y_full_train_np, dtype=float)\n",
    "            kf = KFold(n_splits=k_folds_oof_val, shuffle=True, random_state=seed_oof)\n",
    "\n",
    "            for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_full_train_np, y_full_train_np)):\n",
    "                print(f\"    OOF Fold {fold_num+1}/{k_folds_oof_val} for {model_type_oof}...\")\n",
    "                X_f_train, X_f_val = X_full_train_np[train_idx], X_full_train_np[val_idx]\n",
    "                y_f_train = y_full_train_np[train_idx]\n",
    "                \n",
    "                model_fold_params = sanitized_best_hyperparams.copy()\n",
    "                if model_type_oof in ['lightgbm', 'xgboost', 'catboost'] and oof_monotone_constraints_val:\n",
    "                    if model_type_oof == 'xgboost': model_fold_params['monotone_constraints'] = tuple(oof_monotone_constraints_val)\n",
    "                    else: model_fold_params['monotone_constraints'] = oof_monotone_constraints_val\n",
    "\n",
    "                model_fold = None # Instantiate model (as in objective_function)\n",
    "                if model_type_oof == 'lightgbm': model_fold = lgb.LGBMClassifier(**model_fold_params, random_state=seed_oof, n_jobs=-1, verbose=-1)\n",
    "                elif model_type_oof == 'xgboost': model_fold = xgb.XGBClassifier(**model_fold_params, random_state=seed_oof, use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n",
    "                elif model_type_oof == 'catboost': model_fold = cb.CatBoostClassifier(**model_fold_params, random_state=seed_oof, verbose=0, allow_writing_files=False)\n",
    "                else: raise ValueError(f\"Unsupported model type for OOF: {model_type_oof}\")\n",
    "                \n",
    "                model_fold.fit(X_f_train, y_f_train)\n",
    "                oof_pred_probas_np[val_idx] = model_fold.predict_proba(X_f_val)[:, 1]\n",
    "            \n",
    "            oof_auc_roc_val = roc_auc_score(y_full_train_np, oof_pred_probas_np)\n",
    "            oof_logloss_val = log_loss(y_full_train_np, oof_pred_probas_np)\n",
    "            mlflow.log_metric(\"oof_auc_roc\", oof_auc_roc_val); mlflow.log_metric(\"oof_logloss\", oof_logloss_val)\n",
    "            print(f\"    {model_type_oof} OOF AUC_ROC: {oof_auc_roc_val:.4f}, OOF LogLoss: {oof_logloss_val:.4f}\")\n",
    "\n",
    "            # Train final model on ALL training data\n",
    "            final_model_params = sanitized_best_hyperparams.copy() # Apply monotonicity to final model too\n",
    "            if model_type_oof in ['lightgbm', 'xgboost', 'catboost'] and oof_monotone_constraints_val:\n",
    "                if model_type_oof == 'xgboost': final_model_params['monotone_constraints'] = tuple(oof_monotone_constraints_val)\n",
    "                else: final_model_params['monotone_constraints'] = oof_monotone_constraints_val\n",
    "            \n",
    "            final_model = None # Instantiate final model\n",
    "            if model_type_oof == 'lightgbm': final_model = lgb.LGBMClassifier(**final_model_params, random_state=seed_oof, n_jobs=-1, verbose=-1)\n",
    "            elif model_type_oof == 'xgboost': final_model = xgb.XGBClassifier(**final_model_params, random_state=seed_oof, use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n",
    "            elif model_type_oof == 'catboost': final_model = cb.CatBoostClassifier(**final_model_params, random_state=seed_oof, verbose=0, allow_writing_files=False)\n",
    "            else: raise ValueError(f\"Unsupported model type for final training: {model_type_oof}\")\n",
    "\n",
    "            final_model.fit(X_full_train_np, y_full_train_np)\n",
    "            final_model_test_pred_probas = final_model.predict_proba(X_test_np)[:, 1]\n",
    "\n",
    "            # Save OOF (train) and Test predictions (probabilities) as Parquet\n",
    "            oof_df_to_save = pd.DataFrame({f'oof_pred_proba_{model_type_oof}': oof_pred_probas_np}, index=X_full_train_pdf.index)\n",
    "            oof_df_to_save[label_col_name_oof] = y_full_train_np\n",
    "            \n",
    "            test_preds_df_to_save = pd.DataFrame({f'test_pred_proba_{model_type_oof}': final_model_test_pred_probas}, index=X_test_pdf.index)\n",
    "            if y_test_np is not None: test_preds_df_to_save[label_col_name_oof] = y_test_np\n",
    "\n",
    "            os.makedirs(oof_output_dir_ucv_oof, exist_ok=True); os.makedirs(test_preds_output_dir_ucv_oof, exist_ok=True)\n",
    "            oof_file_path = os.path.join(oof_output_dir_ucv_oof, f\"oof_pred_probas_{model_type_oof}.parquet\")\n",
    "            test_preds_file_path = os.path.join(test_preds_output_dir_ucv_oof, f\"test_pred_probas_{model_type_oof}.parquet\")\n",
    "            oof_df_to_save.to_parquet(oof_file_path); test_preds_df_to_save.to_parquet(test_preds_file_path) # index=True if index is meaningful\n",
    "            mlflow.log_artifact(oof_file_path, \"oof_predictions_parquet\"); mlflow.log_artifact(test_preds_file_path, \"test_predictions_parquet\")\n",
    "            mlflow.set_tag(f\"oof_pred_probas_path_{model_type_oof}\", oof_file_path); mlflow.set_tag(f\"test_pred_probas_path_{model_type_oof}\", test_preds_file_path)\n",
    "\n",
    "            if y_test_np is not None:\n",
    "                final_model_auc = roc_auc_score(y_test_np, final_model_test_pred_probas); mlflow.log_metric(\"final_model_test_auc_roc\", final_model_auc)\n",
    "                final_model_logloss = log_loss(y_test_np, final_model_test_pred_probas); mlflow.log_metric(\"final_model_test_logloss\", final_model_logloss)\n",
    "                print(f\"    {model_type_oof} Final Model Test AUC_ROC: {final_model_auc:.4f}, LogLoss: {final_model_logloss:.4f}\")\n",
    "            else: final_model_auc, final_model_logloss = None, None\n",
    "            \n",
    "            model_signature = mlflow.models.infer_signature(X_test_np, pd.Series(final_model_test_pred_probas, name=TARGET_COLUMN_NAME))\n",
    "            if model_type_oof == 'lightgbm': mlflow.lightgbm.log_model(final_model, \"final_model\", signature=model_signature)\n",
    "            elif model_type_oof == 'xgboost': mlflow.xgboost.log_model(final_model, \"final_model\", signature=model_signature)\n",
    "            elif model_type_oof == 'catboost': mlflow.catboost.log_model(final_model, \"final_model\", signature=model_signature)\n",
    "            \n",
    "            # PDP and Feature Importance for the final_model\n",
    "            plot_and_log_pdp_sklearn(final_model, X_test_pdf, PREMIUM_COLUMN_NAME, f\"Final_{model_type_oof}\", TARGET_COLUMN_NAME, PDP_N_GRID_POINTS, PDP_PERCENTILES)\n",
    "            plot_and_log_feature_importance(final_model, test_feature_names, f\"Final_{model_type_oof}\")\n",
    "\n",
    "            mlflow.set_tag(\"status\", \"success_oof_final\")\n",
    "            return {\"status\": \"success\", \"model_type\": model_type_oof, \"final_model_run_id\": final_model_run_id,\n",
    "                    \"oof_auc_roc\": oof_auc_roc_val, \"oof_logloss\": oof_logloss_val,\n",
    "                    \"final_model_test_auc_roc\": final_model_auc, \"final_model_test_logloss\": final_model_logloss,\n",
    "                    \"oof_pred_probas_path\": oof_file_path, \"test_pred_probas_path\": test_preds_file_path }\n",
    "        except Exception as e: # ... Error handling ...\n",
    "             print(f\"ERROR OOF/Final for {model_type_oof}: {e}\"); import traceback; traceback.print_exc()\n",
    "             mlflow.set_tag(\"status\", \"failed_oof_final\"); mlflow.log_param(\"error_oof_final\", str(e)[:250])\n",
    "             return {\"status\": \"failed\", \"model_type\": model_type_oof, \"error_message\": str(e)}\n",
    "\n",
    "\n",
    "# --- Function to Optimize Ensemble Weights for AUC ---\n",
    "def optimize_ensemble_weights_auc(oof_pred_probas_df: pd.DataFrame, y_true_oof: np.ndarray, model_names: List[str]):\n",
    "    \"\"\"Finds optimal weights for ensemble to maximize AUC on OOF predictions.\"\"\"\n",
    "    print(f\"  Optimizing ensemble weights for models: {model_names} to maximize AUC...\")\n",
    "    \n",
    "    oof_preds_array = oof_pred_probas_df[[f\"oof_pred_proba_{m}\" for m in model_names]].values\n",
    "    \n",
    "    def auc_objective(weights):\n",
    "        if not (0.999 <= np.sum(weights) <= 1.001): # Allow for small float precision issues\n",
    "            return 2.0 # Penalize if weights don't sum to 1 (since we want to maximize AUC, loss is -AUC)\n",
    "        \n",
    "        weighted_oof_preds = np.sum(oof_preds_array * weights, axis=1)\n",
    "        # Clip probabilities to avoid issues with log_loss if used, also good for AUC\n",
    "        weighted_oof_preds = np.clip(weighted_oof_preds, 1e-15, 1 - 1e-15)\n",
    "        auc = roc_auc_score(y_true_oof, weighted_oof_preds)\n",
    "        return -auc # We minimize -AUC to maximize AUC\n",
    "\n",
    "    num_models = len(model_names)\n",
    "    initial_weights = np.array([1.0 / num_models] * num_models)\n",
    "    bounds = [(0, 1)] * num_models # Weights between 0 and 1\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}) # Weights sum to 1\n",
    "\n",
    "    result = minimize(auc_objective, initial_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "    if result.success:\n",
    "        optimized_weights = result.x\n",
    "        optimized_auc = -result.fun\n",
    "        print(f\"    Optimized weights: {dict(zip(model_names, optimized_weights))}, Optimized OOF AUC: {optimized_auc:.4f}\")\n",
    "        return dict(zip(model_names, optimized_weights)), optimized_auc\n",
    "    else:\n",
    "        print(f\"    WARNING: Ensemble weight optimization failed or did not converge. Message: {result.message}. Using equal weights.\")\n",
    "        equal_weights = dict(zip(model_names, initial_weights))\n",
    "         # Recalculate AUC with equal weights for reporting\n",
    "        weighted_oof_preds_equal = np.sum(oof_preds_array * initial_weights, axis=1)\n",
    "        weighted_oof_preds_equal = np.clip(weighted_oof_preds_equal, 1e-15, 1 - 1e-15)\n",
    "        equal_auc = roc_auc_score(y_true_oof, weighted_oof_preds_equal)\n",
    "        return equal_weights, equal_auc\n",
    "\n",
    "\n",
    "# --- Pyfunc Model for Weighted Ensemble ---\n",
    "class WeightedEnsembleClassifierPyfunc(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, base_model_details_for_pyfunc: Dict[str, str], # {model_type: model_run_id}\n",
    "                       weights_for_pyfunc: Dict[str, float],\n",
    "                       feature_names_for_pyfunc: List[str], # For signature and internal use\n",
    "                       premium_col_name_for_pyfunc: str, # For PDP\n",
    "                       target_col_name_for_pyfunc: str # For signature\n",
    "                       ):\n",
    "        self.base_model_details = base_model_details_for_pyfunc\n",
    "        self.weights = weights_for_pyfunc\n",
    "        self.feature_names = feature_names_for_pyfunc\n",
    "        self.premium_col_name_pyfunc = premium_col_name_for_pyfunc\n",
    "        self.target_col_name_pyfunc = target_col_name_for_pyfunc\n",
    "        self.loaded_base_models = {} # To store loaded models in context\n",
    "\n",
    "    def load_context(self, context):\n",
    "        print(\"WeightedEnsemblePyfunc: Loading base models...\")\n",
    "        for model_type, model_run_id in self.base_model_details.items():\n",
    "            model_uri = f\"runs:/{model_run_id}/final_model\" # Assuming \"final_model\" is the artifact path\n",
    "            print(f\"  Loading base model: {model_type} from {model_uri}\")\n",
    "            try:\n",
    "                if model_type == 'lightgbm': self.loaded_base_models[model_type] = mlflow.lightgbm.load_model(model_uri)\n",
    "                elif model_type == 'xgboost': self.loaded_base_models[model_type] = mlflow.xgboost.load_model(model_uri)\n",
    "                elif model_type == 'catboost': self.loaded_base_models[model_type] = mlflow.catboost.load_model(model_uri)\n",
    "                else: self.loaded_base_models[model_type] = mlflow.sklearn.load_model(model_uri)\n",
    "                print(f\"    {model_type} loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ERROR loading base model {model_type} from {model_uri}: {e}\")\n",
    "                # Decide how to handle: raise error or allow ensemble to work with fewer models?\n",
    "                # For now, let it raise, as weights are specific to the full set.\n",
    "                raise\n",
    "        if not self.loaded_base_models or len(self.loaded_base_models) != len(self.weights):\n",
    "            raise ValueError(\"Mismatch between expected base models for weights and loaded models.\")\n",
    "        print(\"WeightedEnsemblePyfunc: All base models loaded.\")\n",
    "\n",
    "\n",
    "    def predict(self, context, model_input_pdf: pd.DataFrame):\n",
    "        print(f\"WeightedEnsemblePyfunc: Predicting on input shape {model_input_pdf.shape}\")\n",
    "        # Ensure model_input_pdf has the correct features in the correct order if models expect NumPy\n",
    "        # If base models were trained on X_np which came from X_pdf.values, then the order is preserved.\n",
    "        # Our base models were trained on X_np derived from X_pdf.values where X_pdf had feature_names_ordered.\n",
    "        # For safety, reorder input if self.feature_names is reliable\n",
    "        \n",
    "        X_input_np = None\n",
    "        if self.feature_names and all(col in model_input_pdf.columns for col in self.feature_names):\n",
    "            X_input_np = model_input_pdf[self.feature_names].values\n",
    "        else: # Fallback if feature_names not perfectly set, or input is already just features\n",
    "            X_input_np = model_input_pdf.values\n",
    "            if X_input_np.shape[1] != len(self.feature_names) and self.feature_names:\n",
    "                 print(f\"  Warning: Input columns for ensemble predict ({model_input_pdf.columns.tolist()}) do not match expected feature_names ({self.feature_names}). Using raw values.\")\n",
    "\n",
    "\n",
    "        final_predictions = np.zeros(len(X_input_np))\n",
    "        total_weight_applied = 0.0\n",
    "\n",
    "        for model_type, model_instance in self.loaded_base_models.items():\n",
    "            if model_type in self.weights:\n",
    "                weight = self.weights[model_type]\n",
    "                base_pred_proba = model_instance.predict_proba(X_input_np)[:, 1] # Probability of class 1\n",
    "                final_predictions += weight * base_pred_proba\n",
    "                total_weight_applied += weight\n",
    "            else:\n",
    "                print(f\"  Warning: No weight found for loaded base model {model_type}. Skipping its contribution.\")\n",
    "        \n",
    "        # Normalize if total_weight_applied is slightly off 1 due to float precision or filtering\n",
    "        if total_weight_applied > 1e-6 and not (0.999 <= total_weight_applied <= 1.001):\n",
    "            print(f\"  Normalizing final_predictions as total_weight_applied is {total_weight_applied}\")\n",
    "            final_predictions /= total_weight_applied\n",
    "        \n",
    "        final_predictions = np.clip(final_predictions, 0.0, 1.0) # Ensure valid probabilities\n",
    "        print(f\"WeightedEnsemblePyfunc: Prediction complete.\")\n",
    "        return pd.Series(final_predictions)\n",
    "\n",
    "\n",
    "# --- Function to plot ensemble weights ---\n",
    "def plot_and_log_ensemble_weights(weights_dict: Dict[str, float], ensemble_name: str):\n",
    "    print(f\"  Plotting weights for {ensemble_name}...\")\n",
    "    try:\n",
    "        names = list(weights_dict.keys())\n",
    "        values = list(weights_dict.values())\n",
    "        fig, ax = plt.subplots(figsize=(max(8, len(names) * 1.5), 5))\n",
    "        ax.bar(names, values, color='teal')\n",
    "        ax.set_ylabel(\"Optimized Weight\", fontsize=12)\n",
    "        ax.set_xlabel(\"Base Model Type\", fontsize=12)\n",
    "        ax.set_title(f\"Optimized Ensemble Weights - {ensemble_name}\", fontsize=14)\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        fig.tight_layout()\n",
    "        mlflow.log_figure(fig, f\"ensemble_weights_{ensemble_name.replace(' ', '_')}.png\")\n",
    "        plt.close(fig)\n",
    "        print(f\"    Ensemble weights plot for {ensemble_name} logged.\")\n",
    "    except Exception as e_plot_weights:\n",
    "        print(f\"    ERROR plotting ensemble weights for {ensemble_name}: {e_plot_weights}\")\n",
    "\n",
    "print(\"--- All Utility Functions & Core Logic (HPO, OOF, Ensemble) Defined ---\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# <-------------------- CELL 4: MAIN ORCHESTRATION LOGIC (HPO, OOF, Ensembling) -------------------->\n",
    "print(\"\\nCell 4: Main Orchestration Logic - Executing...\")\n",
    "\n",
    "# --- 0. Setup MLflow Experiment ---\n",
    "global main_training_mlflow_experiment_id\n",
    "main_training_mlflow_experiment_id = None\n",
    "try:\n",
    "    main_training_mlflow_experiment_id = get_or_create_experiment(MLFLOW_EXPERIMENT_PATH, spark)\n",
    "    if main_training_mlflow_experiment_id:\n",
    "        mlflow.set_experiment(experiment_id=main_training_mlflow_experiment_id)\n",
    "        print(f\"MLflow experiment '{MLFLOW_EXPERIMENT_PATH}' for Training is set with ID: {main_training_mlflow_experiment_id}\")\n",
    "    else: raise Exception(\"Main Training MLflow experiment could not be set. Halting.\")\n",
    "except Exception as e: print(f\"CRITICAL: Could not initialize main MLflow experiment. Error: {e}\") # Halt\n",
    "\n",
    "if main_training_mlflow_experiment_id:\n",
    "    # These globals are used by the objective function\n",
    "    global HPO_PARENT_RUN_ID_HPO_OBJ, CURRENT_ALGORITHM_TYPE_HPO_OBJ, FEATURE_NAMES_FOR_HPO_OBJ\n",
    "\n",
    "    # --- 1. Individual HPO for each Base Algorithm (Sequential) ---\n",
    "    print(\"\\n--- Phase 1: Individual Hyperparameter Optimization for Base Models (Sequential) ---\")\n",
    "    best_hpo_configs_per_algorithm = {} \n",
    "    best_hpo_models_for_pdp = {} # Store {'algo_type': model_instance} for PDP\n",
    "    X_test_pdf_for_pdp, _, test_feature_names_for_pdp = load_processed_parquet_to_sklearn(SHARED_PROCESSED_TEST_PATH, TARGET_COLUMN_NAME)\n",
    "\n",
    "\n",
    "    for algo_type_hpo in BASE_ALGORITHMS_TO_TRAIN:\n",
    "        print(f\"\\nStarting HPO Campaign for Algorithm: {algo_type_hpo}...\")\n",
    "        if algo_type_hpo not in ALGORITHM_CLASSIFIER_SEARCH_SPACES:\n",
    "            print(f\"  Warning: Search space for {algo_type_hpo} not defined. Skipping HPO.\"); continue\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"HPO_Campaign_{algo_type_hpo}\", nested=False) as hpo_campaign_run:\n",
    "            HPO_PARENT_RUN_ID_HPO_OBJ = hpo_campaign_run.info.run_id\n",
    "            CURRENT_ALGORITHM_TYPE_HPO_OBJ = algo_type_hpo\n",
    "            \n",
    "            mlflow.log_params({ # Log main HPO config\n",
    "                \"hpo_algorithm_target\": algo_type_hpo, \"num_hpo_trials_config\": NUM_HPO_TRIALS_PER_ALGO,\n",
    "                \"primary_metric_config\": HPO_OPTIMIZATION_METRIC, \"global_seed_config\": GLOBAL_SEED\n",
    "            })\n",
    "            try: mlflow.log_dict({k:str(v) for k,v in ALGORITHM_CLASSIFIER_SEARCH_SPACES[algo_type_hpo]['model_params'].items()}, f\"search_space_{algo_type_hpo}.json\")\n",
    "            except Exception as log_e: print(f\"  Warning: Could not log search space: {log_e}\")\n",
    "\n",
    "            hpo_trials_db = Trials()\n",
    "            try:\n",
    "                current_search_space = ALGORITHM_CLASSIFIER_SEARCH_SPACES[algo_type_hpo]['model_params']\n",
    "                print(f\"  Running fmin for {algo_type_hpo} with {NUM_HPO_TRIALS_PER_ALGO} trials...\")\n",
    "                best_indices = fmin(fn=objective_function_classification, space=current_search_space, algo=tpe.suggest,\n",
    "                                    max_evals=NUM_HPO_TRIALS_PER_ALGO, trials=hpo_trials_db, rstate=np.random.default_rng(GLOBAL_SEED))\n",
    "                \n",
    "                best_params = space_eval(current_search_space, best_indices)\n",
    "                best_trial = hpo_trials_db.best_trial\n",
    "                if best_trial and best_trial['result']['status'] == STATUS_OK:\n",
    "                    best_trial_run_id = best_trial['result'].get('run_id')\n",
    "                    best_loss = best_trial['result']['loss']\n",
    "                    attachments = best_trial['result'].get('attachments', {})\n",
    "                    print(f\"    Best HPO trial for {algo_type_hpo}: Loss={best_loss:.4f}, Params={best_params}, MLflow Trial Run ID={best_trial_run_id}\")\n",
    "                    mlflow.log_params({f\"best_hpo_{k}\": v for k,v in best_params.items()})\n",
    "                    mlflow.log_metric(\"best_hpo_loss_campaign\", best_loss)\n",
    "                    if best_trial_run_id: mlflow.set_tag(\"best_hpo_trial_run_id\", best_trial_run_id)\n",
    "                    for att_k, att_v in attachments.items():\n",
    "                        if isinstance(att_v, (int, float)) and att_k != \"model_type\": mlflow.log_metric(f\"best_hpo_trial_{att_k}\", att_v)\n",
    "                    \n",
    "                    best_hpo_configs_per_algorithm[algo_type_hpo] = {\"best_params\": best_params, \"best_trial_run_id\": best_trial_run_id, \"hpo_campaign_run_id\": HPO_PARENT_RUN_ID_HPO_OBJ, \"attachments\": attachments, \"best_trial_model_instance\": best_trial['result'].get('model_instance')}\n",
    "                    mlflow.set_tag(\"status_hpo_campaign\", \"success\")\n",
    "                    \n",
    "                    # PDP for the best model from this HPO campaign\n",
    "                    best_model_instance_for_pdp = best_trial['result'].get('model_instance')\n",
    "                    X_test_pdf_for_pdp_local = best_trial['result'].get('X_test_pdf_for_pdp') # Get X_test_pdf from objective\n",
    "                    if best_model_instance_for_pdp and X_test_pdf_for_pdp_local is not None and PREMIUM_COLUMN_NAME in X_test_pdf_for_pdp_local.columns:\n",
    "                        plot_and_log_pdp_sklearn(best_model_instance_for_pdp, X_test_pdf_for_pdp_local, PREMIUM_COLUMN_NAME,\n",
    "                                                 f\"BestHPO_{algo_type_hpo}\", TARGET_COLUMN_NAME, PDP_N_GRID_POINTS, PDP_PERCENTILES)\n",
    "                        plot_and_log_feature_importance(best_model_instance_for_pdp, list(X_test_pdf_for_pdp_local.columns), f\"BestHPO_{algo_type_hpo}\")\n",
    "                    else: print(f\"    Could not generate PDP/FI for BestHPO_{algo_type_hpo}, model instance or test data for PDP missing from trial result attachments.\")\n",
    "\n",
    "                else: print(f\"    HPO for {algo_type_hpo} no successful best trial.\"); mlflow.set_tag(\"status_hpo_campaign\", \"no_successful_best_trial\")\n",
    "            except Exception as e_fmin: # ... error handling for fmin ...\n",
    "                print(f\"  ERROR HPO fmin for {algo_type_hpo}: {e_fmin}\"); import traceback; traceback.print_exc(); mlflow.set_tag(\"status_hpo_campaign\", \"fmin_error\"); mlflow.log_param(\"error_fmin\", str(e_fmin)[:250])\n",
    "    print(\"--- Individual HPO Phase Completed ---\")\n",
    "\n",
    "    # --- 2. OOF Generation & Final Base Model Training ---\n",
    "    print(\"\\n--- Phase 2: OOF Prediction Generation & Final Base Model Training ---\")\n",
    "    final_base_model_outputs = {}\n",
    "    for algo_type_oof, hpo_data in best_hpo_configs_per_algorithm.items():\n",
    "        if hpo_data and hpo_data.get(\"best_params\"):\n",
    "            print(f\"\\nGenerating OOF & Final Model for: {algo_type_oof}...\")\n",
    "            oof_result = train_final_model_and_generate_oof_classif(\n",
    "                model_type_oof=algo_type_oof, best_hyperparams_oof=hpo_data['best_params'],\n",
    "                train_parquet_path_oof=SHARED_PROCESSED_TRAIN_PATH, test_parquet_path_oof=SHARED_PROCESSED_TEST_PATH,\n",
    "                label_col_name_oof=TARGET_COLUMN_NAME, premium_col_name_oof=PREMIUM_COLUMN_NAME,\n",
    "                k_folds_oof_val=K_FOLDS_OOF, seed_oof=GLOBAL_SEED, mlflow_parent_run_name_prefix_oof=\"MVP\",\n",
    "                oof_output_dir_ucv_oof=OOF_PREDS_DIR_UCV, test_preds_output_dir_ucv_oof=TEST_PREDS_DIR_UCV\n",
    "            )\n",
    "            if oof_result['status'] == 'success': final_base_model_outputs[algo_type_oof] = oof_result\n",
    "            else: print(f\"  Failed OOF/final model for {algo_type_oof}: {oof_result.get('error_message')}\")\n",
    "        else: print(f\"  Skipping OOF for {algo_type_oof}, no successful HPO result.\")\n",
    "    print(\"--- OOF Generation & Final Base Model Training Phase Completed ---\")\n",
    "\n",
    "    # --- 3. Weighted Ensemble Creation ---\n",
    "    print(\"\\n--- Phase 3: Weighted Ensemble Creation ---\")\n",
    "    if not final_base_model_outputs: print(\"No base models for ensembling. Skipping.\")\n",
    "    else:\n",
    "        y_true_test_for_ensemble_eval_np = None # Load true test labels for evaluating ensembles\n",
    "        try:\n",
    "            _, y_true_test_for_ensemble_eval_series, _ = load_processed_parquet_to_sklearn(SHARED_PROCESSED_TEST_PATH, TARGET_COLUMN_NAME)\n",
    "            if y_true_test_for_ensemble_eval_series is not None: y_true_test_for_ensemble_eval_np = y_true_test_for_ensemble_eval_series.values\n",
    "            else: print(\"    Warning: Could not load true test labels for ensemble evaluation.\")\n",
    "        except Exception as e_label: print(f\"    Error loading true test labels for ensemble: {e_label}\")\n",
    "\n",
    "        # Define ensemble combinations (all pairs, and all three)\n",
    "        algo_names_for_ensemble = [algo for algo, res in final_base_model_outputs.items() if res['status'] == 'success']\n",
    "        dynamic_ensemble_combinations = []\n",
    "        if len(algo_names_for_ensemble) >= 2:\n",
    "            for i in range(2, len(algo_names_for_ensemble) + 1):\n",
    "                for combo in itertools.combinations(algo_names_for_ensemble, i):\n",
    "                    dynamic_ensemble_combinations.append(combo)\n",
    "        \n",
    "        print(f\"  Will attempt to create weighted ensembles for combinations: {dynamic_ensemble_combinations}\")\n",
    "\n",
    "        for combo_idx, model_combo_tuple in enumerate(dynamic_ensemble_combinations):\n",
    "            current_combo_model_types = list(model_combo_tuple)\n",
    "            combo_name = \"_\".join(current_combo_model_types)\n",
    "            print(f\"\\n  Creating Weighted Ensemble for combination: {combo_name} ({combo_idx+1}/{len(dynamic_ensemble_combinations)})\")\n",
    "\n",
    "            # Gather OOF predictions and test predictions for current combination\n",
    "            combo_oof_dfs = []\n",
    "            combo_test_dfs = []\n",
    "            combo_oof_metrics_for_weighting = []\n",
    "            valid_models_in_combo_for_meta_features = []\n",
    "\n",
    "            # Load all OOF predictions once to get y_meta_train\n",
    "            # (create_ensemble_meta_features_from_parquet can be adapted or its logic used here)\n",
    "            # For simplicity, let's assume OOF predictions are DFs with one pred column each, and a label col\n",
    "            \n",
    "            temp_X_meta_train_pdf, temp_y_meta_train_np, temp_X_meta_test_pdf, _ = create_ensemble_meta_features_from_parquet(\n",
    "                base_model_types_ens=current_combo_model_types, # Only models in current combo\n",
    "                oof_pred_dir_ucv_ens=OOF_PREDS_DIR_UCV,\n",
    "                test_pred_dir_ucv_ens=TEST_PREDS_DIR_UCV,\n",
    "                label_col_name_in_oof_ens=TARGET_COLUMN_NAME,\n",
    "                test_true_labels_series_ens=pd.Series(y_true_test_for_ensemble_eval_np, name=TARGET_COLUMN_NAME) if y_true_test_for_ensemble_eval_np is not None else None\n",
    "            )\n",
    "\n",
    "            if temp_X_meta_train_pdf is None or temp_y_meta_train_np is None:\n",
    "                print(f\"    Could not create meta-features for combo {combo_name}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Ensure correct columns are selected for weight optimization based on combo\n",
    "            oof_pred_cols_for_combo = [f\"oof_pred_proba_{m}\" for m in current_combo_model_types if f\"oof_pred_proba_{m}\" in temp_X_meta_train_pdf.columns]\n",
    "            if len(oof_pred_cols_for_combo) != len(current_combo_model_types):\n",
    "                print(f\"    Warning: Mismatch in OOF pred columns for combo {combo_name}. Expected {len(current_combo_model_types)}, found {len(oof_pred_cols_for_combo)}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            current_oof_pred_probas_df_for_optim = temp_X_meta_train_pdf[oof_pred_cols_for_combo]\n",
    "\n",
    "\n",
    "            # Get OOF metrics for only the models in the current combination\n",
    "            for algo_type in current_combo_model_types:\n",
    "                if algo_type in final_base_model_outputs and final_base_model_outputs[algo_type]['status'] == 'success':\n",
    "                     combo_oof_metrics_for_weighting.append({\n",
    "                         'model_type': algo_type,\n",
    "                         'oof_auc_roc': final_base_model_outputs[algo_type].get('oof_auc_roc'),\n",
    "                         'oof_logloss': final_base_model_outputs[algo_type].get('oof_logloss')\n",
    "                         # Add primary metric for weighting here based on HPO_OPTIMIZATION_METRIC\n",
    "                     })\n",
    "                else: # Should not happen if already filtered by ensemble_base_model_types_list\n",
    "                    print(f\"    Warning: Model {algo_type} for combo {combo_name} missing from final_base_model_outputs or was not successful.\")\n",
    "\n",
    "\n",
    "            if not combo_oof_metrics_for_weighting or len(combo_oof_metrics_for_weighting) != len(current_combo_model_types) :\n",
    "                print(f\"    Not enough valid OOF metrics for weighting combo {combo_name}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            optimized_weights_dict, optimized_oof_auc = optimize_ensemble_weights_auc(\n",
    "                oof_pred_probas_df=current_oof_pred_probas_df_for_optim, # DF with OOF pred probas for current combo models\n",
    "                y_true_oof=temp_y_meta_train_np, # True labels for OOF set\n",
    "                model_names=current_combo_model_types # Names of models in current combo\n",
    "            )\n",
    "\n",
    "            with mlflow.start_run(run_name=f\"Ensemble_Weighted_{combo_name}\", nested=False) as ens_run:\n",
    "                mlflow.set_tag(\"ensemble_type\", \"weighted_average\"); mlflow.set_tag(\"base_models_in_ensemble\", combo_name)\n",
    "                mlflow.log_params({f\"weight_{m}\": w for m, w in optimized_weights_dict.items()})\n",
    "                mlflow.log_metric(\"optimized_ensemble_oof_auc\", optimized_oof_auc)\n",
    "                \n",
    "                # Apply weights to test predictions for this combo\n",
    "                test_pred_cols_for_combo = [f\"test_pred_proba_{m}\" for m in current_combo_model_types if f\"test_pred_proba_{m}\" in temp_X_meta_test_pdf.columns]\n",
    "                if len(test_pred_cols_for_combo) != len(current_combo_model_types):\n",
    "                    print(f\"    Warning: Mismatch in test pred columns for weighted ensemble {combo_name}. Skipping evaluation.\")\n",
    "                    mlflow.set_tag(\"status\", \"skipped_test_pred_mismatch\")\n",
    "                    continue\n",
    "                \n",
    "                current_test_pred_probas_df_for_eval = temp_X_meta_test_pdf[test_pred_cols_for_combo]\n",
    "                \n",
    "                # Ensure weights are applied in the correct order\n",
    "                weights_array = np.array([optimized_weights_dict[model_name] for model_name in current_combo_model_types])\n",
    "                ensemble_test_pred_probas = np.sum(current_test_pred_probas_df_for_eval.values * weights_array, axis=1)\n",
    "                ensemble_test_pred_probas = np.clip(ensemble_test_pred_probas, 0.0, 1.0)\n",
    "\n",
    "                if y_true_test_for_ensemble_eval_np is not None:\n",
    "                    ens_auc = roc_auc_score(y_true_test_for_ensemble_eval_np, ensemble_test_pred_probas)\n",
    "                    ens_logloss = log_loss(y_true_test_for_ensemble_eval_np, ensemble_test_pred_probas)\n",
    "                    mlflow.log_metrics({\"ensemble_test_auc_roc\": ens_auc, \"ensemble_test_logloss\": ens_logloss})\n",
    "                    print(f\"    Weighted Ens ({combo_name}) Test AUC: {ens_auc:.4f}, LogLoss: {ens_logloss:.4f}\")\n",
    "                else:\n",
    "                    print(f\"    Weighted Ens ({combo_name}) predictions generated, no true test labels for metrics.\")\n",
    "\n",
    "                # Package this specific weighted ensemble as PyFunc\n",
    "                base_model_uris_for_pyfunc = {\n",
    "                    mt: f\"runs:/{final_base_model_outputs[mt]['final_model_run_id']}/final_model\" \n",
    "                    for mt in current_combo_model_types if mt in final_base_model_outputs\n",
    "                }\n",
    "                \n",
    "                # Get feature names from one of the base model's X_test_pdf (they all see same features)\n",
    "                # This assumes X_test_pdf was loaded earlier for PDP for example.\n",
    "                # Let's load one test pdf to get feature names.\n",
    "                _, _, current_feature_names = load_processed_parquet_to_sklearn(SHARED_PROCESSED_TEST_PATH, TARGET_COLUMN_NAME)\n",
    "\n",
    "\n",
    "                pyfunc_ensemble = WeightedEnsembleClassifierPyfunc(\n",
    "                    base_model_details_for_pyfunc=base_model_uris_for_pyfunc,\n",
    "                    weights_for_pyfunc=optimized_weights_dict,\n",
    "                    feature_names_for_pyfunc=current_feature_names, # Pass feature names\n",
    "                    premium_col_name_for_pyfunc=PREMIUM_COLUMN_NAME,\n",
    "                    target_col_name_for_pyfunc=TARGET_COLUMN_NAME\n",
    "                )\n",
    "                # Define conda_env for pyfunc\n",
    "                ens_conda_env = { 'channels': ['conda-forge', 'defaults'], 'dependencies': [f'python={pd.__version__.split(\".\")[0]}.{pd.__version__.split(\".\")[1]}', 'pip',\n",
    "                    {'pip': [f'mlflow>={mlflow.__version__}', f'pandas>={pd.__version__}', f'numpy>={np.__version__}', f'scikit-learn>={sklearn.__version__}', f'lightgbm>={lgb.__version__}', f'xgboost>={xgb.__version__}', f'catboost>={cb.__version__}']}],\n",
    "                    'name': f'weighted_ens_{combo_name}_env'\n",
    "                }\n",
    "                # Need preprocessed test data (features only) for signature and PDP\n",
    "                X_test_pdf_for_ens_pdp, _, _ = load_processed_parquet_to_sklearn(SHARED_PROCESSED_TEST_PATH, TARGET_COLUMN_NAME)\n",
    "\n",
    "                # Infer signature using the pyfunc model itself on a sample\n",
    "                sample_input_for_sig = X_test_pdf_for_ens_pdp.head()\n",
    "                try:\n",
    "                    sample_output_for_sig = pyfunc_ensemble.predict(None, sample_input_for_sig)\n",
    "                    ens_signature = mlflow.models.infer_signature(sample_input_for_sig, pd.Series(sample_output_for_sig, name=TARGET_COLUMN_NAME))\n",
    "                except Exception as sig_e:\n",
    "                    print(f\"    Warning: Could not infer signature for ensemble {combo_name}: {sig_e}\")\n",
    "                    ens_signature = None\n",
    "\n",
    "                mlflow.pyfunc.log_model(\n",
    "                    artifact_path=f\"weighted_ensemble_{combo_name}\",\n",
    "                    python_model=pyfunc_ensemble,\n",
    "                    conda_env=ens_conda_env,\n",
    "                    signature=ens_signature,\n",
    "                    input_example=sample_input_for_sig if ens_signature else None\n",
    "                )\n",
    "                mlflow.set_tag(\"status\", \"success_ensemble_packaged\")\n",
    "                print(f\"    Weighted Ensemble Pyfunc model for {combo_name} logged.\")\n",
    "\n",
    "                # PDP for this weighted ensemble pyfunc model\n",
    "                # This requires loading the pyfunc model and then calling plot_and_log_pdp_sklearn\n",
    "                # For simplicity in this script, we can generate PDP by directly using the predict logic\n",
    "                # with varied premium on X_test_pdf_for_ens_pdp\n",
    "                # Or, more robustly, load the logged pyfunc model and then plot PDP.\n",
    "                # Let's defer PDP for pyfunc ensembles for now to keep this main script less complex\n",
    "                # but user knows they need to load the pyfunc and then call PDP function with it.\n",
    "                # We can log the weights plot.\n",
    "                plot_and_log_ensemble_weights(optimized_weights_dict, f\"WeightedEns_{combo_name}\")\n",
    "\n",
    "\n",
    "        else: # No valid base models after OOF\n",
    "            print(\"  Skipping ensemble creation as no base models passed the OOF stage successfully.\")\n",
    "\n",
    "else: # MLflow experiment not set\n",
    "    print(\"Halting script because main MLflow experiment for Training could not be set.\")\n",
    "\n",
    "print(\"\\n--- FULL TRAINING & ENSEMBLING ORCHESTRATION COMPLETED ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
